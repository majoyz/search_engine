<doc>
	<docid>0</docid>
	<title>c 语言宏定义 #define 的理解与资料整理</title>
	<link>http://blog.jobbole.com/107047/</link>
	<author></author>
	<content>
1. 利用 define 来定义 数值宏常量

#define 宏定义是个演技非常高超的替身演员，但也会经常耍大牌的，所以我们用它要慎之又慎。它可以出现在代码的任何地方，从本行宏定义开始，以后的代码就就都认识这个宏了；也可以把任何东西定义成宏。因为编译器会在预编译的时候用真身替换替身，而在我们的代码里面却又用常常用替身来帮忙。
看例子：

#define PI 3.141592654

在此后的代码中你尽可以使用PI 来代替3.141592654，而且你最好就这么做。不然的话，如果我要把PI 的精度再提高一些，你是否愿意一个一个的去修改这串数呢？你能保证不漏不出错？而使用PI 的话，我们却只需要修改一次（这是十分高效的）。
这种情况还不是最要命的，我们再看一个例子：

#define ERROR_POWEROFF  -1

如果你在代码里不用ERROR_POWEROFF 这个宏而用-1，尤其在函数返回错误代码的时候（往往一个开发一个系统需要定义很多错误代码）。肯怕上帝都无法知道-1 表示的是什么意思吧。这个-1，我们一般称为“魔鬼数”，上帝遇到它也会发狂的。所以，我奉劝你代码里一定不要出现“魔鬼数”。(这里是从代码可读性的角度进行考虑！)
但是我们利用define来定义数值类型的数据，一般只是用来定义  常量 ，如果 要定义一些变量，则可以使用c语言中const这个关键字。
我们已经讨论了const 这个关键字，我们知道const 修饰的数据是有类型的，而define 宏定义的数据没有类型。为了安全，我建议你以后在定义一些宏常数的时候用const代替，编译器会给const 修饰的只读变量做类型校验，减少错误的可能。
但一定要注意const修饰的不是常量而是readonly 的变量，const 修饰的只读变量不能用来作为定义数组的维数，也不能放在case 关键字后面。
2.利用 define 来定义 字符串宏常量
除了定义宏常数之外，经常还用来定义字符串，尤其是路径：

A),#define ENG_PATH_1 E:\English\listen_to_this\listen_to_this_3
B),#define ENG_PATH_2 “E:\English\listen_to_this\listen_to_this_3”

噢，到底哪一个正确呢？如果路径太长，一行写下来比较别扭怎么办？用反斜杠接续符 &#8221; 啊：

C), #define ENG_PATH_3 E:\English\listen_to_this\listen\_to_this_3

还没发现问题？这里用了4 个反斜杠，到底哪个是接续符？回去看看接续符反斜杠。
反斜杠作为接续符时，在本行其后面不能再有任何字符，空格都不行。所以，只有最后一个反斜杠才是接续符。至于A)和B)，那要看你怎么用了，既然define 宏只是简单的替换，那给ENG_PATH_1 加上双引号不就成了：“ENG_PATH_1”。
但是请注意：有的系统里规定路径的要用双反斜杠“\”,比如（这是正确的版本）：

#define ENG_PATH_4 E:\\English\\listen_to_this\\listen_to_this_3

3.用 define 宏定义 注释符号
上面对define 的使用都很简单，再看看下面的例子：

#define BSC //
#define BMC /*
#define EMC */
 
D),BSC my single-line comment
E),BMC my multi-line comment EMC

D)和E)都错误，为什么呢？因为注释先于预处理指令被处理,当这两行被展开成//…或/*…*/时,注释已处理完毕,此时再出现//…或/*…*/自然错误.（这一条需要对编译预处理有所理解，才能体会。看来我还得再写一篇这方面的文章。）
因此,试图用宏开始或结束一段注释是不行的。
4.用define 宏定义表达式
这些都好理解，下面来点有“技术含量”的，定义一年有多少秒：

#define SEC_A_YEAR 60*60*24*365

这个定义没错吧？很遗憾，很有可能错了，至少不可靠。你有没有考虑在16 位系统下把这样一个数赋给整型变量的时候可能会发生溢出？一年有多少秒也不可能是负数吧。
改一下：

#define SEC_A_YEAR （60*60*24*365）UL

又出现一个问题，这里的括号到底需不需要呢？继续看一个例子，定义一个宏函数，求x 的平方：

#define SQR (x) x * x

对不对？试试：假设x 的值为10，SQR (x)被替换后变成10*10。没有问题。
再试试：假设x 的值是个表达式10+1，SQR (x)被替换后变成10+1*10+1。问题来了，这并不是我想要得到的。怎么办？括号括起来不就完了？

#define SQR (x) （（x）*（x））

最外层的括号最好也别省了，看例子，求两个数的和：

#define SUM (x) （x）+（x）

如果x 的值是个表达式5*3,而代码又写成这样：SUM (x)* SUM (x)。替换后变成：（5*3）+（5*3）*（5*3）+（5*3）。又错了！所以最外层的括号最好也别省了。我说过define 是个演技高超的替身演员，但也经常耍大牌。要搞定它其实很简单，别吝啬括号就行了。
注意这一点：宏函数被调用时是以实参代换形参。而不是“值传送”。
5.宏定义中的空格
另外还有一个问题需要引起注意，看下面例子：

#define SUM （x） （x）+（x）

这还是定义的宏函数SUM（x）吗？显然不是。编译器认为这是定义了一个宏：SUM，其代表的是（x） （x）+（x）。
为什么会这样呢？其关键问题还是在于SUM 后面的这个空格。所以在定义宏的时候一定要注意什么时候该用空格，什么时候不该用空格。这个空格仅仅在定义的时候有效，在使用这个宏函数的时候，空格会被编译器忽略掉。也就是说，上一节定义好的宏函数SUM（x）在使用的时候在SUM 和（x）之间留有空格是没问题的。比如：SUM（3）和SUM （3）的意思是一样的。
6.#undef
#undef 是用来撤销宏定义的，用法如下：

#define PI 3.141592654

…

// code

#undef PI

//下面的代码就不能用PI 了，它已经被撤销了宏定义。
写好C语言，漂亮的宏定义很重要，使用宏定义可以防止出错，提高可移植性，可读性，方便性 等等。下面列举一些成熟软件中常用得宏定义：
1，防止一个头文件被重复包含

#ifndef COMDEF_H

#define COMDEF_H

//头文件内容

#endif

2，重新定义一些类型，防止由于各种平台和编译器的不同，而产生的类型字节数差异，方便移植。这里已经不是#define的范畴了。

typedef unsigned char boolean; /* Boolean value type. */
typedef unsigned long int uint32; /* Unsigned 32 bit value */
typedef unsigned short uint16; /* Unsigned 16 bit value */
typedef unsigned char uint8; /* Unsigned 8 bit value */
typedef signed long int int32; /* Signed 32 bit value */
typedef signed short int16; /* Signed 16 bit value */
typedef signed char int8; /* Signed 8 bit value */
//下面的不建议使用
typedef unsigned char byte; /* Unsigned 8 bit value type. */
typedef unsigned short word; /* Unsinged 16 bit value type. */
typedef unsigned long dword; /* Unsigned 32 bit value type. */
typedef unsigned char uint1; /* Unsigned 8 bit value type. */
typedef unsigned short uint2; /* Unsigned 16 bit value type. */
typedef unsigned long uint4; /* Unsigned 32 bit value type. */
typedef signed char int1; /* Signed 8 bit value type. */
typedef signed short int2; /* Signed 16 bit value type. */
typedef long int int4; /* Signed 32 bit value type. */
typedef signed long sint31; /* Signed 32 bit value */
typedef signed short sint15; /* Signed 16 bit value */
typedef signed char sint7; /* Signed 8 bit value */

3，得到指定地址上的一个字节或字

#define MEM_B( x ) ( *( (byte *) (x) ) )
#define MEM_W( x ) ( *( (word *) (x) ) )

4，求最大值和最小值

#define MAX( x, y ) ( ((x) > (y)) ? (x) : (y) )
#define MIN( x, y ) ( ((x) < (y)) ? (x) : (y) )

5，得到一个field在结构体(struct)中的偏移量

#define FPOS( type, field ) \
/*lint -e545 */ ( (dword) &(( type *) 0)-> field ) /*lint +e545 */

6,得到一个结构体中field所占用的字节数

#define FSIZ( type, field ) sizeof( ((type *) 0)->field )

7，按照LSB格式把两个字节转化为一个Word

#define FLIPW( ray ) ( (((word) (ray)[0]) * 256) + (ray)[1] )

8，按照LSB格式把一个Word转化为两个字节

#define FLOPW( ray, val ) \
 
(ray)[0] = ((val) / 256); \
 
(ray)[1] = ((val) & 0xFF)

9，得到一个变量的地址（word宽度）

#define B_PTR( var ) ( (byte *) (void *) &(var) )
 
#define W_PTR( var ) ( (word *) (void *) &(var) )

10，得到一个字的高位和低位字节

#define WORD_LO(xxx) ((byte) ((word)(xxx) & 255))
 
#define WORD_HI(xxx) ((byte) ((word)(xxx) >> 8))

11，返回一个比X大的最接近的8的倍数

#define RND8( x ) ((((x) + 7) / 8 ) * 8 )

12，将一个字母转换为大写

#define UPCASE( c ) ( ((c) >= 'a' && (c) <= 'z') ? ((c) - 0x20) : (c) )

13，判断字符是不是10进值的数字

#define DECCHK( c ) ((c) >= '0' && (c) <= '9')

14，判断字符是不是16进值的数字

#define HEXCHK( c ) ( ((c) >= '0' && (c) <= '9') ||\
 
((c) >= 'A' && (c) <= 'F') ||\
 
((c) >= 'a' && (c) <= 'f') )

15，防止溢出的一个方法

#define INC_SAT( val ) (val = ((val)+1 > (val)) ? (val)+1 : (val))

16，返回数组元素的个数

#define ARR_SIZE( a ) ( sizeof( (a) ) / sizeof( (a[0]) ) )

17，返回一个无符号数n尾的值MOD_BY_POWER_OF_TWO(X,n)=X%(2^n)

#define MOD_BY_POWER_OF_TWO( val, mod_by ) \
( (dword)(val) & (dword)((mod_by)-1) )

18，对于IO空间映射在存储空间的结构，输入输出处理

#define inp(port) (*((volatile byte *) (port)))
 
#define inpw(port) (*((volatile word *) (port)))
 
#define inpdw(port) (*((volatile dword *)(port)))
 
#define outp(port, val) (*((volatile byte *) (port)) = ((byte) (val)))
 
#define outpw(port, val) (*((volatile word *) (port)) = ((word) (val)))
 
#define outpdw(port, val) (*((volatile dword *) (port)) = ((dword) (val)))

19,使用一些宏跟踪调试
A N S I标准说明了五个预定义的宏名。它们是：

_ LINE _
_ FILE _
_ DATE _
_ TIME _
_ STDC _

可以定义宏，例如:
当定义了_DEBUG，输出数据信息和所在文件所在行

#ifdef _DEBUG
 
#define DEBUGMSG(msg,date) printf(msg);printf(“%d%d%d”,date,_LINE_,_FILE_)
 
#else
 
#define DEBUGMSG(msg,date)
 
#endif

20，宏定义防止使用是错误
用小括号包含。 
例如：

#define ADD(a,b) （a+b）

用do{}while(0)语句包含多语句防止错误
例如：

#define DO(a,b) a+b;\
 
a++;

应用时：

if(….)
 
DO(a,b); //产生错误
 
else

解决方法: 代码就只会执行一次。和直接加花括号有什么区别呢。哦对，不能随便在程序中，任意加｛｝，组成代码块的。

#define DO(a,b) do{a+b;\
 
a++;}while(0)

c 语言宏定义 #define 的理解与资料整理，首发于文章 - 伯乐在线。</content>
</doc>
<doc>
	<docid>1</docid>
	<title>程序员如何保持身体健康？</title>
	<link>http://blog.jobbole.com/107038/</link>
	<author></author>
	<content>最近听说公司的几个同事都大病了一场，有的还进行了大的手术，差点跟阎王爷报道了。努力工作固然重要，但是一定注意身体，身体不好了，随着来的就是，工作和生活双失。
我根据自己的实践，列举了几点习惯，可能可以帮助程序员调整一下工作和生活的节奏，从而达到生活和工作平衡，进而改进身体体质和健康。
1、请早睡
早睡都做不到的话，其他就都别谈了。像要跑步，要健身，要努力工作，如果没有早睡作为前提，通通没用的。只有早睡，白天精神气才有可能足，精神气足了，干事情才有劲。熬夜的话，现在80后程序员真心是熬不起了，你可能经常有这样的感慨：怎么现在敖几次夜后，就浑身不爽呢？。

2、请吃好
像街边摊，大排档，这些地方就别去了，他们用的油不好，食材也不好，你把这些不健康的东西吃进肚子里，真心会把身体搞坏。如果公司有食堂了，就去食堂吃，如果没有食堂的话，就自己做饭，然后带饭到公司。你自己可以仔细对比观察周围的人，吃的健康的人的起色和精神气通常都非常好。
3、请有规律的准时吃饭
比如吃中午饭，你第一天11点吃，第二天12点半吃，第三天一点半吃，这种情况，你的胃是吃不消的。得有规律的吃。最好能跟同事一起吃，这样比较准时，同时，跟大家一起吃，你也自然会吃的比较慢。吃的慢是护胃的一种非常好的方式。
千万不好因为工作忙，就不按时准时的吃饭。这种不准时吃饭，吃饭快的，可能会导致胃癌。
4、请健身
有了前面3点之后，你才来考虑健身的问题。健身除了塑形之外呢，还可以增强体质。健身不一定要去健身房的，在家里就可以了。你只要买了哑铃和杠铃，大部分健身动作就都可以完成了。不过健身一定要注意动作的正确性,这个是极其关键的，动作不对，除了健身效果不明显之外，还会伤到肌肉和骨头。
如果周围没有熟悉健身的朋友，可以看一些健身视频，慢慢揣摩动作。
本人就是个健身爱好者，每天都健身，不健身就浑身不爽。二话不说，附上本人的健身后效果图。(^__^) 嘻嘻……


5、请跑步或者游泳
游泳的效果是跑步的三倍，只是我自己的实践经验。当然女生的话，由于头发问题，游泳不能经常做，但是跑步则可以。游泳和跑步这些有氧运动，可以改进心肺机能。

6、加班不要太严重了
这个才是最关键的，经常性的频繁的加班，必定打乱你所有的生活节奏，节奏不对，身体就很快吃不消，各种健康问题也都随之而来。
7、多掌握一些编程以外的技能
程序员不能简单的做码农，日子也要过的精彩。平时可以多学习一些技能，像健身、乒乓球、羽毛球、溜冰、唱歌、保龄球、钢琴、吉他、做家常菜等等。可以帮助你把日子过的精彩些，要往自己身上多投资。
程序员如何保持身体健康？，首发于文章 - 伯乐在线。</content>
</doc>
<doc>
	<docid>2</docid>
	<title>全栈必备：DevOps</title>
	<link>http://blog.jobbole.com/107021/</link>
	<author></author>
	<content>全栈不仅是一个研发多面手，而且必须要关注产品的最终交付，以及线上服务的稳定运行。工具化使开发、交付、运维紧密地联系在一起，于是DevOps 逐渐成为了全栈们手中的利器，但由于DevOps的复杂性，如果没有科学的人员、流程与工具相配合，DevOps根本无从谈起，因此，DevOps 更是一柄双刃剑。

什么是DevOps呢？
先看一下wiki百科给出的定义：
DevOps (a clipped compound of development and operations) is a culture, movement or practice that emphasizes the collaboration and communication of both software developers and other information-technology (IT) professionals while automating the process of software delivery and infrastructure changes. It aims at establishing a culture and environment where building, testing, and releasing software can happen rapidly, frequently, and more reliably.
简单地说，DevOps是一种开发、测试、运营、维护部门之间沟通、协作与整合的软件过程、方法与系统。
DevOps是一种高度强调人与人间互动的工作方式，不能先入为主地认为参与者了解某方面技能，在完成高频率部署的同时，提高生产环境的可靠稳定和安全行。
DevOps能够为团队提供一种极具凝聚力的文化氛围，DevOps不光是一个方法理念，而且是一个有力的技术手段，人员、文化、流程与工具这几大要素在DevOps中同样重要。
为什么DevOps姗姗来迟
DevOps 的概念在2009年就诞生了，但没有相关的技术支持，只是出现在教科书和论文里。然而，近年来所谓DevOps的最佳实践逐渐越来越多，原因何在？

云服务的普遍使用，各种云服务成为IT基础设施中不可分隔的一部分。运维有一个很重要的概念就是Infrastructure as code。
容器技术开始成熟，特别是Docker技术的大行其道。容器 Container是用来存储和组织其他对象的对象。Docker是一个开源的应用容器引擎。
微服务架构技术的广泛使用。
微服务 MicroService是指一个单纯的小型有意义的功能。
微服务，是支撑DevOps方法的手段，传统开发是在一个服务器里面，把各种元素装在一起组合成一个程序，但微服务是每一个服务是一个单独的单元，可以部署在不同的服务器上，通过SOA的方法，把它连接起来，再提供整个功能。
微服务是由一个个团队组成，每团队有自己的服务，做好后，可以独立的进行测试、开发、部署，然后整个应用组合到一起。张侠表示，开发运维一体化、微服务和Container是同等的，把它们组合起来，加上云的手段才成为可能。

4.敏捷开发流程的深入人心。
诸如Scrum, Agile, Kanban等敏捷方式被团队广泛使用，TDD、BDD、DDD这些测试驱动设计、行为驱动设计、域驱动设计等设计方式的采纳，CI和CD这些持续集成和持续部署等方式的实施，这些都是对DevOps的强烈需求。
DevOps中的技术栈与工具链
在全栈眼中，Everything is Code，所以DevOps 是通过技术工具链完成持续集成、持续交付、用户反馈和系统优化的整合，实现跨团队的无缝协作。
DevOps 中涉及的技术栈与工具链如下：

DevOps 流程门户： 这是统一操作的web网站，主要是进度看板，Sprint周期等。本着拿来主义，在一定条件下，可以采用类似Trello，worktile等工具代替。
身份及访问管理： 用户权限管理的重要组成，可以采用RABC的方式实现，也可以与LDAP服务对接
产品管理： 产品的需求，定义，依赖，推广等产品线的全面管理，confluence 可能是个不错的选择，禅道也可以满足一部分的功能
配置管理： 提高产品的配置维护能力，zookeeper 大概是不二之选。
持续集成： 提供持续集成任务调度和执行的能力，Jenkins的用武之地，提供产品和组件自动编译、打包和部署的能力，支持编译和部署的流程编制，进度跟踪和日志查看
环境管理： 提供资源配给和负载均衡的能力，需要配合云服务的资源管理能力。初级的负载均衡可以选择nginx或者Haproxy，生产环境的入口最好采用云服务的SLB负载均衡，以便简单地解决HA的问题。资源的调度采用云的弹性能力，辅助脚本实现。同时，微服务的容器化（docker）管理需要特别关注。
质量反馈： 提供产品的质量管理和监控能力，包括测试用例，缺陷跟踪和质量监控。Jira 是个不错的选择，其他的开源工具例如禅道，bugzila，mantis等等，因团队而异。
版本控制： 代码库的创建和维护，分支管理等。Git 几乎是行业的标准，可以自建Git仓库的服务器，也可以使用github 或者bitbucket这样的第三方服务。
自动化测试： 包括客户端与服务器端的自动化测试框架，例如Appium，Selenium 以及各种Mock技术和xUnit
文档管理：各种开发、运维、部署文档的统一管理，同样最好放到git上，同时指出文档的自动化生成
运营管理：这就是传说中的OAM 中心，这是广义的运营，其中还包括运维的部分。OAM 不但提供了业务系统的运营操作，还提供了面向运维的统一Monitor，alarm，fault handling等能力，以及产品的资源使用和运行状况等，涉及的技术很多，尽量采用云监控＋脚本的方式，规模较小时可以尝zZabbix 实现部分功能。
沟通管理： 敏捷的一个原则就是沟通优于文档，IM是团队必备，微信和QQ可以满足大部分的需求，但是Slack 因为其强大的web hook 功能显得更加出色。

DevOps 的双刃剑
DevOps 的成功与技术、流程和组织的全面支撑是密不可分的。技术栈和工具链只是DevOps的一个前提和基础，技术方面的实践相对容易，流程较难，组织变革最为艰难。DevOps还是以工程实践为主，管理实践这块，像Scrum成体系的还比较少。DevOps玩得好，可以提高团队的生产力。若是玩不好，可能还不如传统的生产模式有效率。
狭义上看，DevOps主要困难点在于开发和运维是两种完全不同性质的技术工作。很多开发的同事，看着运维人员整天就是玩几个工具，写几个脚本，觉得蛮简单，实际上，很多东西要在生产环境下快速稳定应用，并没有看上去那么容易。生产系统少出问题（软件本身bug除外）是运维的绩效，多实现业务需求是开发的绩效，这一少一多，体现了两种技术角色的根本性区别。
业务部门压力往往导致技术部门的任务主要是求“快”，在这种情况下，DevOps必然失衡，因为只追求快，就不需要ops了，只需要dev加班加点即可，不重视ops，结果必然是可悲的，往往业务上线后鸡飞狗跳，各种问题不断。在激烈竞争环境中，出几次事故就可能对产品形象的伤害很大。
对全栈来说，业务初期到底要不要考虑高可用？从Dev角度看，简洁明快的实现就行了，从Ops的角度看，高可用、监控、报表这些东西在业务正式上线前就是必须要考虑的。
因此，DevOps实施成功的关键，涉及到团队管理，项目管理，技术管理等诸多方面。DevOps并非治病良药，如果团队正能量大，实施起来就相对容易，否则引入DevOps可能也无法改变什么。对于一个全栈而言，DevOps是一柄必备的双刃剑。

全栈必备：DevOps，首发于文章 - 伯乐在线。</content>
</doc>
<doc>
	<docid>3</docid>
	<title>C++ 的强制类型转换</title>
	<link>http://blog.jobbole.com/107033/</link>
	<author></author>
	<content>Q:什么是C风格转换？什么是static_cast, dynamic_cast 以及 reinterpret_cast？区别是什么？为什么要注意？
A:转换的含义是通过改变一个变量的类型为别的类型从而改变该变量的表示方式。为了类型转换一个简单对象为另一个对象你会使用传统的类型转换操作符。
比如，为了转换一个类型为doubole的浮点数的指针到整型：
代码int i;
double d;
i = (int) d;或者：i = int (d);对于具有标准定义转换的简单类型而言工作的很好。然而，这样的转换符也能不分皂白的应用于类（class）和类的指针。ANSI-C++标准定义了四个新的转换符：&#8217;reinterpret_cast&#8217;, &#8216;static_cast&#8217;, &#8216;dynamic_cast&#8217; 和 &#8216;const_cast&#8217;，目的在于控制类(class)之间的类型转换。
代码:reinterpret_cast<new_type>(expression)
dynamic_cast<new_type>(expression)
static_cast<new_type>(expression)
const_cast<new_type>(expression)
1 reinterpret_cast
reinterpret_cast 转换一个指针为其它类型的指针。它也允许从一个指针转换为整数类型。反之亦然。（译注：是指针具体的地址值作为整数值？）
这个操作符能够在非相关的类型之间转换。操作结果只是简单的从一个指针到别的指针的值的二进制拷贝。在类型之间指向的内容不做任何类型的检查和转换。如果情况是从一个指针到整型的拷贝，内容的解释是系统相关的，所以任何的实现都不是方便的。一个转换到足够大的整型能够包含它的指针是能够转换回有效的指针的。
代码:class A {};
class B {};
A * a = new A;
B * b = reinterpret_cast<B *>(a);reinterpret_cast 就像传统的类型转换一样对待所有指针的类型转换。
2 static_cast
static_cast 允许执行任意的隐式转换和相反转换动作。（即使它是不允许隐式的）
意思是说它允许子类类型的指针转换为父类类型的指针（这是一个有效的隐式转换），同时，也能够执行相反动作：转换父类为它的子类。在这最后例子里，被转换的父类没有被检查是否与目的类型相一致。
代码：class Base {};
class Derived : public Base {};

Base *a = new Base;
Derived *b = static_cast<Derived *>(a);static_cast 除了操作类型指针，也能用于执行类型定义的显式的转换，以及基础类型之间的标准转换:
代码:double d = 3.14159265;
int i = static_cast<int>(d);
3 dynamic_cast
dynamic_cast只用于对象的指针和引用。当用于多态类型时，它允许任意的隐式类型转换以及相反过程。不过，与static_cast不同，在后一种情况里（注：即隐式转换的相反过程），dynamic_cast会检查操作是否有效。也就是说，它会检查转换是否会返回一个被请求的有效的完整对象。
检测在运行时进行。如果被转换的指针不是一个被请求的有效完整的对象指针，返回值为NULL.
代码：class Base { virtual dummy() {} };
class Derived : public Base {};

Base* b1 = new Derived;
Base* b2 = new Base;

Derived* d1 = dynamic_cast<Derived *>(b1); // succeeds
Derived* d2 = dynamic_cast<Derived *>(b2); // fails: returns 'NULL'如果一个引用类型执行了类型转换并且这个转换是不可能的，一个bad_cast的异常类型被抛出：
代码:class Base { virtual dummy() {} };
class Derived : public Base { };

Base* b1 = new Derived;
Base* b2 = new Base;

Derived d1 = dynamic_cast<Derived &*>(b1); // succeeds
Derived d2 = dynamic_cast<Derived &*>(b2); // fails: exception thrown
4 const_cast
这个转换类型操纵传递对象的const属性，或者是设置或者是移除：
代码:class C {};
const C *a = new C;

C *b = const_cast<C *>(a);其它三种操作符是不能修改一个对象的常量性的。注意：&#8217;const_cast&#8217;也能改变一个类型的volatile qualifier。
C++ 的四种强制转型形式每一种适用于特定的目的

dynamic_cast 主要用于执行“安全的向下转型（safe downcasting）”，也就是说，要确定一个对象是否是一个继承体系中的一个特定类型。它是唯一不能用旧风格语法执行的强制转型，也是唯一可能有重大运行时代价的强制转型。
static_cast 可以被用于强制隐型转换（例如，non-const 对象转型为 const 对象，int 转型为 double，等等），它还可以用于很多这样的转换的反向转换（例如，void* 指针转型为有类型指针，基类指针转型为派生类指针），但是它不能将一个 const 对象转型为 non-const 对象（只有 const_cast 能做到），它最接近于C-style的转换。
const_cast 一般用于强制消除对象的常量性。它是唯一能做到这一点的 C++ 风格的强制转型。
reinterpret_cast 是特意用于底层的强制转型，导致实现依赖（implementation-dependent）（就是说，不可移植）的结果，例如，将一个指针转型为一个整数。这样的强制转型在底层代码以外应该极为罕见。

C++ 的强制类型转换，首发于文章 - 伯乐在线。</content>
</doc>
<doc>
	<docid>4</docid>
	<title>Go net/http 超时机制完全手册</title>
	<link>http://blog.jobbole.com/107012/</link>
	<author></author>
	<content>当用Go写HTTP的服务器和客户端的时候，超时处理总是最易犯错和最微妙的地方之一。错误可能来自很多地方，一个错误可能等待很长时间没有结果，直到网络故障或者进程挂起。
HTTP是一个复杂的、多阶段(multi-stage)协议，所以没有一个放之四海而皆准的超时解决方案，比如一个流服务、一个JSON API和一个Comet服务对超时的需求都不相同， 往往默认值不是你想要的。
本文我将拆解需要超时设置的各个阶段，看看用什么不同的方式去处理它， 包括服务器端和客户端。
SetDeadline
首先，你需要了解Go实现超时的网络原语(primitive): Deadline (最后期限)。
net.Conn为Deadline提供了多个方法Set[Read|Write]Deadline(time.Time)。Deadline是一个绝对时间值，当到达这个时间的时候，所有的 I/O 操作都会失败，返回超时(timeout)错误。
Deadline不是超时(timeout)。一旦设置它们永久生效(或者直到下一次调用SetDeadline), 不管此时连接是否被使用和怎么用。所以如果想使用SetDeadline建立超时机制，你不得不每次在Read/Write操作之前调用它。
你可能不想自己调用SetDeadline, 而是让net/http代替你调用，所以你可以调用更高级的timeout方法。但是请记住，所有的超时的实现都是基于Deadline, 所以它们不会每次接收或者发送重新设置这个值(so they do NOT reset every time data is sent or received)。
江南雨的指正：
应该是由于“Deadline是一个绝对时间值”，不是真的超时机制，所以作者特别提醒，这个值不会自动重置的，需要每次手动设置。
服务器端超时设置

对于暴露在网上的服务器来说，为客户端连接设置超时至关重要，否则巨慢的或者隐失的客户端可能导致文件句柄无法释放，最终导致服务器出现下面的错误:
http: Accept error: accept tcp [::]:80: accept4: too many open files; retrying in 5ms
http.Server有两个设置超时的方法: ReadTimeout 和 andWriteTimeout`。你可以显示地设置它们：

srv := &http.Server{  
    ReadTimeout: 5 * time.Second,
    WriteTimeout: 10 * time.Second,
}
log.Println(srv.ListenAndServe())
ReadTimeout的时间计算是从连接被接受(accept)到request body完全被读取(如果你不读取body，那么时间截止到读完header为止)。它的内部实现是在Accept立即调用SetReadDeadline方法(代码行)。

……
  if d := c.server.ReadTimeout; d != 0 {
	c.rwc.SetReadDeadline(time.Now().Add(d))
}
if d := c.server.WriteTimeout; d != 0 {
	c.rwc.SetWriteDeadline(time.Now().Add(d))
}
  ……
WriteTimeout的时间计算正常是从request header的读取结束开始，到 response write结束为止 (也就是 ServeHTTP 方法的声明周期), 它是通过在readRequest方法结束的时候调用SetWriteDeadline实现的(代码行)。

func (c *conn) readRequest(ctx context.Context) (w *response, err error) {
	if c.hijacked() {
		return nil, ErrHijacked
	}
	if d := c.server.ReadTimeout; d != 0 {
		c.rwc.SetReadDeadline(time.Now().Add(d))
	}
	if d := c.server.WriteTimeout; d != 0 {
		defer func() {
			c.rwc.SetWriteDeadline(time.Now().Add(d))
		}()
	}
  ……
}
但是，当连接是HTTPS的时候，SetWriteDeadline会在Accept之后立即调用(代码)，所以它的时间计算也包括 TLS握手时的写的时间。 讨厌的是， 这就意味着(也只有这种情况) WriteTimeout设置的时间也包含读取Headerd到读取body第一个字节这段时间。

if tlsConn, ok := c.rwc.(*tls.Conn); ok {
		if d := c.server.ReadTimeout; d != 0 {
			c.rwc.SetReadDeadline(time.Now().Add(d))
		}
		if d := c.server.WriteTimeout; d != 0 {
			c.rwc.SetWriteDeadline(time.Now().Add(d))
		}
    ……
当你处理不可信的客户端和网络的时候，你应该同时设置读写超时，这样客户端就不会因为读慢或者写慢长久的持有这个连接了。
最后，还有一个http.TimeoutHandler方法。 它并不是Server参数，而是一个Handler包装函数，可以限制 ServeHTTP调用。它缓存response, 如果deadline超过了则发送 504 Gateway Timeout 错误。 注意这个功能在 1.6 中有问题，在1.6.2中改正了。
http.ListenAndServe 的错误
顺便提一句，net/http包下的封装的绕过http.Server的函数http.ListenAndServe, http.ListenAndServeTLS 和 http.Serve并不适合实现互联网的服务器。这些函数让超时设置默认不启用，并且你没有办法设置启用超时处理。所以如果你使用它们，你会很快发现连接泄漏，太多的文件句柄。我犯过这种错误至少五六次。
取而代之，你应该创建一个http.Server示例，设置ReadTimeout和WriteTimeout,像上面的例子中一样使用相应的方法。
关于流
令人心塞的是， 没有办法从ServeHTTP中访问底层的net.Conn，所以提供流服务强制不去设置WriteTimeout（这也可能是为什么这些值的默认值总为0）。如果无法访问net.Conn就不能在每次Write的时候调用SetWriteDeadline来实现一个正确的idle timeout。
而且，也没有办法取消一个阻塞的ResponseWriter.Write，因为ResponseWriter.Close没有文档指出它可以取消一个阻塞并发写。也没有办法使用Timer创建以俄国手工的timeout 杯具就是流服务器不能对于慢读的客户端进行防护。我提交的了一个［bug](https://github.com/golang/go/issues/16100)，欢迎大家反馈。
编者按: 作者此处的说法是有问题的，可以通过Hijack获取net.Conn,既然可以可以获取net.Conn,我们就可以调用它的SetWriteDeadline方法。代码例子如下：

package main
import (
	"fmt"
	"log"
	"net/http"
)
func main() {
	http.HandleFunc("/hijack", func(w http.ResponseWriter, r *http.Request) {
		hj, ok := w.(http.Hijacker)
		if !ok {
			http.Error(w, "webserver doesn't support hijacking", http.StatusInternalServerError)
			return
		}
		conn, bufrw, err := hj.Hijack()
		if err != nil {
			http.Error(w, err.Error(), http.StatusInternalServerError)
			return
		}
		// Don't forget to close the connection:
		defer conn.Close()
		conn.SetWriteDeadline(time.Now().Add(10 * time.Second))
		bufrw.WriteString("Now we're speaking raw TCP. Say hi: ")
		bufrw.Flush()
		s, err := bufrw.ReadString('\n')
		if err != nil {
			log.Printf("error reading string: %v", err)
			return
		}
		fmt.Fprintf(bufrw, "You said: %q\nBye.\n", s)
		bufrw.Flush()
	})
}
客户端超时设置

Client端的超时设置说复杂也复杂，说简单也简单，看你怎么用了，最重要的就是不要有资源泄漏的情况或者程序被卡住。
最简单的方式就是使用http.Client的 Timeout字段。 它的时间计算包括从连接(Dial)到读完response body。

c := &http.Client{  
    Timeout: 15 * time.Second,
}
resp, err := c.Get("https://blog.filippo.io/")
就像服务器端一样，http.GET使用Client的时候也没有超时设置,所以在互联网上使用也很危险。
有一些更细粒度的超时控制：

net.Dialer.Timeout 限制建立TCP连接的时间
http.Transport.TLSHandshakeTimeout 限制 TLS握手的时间
http.Transport.ResponseHeaderTimeout 限制读取response header的时间
http.Transport.ExpectContinueTimeout 限制client在发送包含 Expect: 100-continue的header到收到继续发送body的response之间的时间等待。注意在1.6中设置这个值会禁用HTTP/2(DefaultTransport自1.6.2起是个特例)


c := &http.Client{  
    Transport: &Transport{
        Dial: (&net.Dialer{
                Timeout:   30 * time.Second,
                KeepAlive: 30 * time.Second,
        }).Dial,
        TLSHandshakeTimeout:   10 * time.Second,
        ResponseHeaderTimeout: 10 * time.Second,
        ExpectContinueTimeout: 1 * time.Second,
    }
}
如我所讲，没有办法限制发送request的时间。读取response body (原文是读取request body，按照理解应该是读取response可以手工控制)的时间花费可以手工的通过一个time.Timer来实现, 读取发生在调用Client.Do之后（详见下一节）。
最后将一点，在Go 1.7中，增加了一个http.Transport.IdleConnTimeout， 它不控制client request的阻塞阶段，但是可以控制连接池中一个连接可以idle多长时间。
注意一个Client缺省的可以执行 redirect。http.Client.Timeout包含所有的redirect，而细粒度的超时控制参数只针对单次请求有效， 因为http.Transport是一个底层的类型，没有redirect的概念。
Cancel 和 Context
net/http提供了两种方式取消一个client的请求: Request.Cancel以及Go 1.7新加的Context。
Request.Cancel是一个可选的channel, 当设置这个值并且close它的时候，request就会终止，就好像超时了一样(实际它们的实现是一样的，在写本文的时候我还发现一个1.7 的 一个bug, 所有的cancel操作返回的错误还是timeout error )。
我们可以使用Request.Cancel和time.Timer来构建一个细粒度的超时控制，允许读取流数据的时候推迟deadline:

package main
import (  
    "io"
    "io/ioutil"
    "log"
    "net/http"
    "time"
)
func main() {  
    c := make(chan struct{})
    timer := time.AfterFunc(5*time.Second, func() {
        close(c)
    })
        // Serve 256 bytes every second.
    req, err := http.NewRequest("GET", "http://httpbin.org/range/2048?duration=8&chunk_size=256", nil)
    if err != nil {
        log.Fatal(err)
    }
    req.Cancel = c
    log.Println("Sending request...")
    resp, err := http.DefaultClient.Do(req)
    if err != nil {
        log.Fatal(err)
    }
    defer resp.Body.Close()
    log.Println("Reading body...")
    for {
        timer.Reset(2 * time.Second)
                // Try instead: timer.Reset(50 * time.Millisecond)
        _, err = io.CopyN(ioutil.Discard, resp.Body, 256)
        if err == io.EOF {
            break
        } else if err != nil {
            log.Fatal(err)
        }
    }
}
上面的例子中我们为Do方法执行阶段设置5秒的超时，但是我们至少花费8秒执行8次才能读完所欲的body，每一次设置2秒的超时。我们可以为流 API这样处理避免程序死在那里。 如果超过两秒我们没有从服务器读取到数据， io.CopyN会返回net/http: request canceled错误。
在1.7中， context包升级了，进入到标准库中。Context有很多值得学习的功能，但是对于本文介绍的内容来讲，你只需直到它可以用来替换和扔掉Request.Cancel。
用Context取消请求很简单，我们只需得到一个新的Context和它的cancel()函数，这是通过context.WithCancel方法得到的，然后创建一个request并使用Request.WithContext绑定它。当我们想取消这个请求是，我们调用cancel()取消这个Context:

ctx, cancel := context.WithCancel(context.TODO())  
timer := time.AfterFunc(5*time.Second, func() {  
    cancel()
})
req, err := http.NewRequest("GET", "http://httpbin.org/range/2048?duration=8&chunk_size=256", nil)  
if err != nil {  
    log.Fatal(err)
}
req = req.WithContext(ctx)
Context好处还在于如果parent context被取消的时候(在context.WithCancel调用的时候传递进来的)，子context也会取消， 命令会进行传递。
Go net/http 超时机制完全手册，首发于文章 - 伯乐在线。</content>
</doc>
<doc>
	<docid>5</docid>
	<title>6 个月才解决了这个小 Bug</title>
	<link>http://blog.jobbole.com/107030/</link>
	<author></author>
	<content>我曾经为一家美国著名的国防承包商工作过。我很高兴我做到了，因为它是我一直的梦想；但我也很高兴有机会继续前进。我学到了很多，遇到超多了不起的家伙，而且只要我活着，就永远不会缩写另外一个变量。

（图文无关。F-117 夜鹰隐形战机，图来自维基百科）
我效力过一个隐形飞机的项目，这里不提它的名字，它主要负责制造雷达接收器。你可以能会问，“为什么隐形飞机还需要雷达接收器？”这主要有两个原因。这些接收器是定制的，专门用来接收和识别地理位置以及敌方系统发射雷达脉冲的国籍。首先，知道敌方雷达的地理位置，可以帮助你避免意外飞过他们的上空（如果你的影子飞过敌方空军基地上空，却没有被雷达发现，那是极好的）。第二，它让你分析敌方在哪里寻找你（比如在一个山头上出现了一个俄国的防空导弹基地，上个月还没有呢，那你就知道一定有些东西在上面）。
当接收器在测试实验室的时候，你经常会看到 20~30 磅的精美的航空级铝，里面塞满了 10~20个 定制的电子卡，上面运行着价值 50~100 万美元的软件。移动接收器很容易，绝大多数我在实验室里处理的，都是空军部队在使用中遇到某些问题的。本质上，99.9% 的这些问题都是电子问题，我们不得不欺骗这个盒子，让它认为自己飞在空中，并接受敌方的雷达脉冲。测试仪器和软件在当初采购时，可能是很先进的，在合同获准后都维持不变。然而，因为它们是上世纪 90 年代获准的，你可以想象我们20年后还不得不处理这些遗物。以 MHz 计量的古老 CPU，比我还要老的操作系统，软件的用户友好特性根本无从谈起。我工作的一部分是执行新的合同，负责将升级原有测试仪器以及在上面运行的软件。接下来，有趣的事情发生了。
这项宏大的任务有很多层，我负责的一层是在同一个接收器上运行新旧两个测试，看结果有什么不同，找到软件的问题，然后修正它。因为有很多小问题要处理，所以事情进展地相当缓慢但是还算稳定，但是 SlowPOP 给我留下的印象却挥之不去。SlowPOP，也被称为“慢上升时间的脉冲叠加”，这项测试用来保证在接收到两个叠加的雷达脉冲，而且脉冲的上升、下降时间比正常时间要慢很多的情况下，接收器还能正常工作。这些细节不但枯燥而且还属于机密，所以可以这么说，结果相当糟糕。输入参数稍有调整，测试就可以通过……差不多是这样……但是这仍旧不太正常，后面调整的参数和原始参数并不是很接近，这让我很不舒服。
我不停地重启，重新校准，重新安装（软件），重新测试、测试、不停地测试。
我不断地询问，调查，请求，追问，推测。
随后几个月，我们发布了其它可以通过测试的版本。
有趣的是，每当我问到 WaveGenAPI 函数的时候，每个人都说“那个函数不可能有问题，其他 80% 的测试都在使用它，而且它也正常工作一年多了。”
最后在检查所有可能后，我知道必须要检查 WaveGenAPI 了。
当我研究 WaveGenAPI 函数代码时，经过几天的努力，我找到一些线索。有一行代码看上去不太对。它添加了一堆术语，其中有一个术语看上去不太对。我请 WaveGenAPI 的作者（他有令我羡慕的 26 年经验）下到实验室，和我一起看结果。他沉默地盯着那行代码差不多有半个小时，只问了这个问题的基本信息，包含了基本的检查和可能性。最终他只说了一句“做得好”，然后我们握手，他就离开了。
问题找到了：一个文件 → 一行代码 → 一个术语 → 一个变量 → 一个字母
当时，这位资深程序员接受的训练就是要限定变量名的长度，他在实际工作中使用的变量名长度都不会超过 8 个字符。在这个案例中，有问题变量的含义是脉冲“下降沿十分之一高度的时间”。这个术语应该缩写成 “Ttpfe”，但是他错误地把它命名成“Ttpre”，而它却正好代表相反的含义，即“上升沿”。“Ttpre”这个术语也存在了，所以这个拼写错误才不会造成“undefined”错误。而且除了 SlowPOP 以外，所有测试的时间差异都在1个皮秒内（译注：1皮秒等于百万分之一微秒）。发现和修订这个错误，是对我六个月工作的最高嘉奖，并且是迄今为止我职业生涯中找到最让人满意的 bug。
这就是为什么，只要我活着，就永远不会缩写变量名。
简而言之，我花费了 6 个月的时间去查找一个错误的字母，而它是一个比我多 26 年工作经验的工程师所犯的输入错误。
6 个月才解决了这个小 Bug，首发于文章 - 伯乐在线。</content>
</doc>
<doc>
	<docid>6</docid>
	<title>全栈必备：Git</title>
	<link>http://blog.jobbole.com/107027/</link>
	<author></author>
	<content>
为什么使用Git
Git 是 Linus Torvalds 为了帮助管理 Linux 内核开发而开发的一个开放源码的版本控制软件。大神就是大神，在开发了Linux之后，Git 是又一抗鼎之作。这是唯一的理由么？
Git在软件开发中位置——配置管理SCM
Software configuration management (SCM, or just plain CM) is an organizational framework — that is, a discipline — for managing the evolution of computer systems throughout all stages of systems development.
软件配置管理:通过执行版本控制、变更控制的规程，以及使用合适的配置管理软件，来保证所有配置资源的完整性和可跟踪性。配置管理是对工作成果的一种有效保护。没有软件配置管理，最大的麻烦是工作成果无法回溯。
配置管理的内容和目标
配置管理的内容：
一类是属于产品的组成部分，例如需求文档、设计文档、源代码、测试用例等等；
另一类是在管理过程中产生的文档，例如各种计划、报告等
软件配置管理是在贯穿整个软件生命周期中建立和维护项目产品的完整性。它的基本目标包括：
1. 软件配置管理的各项工作是有计划进行的。
2. 被选择的项目产品得到识别，控制并且可以被相关人员获取。
3. 已识别出的项目产品的更改得到控制。
4. 使相关组别和个人及时了解软件基准的状态和内容。
配置管理的主要任务
软件配置管理的主要任务也就归结为以下几条：
（1）制定项目的配置计划；
（2）对配置项进行标识；
（3）对配置项进行版本控制；
（4）对配置项进行变更控制；
（5）定期进行配置审计；
（6）向相关人员报告配置的状态。
版本控制
版本控制是软件配置管理的核心功能。所有位于配置资源库中的元素都应自动予以版本的标识，并保证版本命名的唯一性。版本在生成过程中，自动依照设定的使用模型自动分支、演进。
版本控制(Revision control)确保由不同人所编辑的同一档案都得到更新。
版本控制中的基本概念
1）签入，提交，检出
2）冲突，解决，合并
3）分支，版本
4）锁定，hook
常见的版本控制工具
作为一个老码农，枚举一下曾经使用过的版本控制工具。
1. VSS： visual source safe, 微软的东东，97年写VC程序时使用，人多的时候性能较差，不知道现在的升级版怎样了
2. clearcase： 99年开发Unix 上分布式式应用时使用，功能强大，不只限于版本控制，有钱的大团队才去用
3. CVS: 02年在互联网热潮的时候使用，开源产品，当时“Copy-Modify-Merge”开发模型眼前一亮。
4. SVN：曾经的挚爱，在曾工作的合资公司使用，权限管理和分支合并等方面做的很出色，并在多个公司推广使用。还记得TortoiseSVN么?那只可爱的小乌龟。
5. perforce:是一款具有轻便快速的SCM工具、真正的客户端/服务器系统等特点的商业软件。高通内部使用的版本管理工具。确实不错。
6. git：现在的最爱……
比较一下cvs,svn,和git：

Git 简要
GIT 是一款免费的、开源的、分布式的版本控制系统。每一个GIT克隆都是一个完整的文件库，含有全部历史记录和修订追踪能力。其最大特色就是“分支”及“合并”操作快速、简便。支持离线工作，GIT是整个项目范围的原子提交，而且GIT中的每个工作树都包含一个具有完整项目历史的仓库。
核心特点：

Git 底层自行维护的存储文件系统：存储的是文件快照，即整个文件内容，并保存指向快照的索引
去中心化的分布式控制

优缺点：
优点：

适合分布式开发，强调个体。
公共服务器压力和数据量都不会太大， 速度快、灵活。
任意两个开发者之间可以很容易的解决冲突。
离线工作。

缺点：

学习周期相对而言比较长。
不符合常规思维。
代码保密性差，一旦开发者把整个库克隆下来就可以完全公开所有代码和版本信息。

Git 原理
Git的目录结构
不论通过git init 还是克隆下来的git 仓库，都有如下的目录结构：

主要目录结构描述见下表：



子目录名
简要描述


branches
Git 项目分支信息，新版 Git 已经不再使用该目录


config
Git 项目配置信息


description
Git 项目描述信息


HEAD
指向 Git 项目当前分支的头指针


hooks
默认的”hooks”脚本，被特定事件发生前后触发。


info
里面含一个 exclude 文件，指 Git 项目要忽略的文件。


objects
Git 的数据对象，包括：commits, trees, blobs, tags。


refs
指向所有 Git 项目分支的指针



所有的分支指针保存在 .git/refs/heads 目录下，HEAD 在 .git/HEAD 目录下，标签在 .git/refs/tags 目录下。
快照
例如： 一个工程中有两个文件A和B， 有3个版本:
V1.0 A和B，V1.5 A1和B，V2.0 A1和B1
在Git 的实际存储中实际存了3个快照 4个文件。
Git对文件进行 SHA-1 计算作为文件的唯一ID，并校验了文件完整性。
SHA-1 算法将文件中的内容通过计算生成一个 40 位的 Hash 值。SHA-1 算法的特点：
由文件内容计算出的 Hash 值；Hash 值相同，文件内容相同。

使用 SHA-1 的前两位创建了文件夹，剩下的 38 位为文件名。
这些 Obj 文件，其实分为四种类型，分别是 Blob、Tree、Commit、Tag。
Blob
用来存放项目文件的内容，但是不包括文件的路径、名字、格式等其它描述信息。项目的任意文件的任意版本都是以 Blob 的形式存放的。
Tree
Tree 用来表示目录。我们知道项目就是一个目录，目录中有文件、有子目录。因此 Tree 中有 Blob、子 Tree，且都是使用 SHA-1值引用的。这是与目录对应的。从顶层的 Tree 纵览整个树状的结构，叶子结点就是 Blob，表示文件的内容，非叶子结点表示项目的目录，顶层的 Tree 对象就代表了当前项目的快照。
Commit
表示一次提交，有 Parent 字段，用来引用父提交。指向了一个顶层 Tree，表示了项目的快照，还有一些其它的信息，比如上一个提交 Committer、Author、Message 等信息。
存储区
Git中有4个类型的存储区：远程仓库，工作区，本地仓库和缓存区。
暂存区的好处：

为了能够实现部分提交
为了不在工作区创建状态文件、会污染工作区。
暂存区记录文件的修改时间等信息，提高文件比较的效率。
暂存区是用来构建项目快照的区域。暂存区是一个文件，路径为： .Git/index


它是一个二进制文件，第四列是文件名，第三列是文件的冲突状态，第二列指的是文件的 Blob。
Commit 命令，将暂存区的内容永久保存到本地仓库。提交时 Git 会使用暂存区的这些信息生成 Tree 对象，也就是项目快照，永久保存到数据库中。
文件的状态可以分为两类。一类是暂存区与本地仓库比较得出的状态，另一类是工作区与暂存区比较得出的状态。为什么要分成两类的愿意也很简单，因为第一类状态在提交时，会直接写入本地仓库。而第二种则不会。一个文件可以同时拥有两种状态。
分支
分支的目的是让我们可以并行的进行开发。 .Git/HEAD 文件，它保存了当前的分支。

分支指向了一次提交，也是 Git 中的分支为什么这么轻量的原因。
因为分支就是指向了一个 Commit 的指针，当提交新的 Commit，这个分支的指向只需要跟着更新就可以了，而创建分支仅仅是创建一个指针。
Git 必备技能
常见命令速查

git add 和 git commit
Add 操作是将修改保存到暂存区，Commit 是将暂存区的内容永久保存到本地仓库。
每当将修改的文件加入到暂存区，Git 都会根据文件的内容计算出 SHA-1，并将内容转换成 Blob，写入数据库。然后使用 SHA-1 值更新该列表中的文件项。
在暂存区的文件列表中，每一个文件名，都会对应一个 SHA-1 值，用于指向文件的实际内容。最后提交的那一刻，Git 会将这个列表信息转换为项目的快照，也就是 Tree 对象。写入数据库，并再构建一个 Commit 对象，写入数据库。然后更新分支指向。
分支合并: merge 和rebase
冲突的状态

DELETED_BY_THEM;
DELETED_BY_US;
BOTH_ADDED;
BOTH_MODIFIED

遇到不可自动合并冲突时，Git 会将这些状态写入到暂存区。
merge
在解决完冲突后，我们可以将修改的内容提交为一个新的提交。这就是 Merge。
Merge 之后仍可以做出新的提交。

rebase
Rebase 会把从 Merge Base 以来的所有提交，以补丁的形式一个一个重新达到目标分支上。这使得目标分支合并该分支的时候会直接 Fast Forward，即不会产生任何冲突。

Rebase 主要在 .Git/Rebase-Merge 下生成了两个文件，分别为 Git-Rebase-todo 和 Done 文件，Git-Rebase-todo 存放了 Rebase 将要操作的 Commit。而 Done 存放正在操作或已经操作完毕的 Commit。
Rebase 的一个缺点，那就是修改了分支的历史提交。如果已经将分支推送到了远程仓库，会导致无法将修改后的分支推送上去，必须使用 -f 参数（Force）强行推送。
所以使用 Rebase 最好不要在公共分支上进行操作。
checkout
经常用来切换分支、或者切换到某一次提交。
Checkout 找到目标提交（Commit），目标提交中的快照也就是 Tree 对象就是我们要检出的项目版本。
Checkout 首先根据Tree生成暂存区的内容，再根据 Tree 与其包含的 Blob 转换成我们的项目文件。然后修改 HEAD 的指向，表示切换分支。
Checkout 并没有修改提交的历史记录。只是将对应版本的项目内容提取出来。
revert
revert 实现了反向提交，就是旧版本添加了的内容，要在新版本中删除；旧版本中删除了的内容，要在新版本中添加。这在分支已经推送到远程仓库的情境下非常有用。
Revert 也不会修改历史提交记录，实际的操作相当于是检出目标提交的项目快照到工作区与暂存区，然后用一个新的提交完成版本的“回退”。
reset
在当前分支进行版本的“回退”，Reset 是会修改历史提交记录的。
Reset 常用的选项有三个，分别是 —Soft, —Mixed, —Hard。他们的作用域依次增大。
Soft 会仅仅修改分支指向。而不修改工作区与暂存区的内容，
Mixed 比 Soft 的作用域多了一个 暂存区。实际上 Mixed 选项与 Soft 只差了一个 Add 操作。
Hard 会比 Mixed作用域又多了一个工作区。
注意：在丢失后可以使用 Git Reset –Hard ORIG_HEAD 立即恢复，或者使用 reflog 命令查看之前分支的引用
stash
有时，在一个分支上做了一些工作，修改了很多代码，而这时需要切换到另一个分支干点别的事。但又不想将只做了一半的工作提交。
Stash 将工作区与暂存区中的内容做一个提交，保存起来，然后使用Reset Hard 选项恢复工作区与暂存区内容。我们可以随时使用 Stash Apply 将修改应用回来。
Stash 实现思路将我们的修改提交到本地仓库，使用特殊的分支指针（.Git/refs/Stash）引用该提交，然后在恢复的时候，将该提交恢复即可。
Git 典型实践
一个典型的git 并行开发的流程模型如下：

主要分支
把origin/master作为主要分支，源码的HEAD总是表示production-ready(可随时部署)状态。

origin/develop上的代码是为下一次的代码发布准备的。每日构建也是基于此分支。
当develop分支达到了一个稳定状态并准备发布时，所有的改变都要合并到master分支，并标上版本号。
辅助分支
Feature branches
继承与合并都与develop 分支相关，用来开发新特性的(短期，远期都可以)。

当要创建一个新特性时，从develop分支上再创建一个 Feature branch。
$ git checkout -b myfeature develop
合并feature 到develop$ git checkout develop
Switched to branch 'develop'
$ git merge --no-ff myfeature
Updating ea1b82a..05e9557 (Summary of changes)
$ git branch -d myfeature
Deleted branch myfeature (was 05e9557).
$ git push origin develop
Release branches
继承分支: develop
合并分支：develop 和 master
命名规范：release-*
创建一个release 分支
Release branch是通过develop分支而创建.$ git checkout -b release-1.2 develop    
Switched to a new branch "release-1.2"

$ ./bump-version.sh 1.2
Files modified successfully, version bumped to 1.2.

$ git commit -a -m "Bumped version number to 1.2"
[release-1.2 74d9424] Bumped version number to 1.2
1 files changed, 1 insertions(+), 1 deletions(-)
完成一个release 分支
当release branch已经准备就绪，需要做几件事。

release分支被合并到master分支上(每一个提交到master上的commit都是一个新版 本，切记)。
master上的commit都要添加tag，方便将来查看和回滚。
release上所做的修改必须合并到develop分支上，保证bug已被修补。
前两个步骤：

$ git checkout master
Switched to branch 'master'
$ git merge --no-ff release-1.2
Merge made by recursive.
(Summary of changes)
$ git tag -a 1.2为了把release上的改变保存到develop，需要合并到develop。$ git checkout develop
Switched to branch 'develop'
$ git merge --no-ff release-1.2
Merge made by recursive.
(Summary of changes)这个步骤可能会导致冲突，如果这样的话，解决冲突，然后再提交。
最后，可以删除release 分支。$ git branch -d release-1.2
Deleted branch release-1.2 (was ff452fe).
Hotfix branches
继承分支: master
合并分支：develop 和 master
命名规范：hotfix-*
运行过程中发现了bug，就必须快速解决，这时就可以创建一个Hotfix branch，解决完后合并到master分支上。好处是开发人员可以继续工作，有专人来负责搞定这个bug。
创建hotfix分支
$ git checkout -b hotfix-1.2.1 master
Switched to a new branch "hotfix-1.2.1"
$ ./bump-version.sh 1.2.1
Files modified successfully, version bumped to 1.2.1.
$ git commit -a -m "Bumped version number to 1.2.1"
[hotfix-1.2.1 41e61bb] Bumped version number to 1.2.1
1 files changed, 1 insertions(+), 1 deletions(-)
fix bug, 解决问题
需要一次或几次commit$ git commit -m "Fixed severe production problem"
[hotfix-1.2.1 abbe5d6] Fixed severe production problem
5 files changed, 32 insertions(+), 17 deletions(-)
完成Hotfix branch
当结束时，bugfix要被合并到master，同时也要合并到develop，保证下个版本发布时该bug已被修复。这跟release branch完成时一样。
首先更新master和tag release$ git checkout master
Switched to branch 'master'
$ git merge --no-ff hotfix-1.2.1
Merge made by recursive.
(Summary of changes)
$ git tag -a 1.2.1接下来与develop合并$ git checkout develop
Switched to branch 'develop'
$ git merge --no-ff hotfix-1.2.1
Merge made by recursive.
(Summary of changes)有一个例外，就是当一个release branch存在时，bugfix要被合并到release而不是develop，因为release最终会被合并到develop。
最后移除branch
$ git branch -d hotfix-1.2.1 
Deleted branch hotfix-1.2.1 (was abbe5d6).
总结
了解Git 在软件工程及敏捷开发中的地位，明白git与其他版本控制工具之间的区别，掌握Git 工作的基本原理和必备操作，复杂问题可以查找git的相关命令，应用git开发的流程模型，让Git 成为我们的真正利器。
参考资料：
1）http://nvie.com/posts/a-successful-git-branching-model/
2）https://community.qingcloud.com/topic/457/%E6%8A%80%E6%9C%AF%E5%9F%B9%E8%AE%AD-git-%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BC%9A%E7%94%A8%E4%B9%88/2

全栈必备：Git，首发于文章 - 伯乐在线。</content>
</doc>
<doc>
	<docid>7</docid>
	<title>后端成长之路：从菜鸟到架构</title>
	<link>http://blog.jobbole.com/107025/</link>
	<author></author>
	<content>
有不少初学者问到，我想学习后端，但是又不知道该怎么学，所以我决定把这几年的经验和经历整理成一篇文章，分析后端的路该怎么走，先说明下面仅仅是个人心得，也许与外面的理论有所不同。（文章最后面会附上学习路线地图和一些自己看过的书籍）

后端任务其实是实现接收输入响应输出

后端初接触
后端首先需要学习的是html，css和js，也许你会问做网页不是前端做的事情吗？答案是对于真实开发环境后端很多时候还兼顾了前端这个角色，对于架构来说优化不仅仅要考虑后端还需要考虑前端。
要学到什么样的程度才可以学下一样技能呢？我的建议是能够搭建起一个简单的页面，最重要的是学会dom操作和ajax。
语言选择
我相信过来人或者正在入门的人都碰到过这个世纪难题，如果你是初学者我建议您使用java。这里并不是说我喜欢java才推荐你们用java入门，而是java的思想对我们以后开发有很不错的启发。当你学会一种语言的基本语法就可以到下一步了（仅仅是语法，任意语言）。
hello world！
没错，这个时候，你得让浏览器可以看到hello world！ 为了实现这个小目标，我们就需要根据语言配置不同的运行环境。由于入门，我建议可以使用一些集成的一键配置环境软件，例如php可以用phpstudy，java看看教程，下载一个tomcat然后跟eclipse集成就好，而python或则ruby则使用一个轻量级的web框架，copy一下官网提供的hello world！例子即可。
高级一些的hello world
这个阶段，你需要实现的是通过浏览器输入不同的网址，你可以输出不同的内容。然后要弄明白什么是get参数，什么是post参数。然后要实现根据不同get参数或者post参数输出不同内容。因为后端研发从根本上来说就是处理这些不同输入，输出特定的内容回去而已，所以这一步，最关键。
数据库
为什么我们要用数据库呢？以一个新闻网站为例，它要保存新闻数据，还得保存用户信息，也要保存评论信息。也许你会问用文件保存就好了，为何还要学习数据库这么庞大的东西呢？首先，我们来看一下什么是数据库，我们可以把数据库每一个table看成excel的sheet，每一个db就是一个excel文件。而sql语句就是一条条指令，可以帮我们操作里面的数据，可以节省我们大量的操作。开始学习数据库的时候，我建议学习mysql。

数据库

模板引擎
一项技术的出现，往往在于解决了一些开发难题。在没有模板引擎之前，我们得使用字符串拼接方式，弄成html字符串，但是这种方式很容易出错的。而模板引擎无需字符串拼接，并且可以处理字符串转义问题，并且模板集成，模板引用等功能，可以使代码可以重用，大大提高了开发效率，工程也更容易维护。

模板引擎

MVC框架
首先，我们得明白为什么要用框架？假如是第一次接触，我相信第一感受是用了框架加重自己的开发负担。然后框架存在必然有自己的合理性，我们来看看框架有什么优点。
1. 框架提供了大量的方法封装，我们可以直接调用来解决业务问题，这可以大大提高开发效率。
2. 框架能够帮我们分层编写代码，能够在后期更好的维护我们的项目。
3. 做一些安全防护，处理一些常见的攻击。
所以对于各种语言，大家可以选择一个对应的框架来学习。那么我们应该如何挑选一个框架呢？
1. 多人用的（最起码使用过程中遇到问题，都能找到人问）
2. 文档详细的（文档不详细，怎么去学习呢？）
3. 简单的（未来的趋势估计都是往轻量级框架走的）

MVC框架有利于维护

Linux与应用部署
开发用window没什么问题，但是我们做的应用绝大部分是要放在linux系统来对外提供服务器，所以不懂linux的操作是不行的，并且服务器为了尽可能节省资源，都是不使用界面的，所以我们要学好大量的linux命令。
服务器环境我们一般就不会用外面的通用的一键安装环境，往往需要优化的。所以基本的软件安装和配置需要学会，特别是近年来很火的docker，越来越多都是基于docker来部署的啦。
缓存
缓存可是处理高并发的万能药，每当你写的功能慢的时候，基本上你都可以通过缓存来大大提高网站性能。实现缓存的方法很简单，但是一当在并发下和海量数据面前一切都不容易，还得上分布式集群这些。
http协议
如果上面的你都接触了，恭喜您普通开发估计是没问题了，如果你说很多需求还是做不出，没关系，其实你只是缺乏开发经验而已，只要多写多请教，你就会发现，其实后端研发也没什么难度。这个时候，如果我们想往高级研发工程师方向走，一些底层的高级的东西我们得学会。

http协议

浏览器与服务器通过http协议交互，其实就是相互之间传递一串特定格式的字符串。get参数，post参数，url，和cookie等信息其实都包含在这字符串里面。我们平时虽然没有怎么碰到这些底层的东西，其实是框架和tomcat这种软件帮我们处理了。
就像平时一个重定向，其实就在header里面有一句Location: XXX。而我们平时声明返回的是html还是json其实是headers有一句Content-Type:XXX，一切神奇的功能其实仅仅是一段特定的字符串而已，所以http协议怎么可以不研究呢。
nosql
非关系型数据库有很多，例如memcached，redis，mongodb，coundb等。这些东西都有自己的适用场景，合理利用可以加快开发效率，对提高应用性能有很大的帮助的。
消息中间件
这种技术的出现是为了解耦，当我们业务过于庞大的时候，就会相应的拆分成几个小系统，系统之间的通讯往往是通过http协议调用和基于消息中间件。通过http协议有一个弊端，就是某个系统一旦修改了地址，那么就必须修改调用该服务的系统。如果通过消息中间件调用，那么应用如何迁移，也不会影响到别的系统。

消息中间件

一点点话
你跟所谓的大牛差距在哪里呢？我的看法是差距仅仅在于项目经验。也许你会说后端怎么需要学习这么多东西，并且年年都有新的技术出现怎么学呢？其实对于一种新技术，我们主要是要弄明白我们为什么要用这种技术，因为存在即合理，新技术的出现必然有自己的原因，所以新技术不一定要追求，但是底层建议要理解。然后什么海量数据和高并发下系统优化，一个原则，找到性能瓶颈并解决它而已。
附录
路线图

一个简单的学习路线图

书单（下面仅仅是一些个人以前看过的书籍，并不是推荐要看，个人建议看看哪种讲解核心的书籍，怎么制作比怎么用更总要）
LINUX操作系统(第2版)
LINUX指令范例速查手册
JSP & SERVLET学习笔记
STRUTS 2．X权威指南(第3版)
HTML 5从入门到精通
JAVA入门经典(第6版)
看透Spring MVC：源代码分析与实践
大型分布式网站架构设计与实践
Docker 容器与容器云
Python Cookbook（第3版）中文版
Go语言程序设计
MongoDB大数据处理权威指南（第2版）
NoSQL数据库技术实战
构建高可用Linux服务器（第3版）
大型网站技术架构 核心原理与案例分析
Linux运维之道
高性能MySQL（第3版）
Java核心技术 卷I
Java核心技术 卷II

后端成长之路：从菜鸟到架构，首发于文章 - 伯乐在线。</content>
</doc>
<doc>
	<docid>8</docid>
	<title>一些实用的 Laravel 小技巧</title>
	<link>http://blog.jobbole.com/107018/</link>
	<author></author>
	<content>Laravel 中一些常用的小技巧，额，说不定你就用上了。。。
1.侧栏
网站一般都有侧栏，用来显示分类，标签，热门文章，热门评论啥的，但是这些侧栏都是相对独立的模块，如果在每一个引入侧栏的视图中都单独导入与视图有关的数据的话，未免太冗余了。。。所以最佳的做法是：新建一个widgets视图文件夹，再利用Laravel 的ViewComposers单独为侧栏绑定数据，这样侧栏就可以随便引入而不用关心数据是否绑定啦~~~
举个栗子？拿最常用的分类侧栏来说，在resources/views/widgets下新建你的分类侧栏视图文件categories.blade.php：


<div class="widget widget-default">
    <div class="widget-header"><h6><i class="fa fa-folder fa-fw"></i>分类</h6></div>
    <ul class="widget-body list-group">
        @forelse($categories as $category)
            @if(str_contains(urldecode(request()->getPathInfo()),'category/'.$category->name))
                <li href="{{ route('category.show',$category->name) }}"
                    class="list-group-item active">
                    {{ $category->name }}
                    <span class="badge">{{ $category->posts_count }}</span>
                </li>
            @else
                <a href="{{ route('category.show',$category->name) }}"
                   class="list-group-item">
                    {{ $category->name }}
                    <span class="badge">{{ $category->posts_count }}</span>
                </a>
            @endif
        @empty
            <p class="meta-item center-block">No categories.</p>
        @endforelse
    </ul>
</div>


新建app/Http/ViewComposers文件夹，然后创建CategoriesComposer.php：<?php
namespace App\Http\ViewComposers;
use App\Http\Repositories\CategoryRepository;
use Illuminate\View\View;
class CategoriesComposer
{
    public function __construct(CategoryRepository $categoryRepository)
    {
        $this->categoryRepository = $categoryRepository;
    }

    public function compose(View $view)
    {
        $categories = $this->categoryRepository->getAll()->reject(function ($category) {
            return $category->posts_count == 0;
        });
        $view->with('categories', $categories);
    }
}再在app/Providers文件夹下新建ComposerServiceProvider.php文件：<?php
namespace App\Providers;
use Illuminate\Support\ServiceProvider;
use Illuminate\Support\Facades\View;
class ComposerServiceProvider extends ServiceProvider
{

    public function boot()
    {
        View::composer('widget.categories', 'App\Http\ViewComposers\CategoriesComposer');
    }

    public function register(){}
}最后别忘了在config/app.php中的providers数组中添加AppProvidersComposerServiceProvider::class啊。好了，现在你可以随时随地@include('widget.categories')了。对了，要善于在ViewComposer中利用Collection的强大方法进行数据处理幺~~
2.善用路由别名
Laravel 最让人喜欢的地方之一是可以给路由起一个别名，比如：Route::get('user/profile', 'UserController@showProfile')->name('user.profile');
// 等价于：
Route::get('user/profile', ['uses' => 'UserController@showProfile' , 'as' => 'user.profile']);;然后，就可以在试图中就可以使用route()方法引用了：// 例如：
<a href="{{ route('user.profile') }}">lufficc</a>因为一个普通的项目路由至少也得有几十个，如果使用url()方法的话，你不但要记住具体的路由，更麻烦的是如果你将来想要改变某个路由（比如把'user/profile'改为'u/profile'，或者加个前缀啥的），必须改变所有相关的视图文件，这。。。这。。。不敢相信，而使用命名路由的话，只要命名不变，毫不受影响。
所以视图文件中尽量避免使用url()方法，为每一个路由命名，一个默认的命名规则为：资源名称.或者，如post.show，image.upload。
3.全局动态设置
仅仅是.env的配置还无法满足我们的需求，有时我们需要可以在后台动态的进行一些设置，比如网站的标题，网站的背景图片或者是否允许评论等等。那么实现这个的最佳实践是什么？
熟悉wordpress的同学知道，wordpress可以进行很多自定义，因为wordpress有一张键值对数据库表，它就是靠这个实现个性化的。因此我们也可以参考这种思路，增加一个键值对表，以Xblog为例子，新建一个maps表：Schema::create('maps', function (Blueprint $table) {
       $table->increments('id');
       $table->string('key')->unique();
       $table->string('tag')->index();
       $table->text('value')->nullable(true);
});maps表的作用就是实现键值对key-value存储，tag的是为了可以有一个分类。然后后台进行存储的话，不要写死，这样就可以随时在变单中添加设置而无需更改代码：$inputs = $request->except('_token');
foreach ($inputs as $key => $value) {
            $map = Map::firstOrNew([
                'key' => $key,
            ]);
            $map->tag = 'settings';
            $map->value = $value;
            $map->save();
}注意firstOrNew的用法：如果不存在这个选项我们就新增一个并保存，否则就更新它。然后我们就可以在视图中随便增加任意多个表单了（或者也可以用js动态生成表单）。有了数据，怎么在视图中利用呢？利用ViewComposer，新建一个SettingsComposer.php，然后将查询的数据以数组的形式传递给视图：//在SettingsComposer.php的compose方法中绑定数据
public function compose(View $view)
{
    $settings = Map::where('tag', 'settings')->get();
    $arr = [];
    foreach ($settings as $setting) {
      $arr[$setting->key] = $setting->value;
    }
   $view->with($arr);
}然后就可以在视图中随便引用了，如你表单新增加了一个description<input type="text" name="description" value="{{ $description or ''}}">然后就可以在任何视图引用了:{{ $description or ''}}。另外还可以绑定一个单例Facades到容器，这样就可以在代码中随时获取配置信息啦~~~
比如：//1.注册
public function register()
{
    $this->app->singleton('XblogConfig', function ($app) {
       return new MapRepository();
   });
}
//2.注册Facade
class XblogConfig extends Facade
{
    public static function getFacadeAccessor()
    {
        return 'XblogConfig';
    }
}
//3.添加到aliases数组

'aliases' => [

        *****************  省略  *************************
        'XblogConfig' => App\Facades\XblogConfig::class,
    ],

//4.愉快的使用，可爽
$page_size = XblogConfig::getValue('page_size', 7);
4.数据库查询
怎么统计一篇文章有多少评论？最快的方法是：$post = Post::where('id',1)->withCount('comments')->first();这样$post变量就有一个属性comments_count了：$post->comments_count;如果想获取点赞数大于的100的评论个数怎么办？这样：$post = Post::where('id',1)->withCount('comments',function($query){
       $query->where('like', '>', 100);
   })->first();简单吧~~
5.多态关联
文章可以有评论，页面可以有评论，评论也可以有评论，但是总不能建三张评论表吧？如果自己写条件判断也太麻烦了吧。。。Laravel的多态关联上场了！！//1.第一步在Comment模型中说明我是可以多态的
public function commentable()
{
    return $this->morphTo();
}

//2.在想要评论的模型中增加comments方法，
public function comments()
{
    return $this->morphMany(Comment::class, 'commentable');
}

//3.使用，就像普通的一对多关系一样：
$model->comments;原理很简单，comments表中增加两个列就行：Schema::create('comments', function (Blueprint $table) {
     ***************省略*******************
     $table->morphs('commentable');
     //等价于
     $table->integer('commentable_id')->index();
     $table->string('commentable_type')->index();
    ****************省略******************
});然后 laravel 会自动维持这些关系。注意，保存的评论的时候是有小技巧的，你的表单中至少要传两个参数：commentable_id和commentable_type：$comment = new Comment();

$commentable_id = $request->get('commentable_id');
//commentable_type取值例如：AppPost，AppPage等等
$commentable = app($request->get('commentable_type'))->where('id', $commentable_id)->firstOrFail();

****************省略******************

$commentable->comments()->save($comment);保存评论的时候并不知道是谁的评论，而是使用容器根据commentable_type生成一个模型实例，这样也就和具体的模型解耦了，你可以让任何东西可以评论，而不需要修改代码。
6.缓存优化相关
如果你想要在.env文件中添加自己的配置，记住一定要在config文件夹下某个配置文件的数组中添加对应的。记住，除了config文件夹下的配置文件，永远不要在其它地方使用env函数，因为部署到线上时，配置文件缓存（php artisan config:cache）后，env函数无法获得正确的值。
另外注意的是，路由文件中尽量不使用闭包函数，统一使用控制器，因为缓存路由的时候php artisan route:cache，无法缓存闭包函数。
7.Redis
如果你缓存使用Redis，session也使用了Redis，队列已使用了Redis，这样没问题，速度很快，但是！！当你运行php artisan cache:clear清除缓存时，会把你的登录信息清除，也会把队列清除。。。这就不优雅了。解决办法很简单，为它们分配不同的连接即可。
首先在configdatabase.php中增加连接，注意database序号：'redis' => [

        'cluster' => false,

        'default' => [
            'host' => env('REDIS_HOST', 'localhost'),
            'password' => env('REDIS_PASSWORD', null),
            'port' => env('REDIS_PORT', 6379),
            'database' => 0,
        ],
        'session' => [
            'host' => env('REDIS_HOST', 'localhost'),
            'password' => env('REDIS_PASSWORD', null),
            'port' => env('REDIS_PORT', 6379),
            'database' => 1,
        ],
        'queue' => [
            'host' => env('REDIS_HOST', 'localhost'),
            'password' => env('REDIS_PASSWORD', null),
            'port' => env('REDIS_PORT', 6379),
            'database' => 2,
        ],

    ],然后分别为session和queue更换连接：//queue.php中的connections数组中：
'redis' => [
            'driver' => 'redis',
            'connection' => 'queue',
            'queue' => 'default',
            'retry_after' => 90,
        ],

//session.php中的connection选项：
'connection' => 'session',这样他们就互不相干了~~
以上经验来自Xblog，示例均可以在Xblog找到
一些实用的 Laravel 小技巧，首发于文章 - 伯乐在线。</content>
</doc>
<doc>
	<docid>9</docid>
	<title>深入Go UDP编程</title>
	<link>http://blog.jobbole.com/107004/</link>
	<author></author>
	<content>用户数据报协议（User Datagram Protocol，缩写为UDP），又称用户数据报文协议，是一个简单的面向数据报(package-oriented)的传输层协议，正式规范为RFC 768。UDP只提供数据的不可靠传递，它一旦把应用程序发给网络层的数据发送出去，就不保留数据备份（所以UDP有时候也被认为是不可靠的数据报协议）。UDP在IP数据报的头部仅仅加入了复用和数据校验。
由于缺乏可靠性且属于非连接导向协议，UDP应用一般必须允许一定量的丢包、出错和复制粘贴。但有些应用，比如TFTP，如果需要则必须在应用层增加根本的可靠机制。但是绝大多数UDP应用都不需要可靠机制，甚至可能因为引入可靠机制而降低性能。流媒体（流技术）、即时多媒体游戏和IP电话（VoIP）一定就是典型的UDP应用。如果某个应用需要很高的可靠性，那么可以用传输控制协议（TCP协议）来代替UDP。
由于缺乏拥塞控制（congestion control），需要基于网络的机制来减少因失控和高速UDP流量负荷而导致的拥塞崩溃效应。换句话说，因为UDP发送者不能够检测拥塞，所以像使用包队列和丢弃技术的路由器这样的网络基本设备往往就成为降低UDP过大通信量的有效工具。数据报拥塞控制协议（DCCP）设计成通过在诸如流媒体类型的高速率UDP流中，增加主机拥塞控制，来减小这个潜在的问题。
典型网络上的众多使用UDP协议的关键应用一定程度上是相似的。这些应用包括域名系统（DNS）、简单网络管理协议（SNMP）、动态主机配置协议（DHCP）、路由信息协议（RIP）和某些影音流服务等等。
UDP报头

IPv4伪头部
IPv6伪头部
以上大段的背景介绍引自维基百科。
而TCP是面向连接(connection-oriented)的协议，可以提供可靠的数据传输。
本文讲介绍Go语言的UDP库及其使用方法，以及了解使用过程中的细节和陷阱。
一个简单的例子
首先看一个简单的UDP的例子，这个例子演示了Go UDP通过Dial方式发送数据报的例子。

package main
import (
	"fmt"
	"net"
)
func main() {
	listener, err := net.ListenUDP("udp", &net.UDPAddr{IP: net.ParseIP("127.0.0.1"), Port: 9981})
	if err != nil {
		fmt.Println(err)
		return
	}
	fmt.Printf("Local: <%s> \n", listener.LocalAddr().String())
	data := make([]byte, 1024)
	for {
		n, remoteAddr, err := listener.ReadFromUDP(data)
		if err != nil {
			fmt.Printf("error during read: %s", err)
		}
		fmt.Printf("<%s> %s\n", remoteAddr, data[:n])
		_, err = listener.WriteToUDP([]byte("world"), remoteAddr)
		if err != nil {
			fmt.Printf(err.Error())
		}
	}
}
package main
import (
	"fmt"
	"net"
)
func main() {
	sip := net.ParseIP("127.0.0.1")
	srcAddr := &net.UDPAddr{IP: net.IPv4zero, Port: 0}
	dstAddr := &net.UDPAddr{IP: ip, Port: 9981}
	conn, err := net.DialUDP("udp", srcAddr, dstAddr)
	if err != nil {
		fmt.Println(err)
	}
	defer conn.Close()
	conn.Write([]byte("hello"))
	fmt.Printf("<%s>\n", conn.RemoteAddr())
}
可以看到, Go UDP的处理类似TCP的处理，虽然不像TCP面向连接的方式ListenTCP和Accept的方式建立连接,但是它通过ListenUDP和ReadFromUDP可以接收各个客户端发送的数据报，并通过WriteToUDP写数据给特定的客户端。
我们稍微修改一下client1.go,让它保持UDP Socket文件一直打开：
func main() {
	ip := net.ParseIP("127.0.0.1")
	srcAddr := &net.UDPAddr{IP: net.IPv4zero, Port: 0}
	dstAddr := &net.UDPAddr{IP: ip, Port: 9981}
	conn, err := net.DialUDP("udp", srcAddr, dstAddr)
	if err != nil {
		fmt.Println(err)
	}
	defer conn.Close()
	b := make([]byte, 1)
	os.Stdin.Read(b)
	conn.Write([]byte("hello"))
	fmt.Printf("<%s>\n", conn.RemoteAddr())
}
使用 netstat可以看到这个网络文件描述符(因为我在同一台机器上运行服务器，所以你会看到两条记录，一个是服务器打开的，一个是客户端打开的)。
udp4       0      0  localhost.54676        localhost.9981
udp4       0      0  localhost.9981         *.*
或者使用lsof命令查看：
server1   59312 smallnest    3u  IPv4 0xad793a9a54467f61      0t0  UDP localhost:9981
client1   59323 smallnest    3u  IPv4 0xad793a9a544681c1      0t0  UDP localhost:54676->localhost:9981
更复杂的例子
我们还可以将上面的例子演化一下，实现双向的读写。
服务器端代码不用修改，因为它已经实现了读写，读是通过listener.ReadFromUDP,写通过listener.WriteToUDP。
客户端修改为读写：
package main
import (
	"fmt"
	"net"
)
func main() {
	ip := net.ParseIP("127.0.0.1")
	srcAddr := &net.UDPAddr{IP: net.IPv4zero, Port: 0}
	dstAddr := &net.UDPAddr{IP: ip, Port: 9981}
	conn, err := net.DialUDP("udp", srcAddr, dstAddr)
	if err != nil {
		fmt.Println(err)
	}
	defer conn.Close()
	conn.Write([]byte("hello"))
	data := make([]byte, 1024)
	n, err := conn.Read(data)
	fmt.Printf("read %s from <%s>\n", data[:n], conn.RemoteAddr())
}
这里client的写是Write,读是Read。
等价的客户端和服务器
下面这个是两个服务器通信的例子，互为客户端和服务器，在发送数据报的时候，我们可以将发送的一方称之为源地址，发送的目的地一方称之为目标地址。
package main
import (
	"fmt"
	"net"
	"os"
	"time"
)
func read(conn *net.UDPConn) {
	for {
		data := make([]byte, 1024)
		n, remoteAddr, err := conn.ReadFromUDP(data)
		if err != nil {
			fmt.Printf("error during read: %s", err)
		}
		fmt.Printf("receive %s from <%s>\n", data[:n], remoteAddr)
	}
}
func main() {
	addr1 := &net.UDPAddr{IP: net.ParseIP("127.0.0.1"), Port: 9981}
	addr2 := &net.UDPAddr{IP: net.ParseIP("127.0.0.1"), Port: 9982}
	go func() {
		listener1, err := net.ListenUDP("udp", addr1)
		if err != nil {
			fmt.Println(err)
			return
		}
		go read(listener1)
		time.Sleep(5 * time.Second)
		listener1.WriteToUDP([]byte("ping to #2: "+addr2.String()), addr2)
	}()
	go func() {
		listener1, err := net.ListenUDP("udp", addr2)
		if err != nil {
			fmt.Println(err)
			return
		}
		go read(listener1)
		time.Sleep(5 * time.Second)
		listener1.WriteToUDP([]byte("ping to #1: "+addr1.String()), addr1)
	}()
	b := make([]byte, 1)
	os.Stdin.Read(b)
}
Read和Write方法集的比较
前面的例子中客户端有时使用DialUDP建立数据报的源对象和目标对象(地址和端口), 它会创建UDP Socket文件描述符,然后调用内部的connect为这个文件描述符设置源地址和目标地址，这时Go将它称之为connected,尽管我们知道UDP是无连接的协议，Go这种叫法我想根源来自Unix/Linux的UDP的实现。这个方法返回*UDPConn。
有的时候却可以通过ListenUDP返回的*UDPConn直接往某个目标地址发送数据报，而不是通过DialUDP方式发送，原因在于两者返回的*UDPConn是不同的。前者是connected，后者是unconnected。
你必须清楚知道你的UDP是连接的(connected)还是未连接(unconnected)的，这样你才能正确的选择的读写方法。
如果*UDPConn是connected,读写方法是Read和Write。
如果*UDPConn是unconnected,读写方法是ReadFromUDP和WriteToUDP（以及ReadFrom和WriteTo)。
事实上Go的这种设计和Unix/Linux设计一致，下面是Linux关于UDP的文档：
When a UDP socket is created, its local and remote addresses are unspecified. Datagrams can be sent immediately using sendto or sendmsg with a valid destination address as an argument. When connect is called on the socket, the default destination address is set and datagrams can now be sent using send or write without specifying a destination address. It is still possible to send to other destinations by passing an address to sendto or sendmsg. In order to receive packets, the socket can be bound to a local address first by using bind. Otherwise, the socket layer will automatically assign a free local port out of the range defined by /proc/sys/net/ipv4/ip_local_port_range and bind the socket to INADDR_ANY.
ReadFrom和WriteTo是为了实现PacketConn接口而实现的方法，它们的实现基本上和ReadFromUDP和WriteToUDP一样，只不过地址换成了更通用的Addr,而不是具体化的UDPAddr。
还有几种情况需要弄清楚:
1、
因为unconnected的*UDPConn还没有目标地址，所以需要把目标地址当作参数传入到WriteToUDP的方法中，但是unconnected的*UDPConn可以调用Read方法吗？
答案是可以,但是在这种情况下，客户端的地址信息就被忽略了。
func main() {
	listener, err := net.ListenUDP("udp", &net.UDPAddr{IP: net.ParseIP("127.0.0.1"), Port: 9981})
	if err != nil {
		fmt.Println(err)
		return
	}
	fmt.Printf("Local: <%s> \n", listener.LocalAddr().String())
	data := make([]byte, 1024)
	for {
		n, err := listener.Read(data)
		if err != nil {
			fmt.Printf("error during read: %s", err)
		}
		fmt.Printf("<%s>\n", data[:n])
	}
}
2、
unconnected的*UDPConn可以调用Write方法吗？
答案是不可以， 因为不知道目标地址。
func main() {
	listener, err := net.ListenUDP("udp", &net.UDPAddr{IP: net.ParseIP("127.0.0.1"), Port: 9981})
	if err != nil {
		fmt.Println(err)
		return
	}
	fmt.Printf("Local: <%s> \n", listener.LocalAddr().String())
	_, err = listener.Write([]byte("hello"))
	if err != nil {
		fmt.Printf(err.Error())
	}
}
出错：
write udp 127.0.0.1:9981: write: destination address requiredsmallnestMBP:udp smallnest
3、
connected的*UDPConn可以调用WriteToUDP方法吗？
答案是不可以， 因为目标地址已经设置。
即使是相同的目标地址也不可以。
func main() {
	ip := net.ParseIP("127.0.0.1")
	srcAddr := &net.UDPAddr{IP: net.IPv4zero, Port: 0}
	dstAddr := &net.UDPAddr{IP: ip, Port: 9981}
	conn, err := net.DialUDP("udp", srcAddr, dstAddr)
	if err != nil {
		fmt.Println(err)
	}
	defer conn.Close()
	_, err = conn.WriteToUDP([]byte("hello"), dstAddr)
	if err != nil {
		fmt.Println(err)
	}
}
报错:
write udp 127.0.0.1:50141->127.0.0.1:9981: use of WriteTo with pre-connected connection
4、
connected的*UDPConn如果调用Closed以后可以调用WriteToUDP方法吗？
答案是不可以。
func main() {
	ip := net.ParseIP("127.0.0.1")
	srcAddr := &net.UDPAddr{IP: net.IPv4zero, Port: 0}
	dstAddr := &net.UDPAddr{IP: ip, Port: 9981}
	conn, err := net.DialUDP("udp", srcAddr, dstAddr)
	if err != nil {
		fmt.Println(err)
	}
	err = conn.Close()
	if err != nil {
		fmt.Println(err)
	}
	_, err = conn.WriteToUDP([]byte("hello"), dstAddr)
	if err != nil {
		fmt.Println(err)
	}
}
同样的报错：
write udp 127.0.0.1:59074->127.0.0.1:9981: use of WriteTo with pre-connected connection
5、
connected的*UDPConn可以调用ReadFromUDP方法吗？
答案是可以,但是它的功能基本和Read一样，只能和它connected的对端通信。
看下面的client的例子：
func main() {
	ip := net.ParseIP("127.0.0.1")
	srcAddr := &net.UDPAddr{IP: net.IPv4zero, Port: 0}
	dstAddr := &net.UDPAddr{IP: ip, Port: 9981}
	conn, err := net.DialUDP("udp", srcAddr, dstAddr)
	if err != nil {
		fmt.Println(err)
	}
	defer conn.Close()
	go func() {
		data := make([]byte, 1024)
		for {
			n, remoteAddr, err := conn.ReadFromUDP(data)
			if err != nil {
				fmt.Printf("error during read: %s", err)
			}
			fmt.Printf("<%s> %s\n", remoteAddr, data[:n])
		}
	}()
	conn.Write([]byte("hello"))
	b := make([]byte, 1)
	os.Stdin.Read(b)
}
6、
*UDPConn还有一个通用的WriteMsgUDP(b, oob []byte, addr *UDPAddr)，同时支持connected和unconnected的UDPConn:

如果UDPConn还未连接，那么它会发送数据报给addr
如果UDPConn已连接，那么它会发送数据报给连接的对端，这种情况下addr应该为nil

通用多播编程
Go标准库也支持多播，但是我们首先我们看通用的多播是如何实现的，它使用golang.org/x/net/ipv4或者golang.org/x/net/ipv6进行控制。
首先找到要进行多播所使用的网卡,然后监听本机合适的地址和服务端口。
将这个应用加入到多播组中，它就可以从组中监听包信息，当然你还可以对包传输进行更多的控制设置。
应用收到包后还可以检查包是否来自这个组的包。
完整的代码如下：
package main
import (
	"fmt"
	"net"
	"golang.org/x/net/ipv4"
)
func main() {
	//1. 得到一个interface
	en4, err := net.InterfaceByName("en4")
	if err != nil {
		fmt.Println(err)
	}
	group := net.IPv4(224, 0, 0, 250)
	//2. bind一个本地地址
	c, err := net.ListenPacket("udp4", "0.0.0.0:1024")
	if err != nil {
		fmt.Println(err)
	}
	defer c.Close()
	//3.
	p := ipv4.NewPacketConn(c)
	if err := p.JoinGroup(en4, &net.UDPAddr{IP: group}); err != nil {
		fmt.Println(err)
	}
	//4.更多的控制
	if err := p.SetControlMessage(ipv4.FlagDst, true); err != nil {
		fmt.Println(err)
	}
	//5.接收消息
	b := make([]byte, 1500)
	for {
		n, cm, src, err := p.ReadFrom(b)
		if err != nil {
			fmt.Println(err)
		}
		if cm.Dst.IsMulticast() {
			if cm.Dst.Equal(group) {
				fmt.Printf("received: %s from <%s>\n", b[:n], src)
				n, err = p.WriteTo([]byte("world"), cm, src)
				if err != nil {
					fmt.Println(err)
				}
			} else {
				fmt.Println("Unknown group")
				continue
			}
		}
	}
}
同一个应用可以加入到多个组中，多个应用也可以加入到同一个组中。
多个UDP listener可以监听同样的端口，加入到同一个group中。
It is possible for multiple UDP listeners that listen on the same UDP port to join the same multicast group. The net package will provide a socket that listens to a wildcard address with reusable UDP port when an appropriate multicast address prefix is passed to the net.ListenPacket or net.ListenUDP.
c1, err := net.ListenPacket("udp4", "224.0.0.0:1024")
if err != nil {
	// error handling
}
defer c1.Close()
c2, err := net.ListenPacket("udp4", "224.0.0.0:1024")
if err != nil {
	// error handling
}
defer c2.Close()
p1 := ipv4.NewPacketConn(c1)
if err := p1.JoinGroup(en0, &net.UDPAddr{IP: net.IPv4(224, 0, 0, 248)}); err != nil {
	// error handling
}
p2 := ipv4.NewPacketConn(c2)
if err := p2.JoinGroup(en0, &net.UDPAddr{IP: net.IPv4(224, 0, 0, 248)}); err != nil {
	// error handling
}
还支持Source-specific multicasting特性。
标准库多播编程
标准库的多播编程简化了上面的操作，当然也减少了更多的控制。如果想实现一个简单的多播程序，可以使用这样的方法。
服务器端的代码：
func main() {
	//如果第二参数为nil,它会使用系统指定多播接口，但是不推荐这样使用
	addr, err := net.ResolveUDPAddr("udp", "224.0.0.250:9981")
	if err != nil {
		fmt.Println(err)
	}
	listener, err := net.ListenMulticastUDP("udp", nil, addr)
	if err != nil {
		fmt.Println(err)
		return
	}
	fmt.Printf("Local: <%s> \n", listener.LocalAddr().String())
	data := make([]byte, 1024)
	for {
		n, remoteAddr, err := listener.ReadFromUDP(data)
		if err != nil {
			fmt.Printf("error during read: %s", err)
		}
		fmt.Printf("<%s> %s\n", remoteAddr, data[:n])
	}
}
写个客户端测试一下：
func main() {
	ip := net.ParseIP("224.0.0.250")
	srcAddr := &net.UDPAddr{IP: net.IPv4zero, Port: 0}
	dstAddr := &net.UDPAddr{IP: ip, Port: 9981}
	conn, err := net.DialUDP("udp", srcAddr, dstAddr)
	if err != nil {
		fmt.Println(err)
	}
	defer conn.Close()
	conn.Write([]byte("hello"))
	fmt.Printf("<%s>\n", conn.RemoteAddr())}
广播
广播的编程方式和多播的编程方式有所不同。简单说，广播意味着你吼一嗓子，局域网内的所有的机器都会收到。
服务器端代码：
func main() {
	listener, err := net.ListenUDP("udp", &net.UDPAddr{IP: net.IPv4zero, Port: 9981})
	if err != nil {
		fmt.Println(err)
		return
	}
	fmt.Printf("Local: <%s> \n", listener.LocalAddr().String())
	data := make([]byte, 1024)
	for {
		n, remoteAddr, err := listener.ReadFromUDP(data)
		if err != nil {
			fmt.Printf("error during read: %s", err)
		}
		fmt.Printf("<%s> %s\n", remoteAddr, data[:n])
		_, err = listener.WriteToUDP([]byte("world"), remoteAddr)
		if err != nil {
			fmt.Printf(err.Error())
		}
	}
}
客户端代码有所不同，它不是通过DialUDP “连接” 广播地址，而是通过ListenUDP创建一个unconnected的 *UDPConn,然后通过WriteToUDP发送数据报，这和你脑海中的客户端不太一致：
func main() {
	ip := net.ParseIP("172.24.14.255")
	srcAddr := &net.UDPAddr{IP: net.IPv4zero, Port: 0}
	dstAddr := &net.UDPAddr{IP: ip, Port: 9981}
	conn, err := net.ListenUDP("udp", srcAddr)
	if err != nil {
		fmt.Println(err)
	}
	n, err := conn.WriteToUDP([]byte("hello"), dstAddr)
	if err != nil {
		fmt.Println(err)
	}
	data := make([]byte, 1024)
	n, _, err = conn.ReadFrom(data)
	if err != nil {
		fmt.Println(err)
	}
	fmt.Printf("read %s from <%s>\n", data[:n], conn.RemoteAddr())
	b := make([]byte, 1)
	os.Stdin.Read(b)
}
你局域网内的广播地址可能和例子中的不同，你可以通过ifconfig查看。
广播地址(Broadcast Address)是专门用于同时向网络中所有工作站进行发送的一个地址。在使用TCP/IP 协议的网络中，主机标识段host ID 为全1 的IP 地址为广播地址，广播的分组传送给host ID段所涉及的所有计算机。例如，对于10.1.1.0 （255.255.255.0 ）网段，其广播地址为10.1.1.255 （255 即为2 进制的11111111 ），当发出一个目的地址为10.1.1.255 的分组（封包）时，它将被分发给该网段上的所有计算机。
任播
在互联网中，通常使用边界网关协议来实现任播。比如域名根服务器就是通过任播的方式提供。13台根服务器使用13个任播地址，但是有500多台实际服务器。你可以通过单播的方式发送数据包，只有最快的(最近的)的一个UDP服务器接收到。
Anycasting最初是在RFC1546中提出并定义的，它的最初语义是，在IP网络上通过一个Anycast地址标识一组提供特定服务的主机，同时服务访问方并不关心提供服务的具体是哪一台主机(比如DNS或者镜像服务)，访问该地址的报文可以被IP网络路由到这一组目标中的任何一台主机上，它提供的是一种无状态的、尽力而为的服务。
RFC2373(IP Version 6 Addressing Architecture, July 1998)提供了较新的说明和动机：任播地址的一个期望应用是标识属于某个提供互联网服务的机构的路由器集合。这种地址可以用作IPv6路由标题的中间地址,使数据分组通过某一聚合或聚合序列传递。其他可能的用途是标识属于某一子网的路由器组或提供进入某一路由范围入口的路由器组。
RFC2373标准对任播的定义是，当一个单播地址被分配到多于一个的接口上时，发到该接口的报文被网络路由到由路由协议度量的“最近”的目标接口上。与Unicast和Multicast类似，Anycast也是IP网络的一种通信模式。Unicast允许源结点向单一目标结点发送数据报，Multicast允许源结点向一组目标结点发送数据报，而Anycast则允许源结点向一组目标结点中的一个结点发送数据报，而这个结点由路由系统选择，对源结点透明；同时，路由系统选择“最近”的结点为源结点提供服务，从而在一定程序上为源结点提供了更好的服务也减轻了网络负载。
参考文档

https://zh.wikipedia.org/wiki/用户数据报协议
https://golang.org/pkg/net/
http://man7.org/linux/man-pages/man7/udp.7.html
https://godoc.org/golang.org/x/net/ipv4
https://github.com/golang/go/issues/13391
http://baike.baidu.com/view/473043.htm
http://baike.baidu.com/view/2032315.htm

深入Go UDP编程，首发于文章 - 伯乐在线。</content>
</doc>
<doc>
	<docid>10</docid>
	<title>测试自动化后，我们还需要QA吗？</title>
	<link>http://blog.jobbole.com/106995/</link>
	<author></author>
	<content>QA的职责
我们先讨论一下传统的瀑布模型下QA是如何工作的，其中最主要的问题是什么；然后作为对比，我们来看看在敏捷团队里QA又是如何工作的，工作重点又是什么；最后，我们详细看一看在新的职责下，QA应该如何做。
瀑布开发模型
即使在今天，在很多企业中，瀑布模型仍然是主流。每一个需求都需要经过分析，设计，开发，测试，上线部署，运维等阶段。虽然一些企业已经在实施敏捷开发，比如项目/产品以迭代的方式运作，也有诸如每日站会，代码检视等敏捷实践，但是如果仔细审视，你会发现其实开发模式骨子里还是瀑布：按照软件组件划分的部门结构（详见康威定律），按照职能划分的团队（开发和测试分属不同部门），过长的反馈周期，永远无法摆脱的集成难题等等。
随着软件变得越来越复杂，团队里没有任何一个人可以说出系统是如何运作的，也不知道最终用户是谁，以及最终用户会以何种方式来使用最终的软件。
更糟糕的是，按照职能划分的团队在物理上都是隔离的，比如独立的测试部门，独立的运维部门，整日忙碌而难以预约到档期的业务人员，当然还有经常疲于交付，无处吐槽的苦逼开发。由于这些隔离，信息的反馈周期会非常长，一个本来很容易修复的缺陷可能在4周之后才可能被另一个部门的测试发现，然后通过复杂的工作流（比如某种形式的缺陷追踪系统）流到开发那里，而开发可能还在拼命的完成早就应该交付的功能，从而形成恶性循环。
瀑布模式中的QA
在这样的环境中，QA们能做的事情非常有限。在需求开始时会他们参加需求澄清的会议，制定一些测试计划，然后进行测试用例的设计。有的企业会用诸如Excel之类的工具来记录这些用例。这些写在Excel里的，死的用例用处非常有限。而最大的问题在于：它们无法自动化执行。另外，在实际软件开发中，需求总是会经常发生变化，需求的优先级也会有调整，然后这些记录在Excel中的死的用例会很快过期，变得无人问津。
除此之外，QA中的有些成员会使用工具来录制一些UI测试的场景，然后在每个新版本出来之后进行回放即可。然而，当UI发生一点变化之后，这些自动化的用例就会失效：比如HTML片段中元素位置的调整，JavaScript的异步调用超时等等。
显然，这种单纯以黑盒的形式来检查功能点的测试方式是不工作的，要真正有效的提升软件质量，仅仅通过事后检查是远远不够的，软件的质量也应该内建于软件之中。QA的工作也应该是一个贯穿软件生命周期的活动，从商业想法，到真实上线，这其中的所有环节，都应该有QA的参与。
系统思考
如果不从一个系统的角度来思考软件质量，就无法真正构建出健壮的、让业务和团队都有信心的软件系统。质量从来都不只是QA的职责，而是整个团队的职责。
关于软件质量，一个根深蒂固的误解是：缺陷在开发过程中被引入，然后在测试阶段被发现，最后在QA和开发的来来回回的撕扯中被解决（或者数量被大规模降低），最后在生产环境中，就只会有很少的，优先级很低的缺陷。
然而事实上，很多需求就没有仔细分析，业务价值不很确定，验收条件模糊，流入开发后又会引入一些代码级别的错误，以及业务规则上的缺陷，测试阶段会漏掉一些功能点，上线之后更是问题百出（网络故障，缓存失效，黑客攻击，操作系统补丁，甚至内存溢出，log文件将磁盘写满等等）。
在一个敏捷团队中，每个个人都应该对质量负责，而QA则以自己的丰富经验和独特视角来发掘系统中可能的质量隐患，并帮助团队将这些隐患消除。

我在ThoughtWorks的同事Anand Bagmar在他的演讲What is Agile testing- How does automation help?中详细讨论过这部分内容。
QA到底应该干什么？
本质上来说，任何软件项目的目标都应该是：更快地将高质量的软件从想法变成产品。
将这个大目标细分一下，会得到这样几个子项，即企业需要：

更多的商业回报（发掘业务价值）
更快的上线时间（做最简单，直接的版本）
更好的软件质量（质量内嵌）
更少的资源投入（减少浪费）

其实就是传说中的多、快、好、省。如果说这是每一个软件项目的目标的话，那么团队里的每一个个人都应该向着这个目标而努力，任何其他形式的工作都可以归类为浪费。用Excel记录那些经常会失效，而且无法自动执行的测试用例是浪费，会因为页面布局变化而大面积失效的UI测试也是浪费，一个容易修复的缺陷要等到数周之后才被发现也是浪费。
在这个大前提下，我们再来思考QA在团队里应该做什么以及怎么做。
QA的职责
Lisa Crispin在《敏捷软件测试》中提到过一个很著名的模型：敏捷测试四象限。这个模型是QA制定测试策略时的一个重要参考：

如果按照纵向划分的话，图中的活动，越向上越面向业务；越向下越面向技术。横向划分的话，往左是支撑团队；往右是评价产品。
其实简化一下，QA在团队里的工作，可以分为两大类：

确保我们在正确的交付产品
确保我们交付了正确的产品

根据这个四象限的划分，大部分团队可能都会从Q2起步：QA会和BA，甚至UX一起，从需求分析入手，进行需求分析，业务场景梳理，这时候没有具体的可以被测试的软件代码。不过这并不妨碍测试活动，比如一些纸上原型的设计（感谢刘海生供图）：

通过这一阶段之后，我们已经有了用户故事，这时候QA需要和开发一起编写用户故事的自动化验收测试。当开发交付一部分功能之后，QA就可以做常规的用户故事测试，几个迭代之后，QA开始进行跨功能需求测试和探索性测试等。根据探索性测试的结果，QA可能会调整测试策略，调整测试优先级，完善测试用例等等。
根据项目的不同，团队可以从不同的象限开始测试策略的制定。事实上，Q1-Q4仅仅是一个编号，与时间、阶段并无关系，Lisa Crispin还专门撰文解释过。
关于QA如何在软件分析的上游就介入，然后通过BDD的方式与业务分析师一起产出软件的各种规格描述，并通过实例的方式来帮助整个团队对需求的理解，ThoughtWorks的林冰玉有一篇文章很好的介绍了BDD的正确做法。如果将QA的外延扩展到在线的生产环境，制定合理的测量指标，调整测试策略，强烈推荐林冰玉写的另一篇文章产品环境中的QA。
其他职责
事实上，软件生命周期中有很多的活动，有很多处于灰色地段。既可以说是应该开发做，又可以说应该QA做，甚至可以推给其他角色（比如OPs）。不过我们知道，一旦涉及角色，人们就再也不会按照全局优化的思路来应对问题了。这种灰色的活动包括：

持续集成的搭建
测试环境的创建于维护
UAT上的数据准备
代码中的测试代码的维护
测试代码的重构

在团队实践中，这些活动我们通常会让QA和开发或者OPs同事一起结对来完成。一方面避免知识孤岛的形成，另一方面在跨角色的工作中，也可以激发出更多不同的思路。
万能的QA？
虽然在这些活动中，QA都会参与，但是并不是说团队里只要有一个QA就可以了。QA在参与这些活动时，侧重点还是有很大不同的。
比如需求分析阶段，如果有QA的加入，一些从QA角度可以发现的有明显缺陷的场景，则可以在分析阶段就得到很好的处理。另一方面，尽早介入可以设计出更合理的测试计划（比如哪些功能的优先级比较高，用户更会频繁使用，那么对应的测试比重也会更高）。在Story分析与书写阶段，QA可以帮助写出更加合理的验收条件，既满足业务需求，又可以很好的指导开发。
在和开发一起编写澄清需求时，主要是编写自动化验收测试，而不是实际编写业务逻辑的实现（虽然QA应该参与Code Reivew环节，学习并分享自己的观点）；甚至在上线运维阶段，QA还需要和OPs一起来设计用户数据的采集指标（比如用户访问的关键路径，浏览器版本，地区的区分等），从而制定出新的测试策略。
扩展阅读
What is Agile testing &#8211; How does automation help?
敏捷实践Showcase的七宗罪
产品环境下的QA
《敏捷软件测试》
测试自动化后，我们还需要QA吗？，首发于文章 - 伯乐在线。</content>
</doc>
<doc>
	<docid>11</docid>
	<title>C++ 文件操作详解</title>
	<link>http://blog.jobbole.com/106992/</link>
	<author></author>
	<content>C++ 通过以下几个类支持文件的输入输出：

ofstream: 写操作（输出）的文件类 (由ostream引申而来)
ifstream: 读操作（输入）的文件类(由istream引申而来)
fstream: 可同时读写操作的文件类 (由iostream引申而来)

打开文件(Open a file)
对这些类的一个对象所做的第一个操作通常就是将它和一个真正的文件联系起来，也就是说打开一个文件。被打开的文件在程序中由一个流对象(stream object)来表示 (这些类的一个实例) ，而对这个流对象所做的任何输入输出操作实际就是对该文件所做的操作。
要通过一个流对象打开一个文件，我们使用它的成员函数open()：void open (const char * filename, openmode mode);
这里filename 是一个字符串，代表要打开的文件名，mode 是以下标志符的一个组合： ios::in 为输入(读)而打开文件

ios::out 为输出(写)而打开文件
ios::ate 初始位置：文件尾
ios::app 所有输出附加在文件末尾
ios::trunc 如果文件已存在则先删除该文件
ios::binary 二进制方式

这些标识符可以被组合使用，中间以”或”操作符(|)间隔。例如，如果我们想要以二进制方式打开文件&#8221;example.bin&#8221; 来写入一些数据，我们可以通过以下方式调用成员函数open（）来实现：ofstream file;
file.open ("example.bin", ios::out | ios::app | ios::binary);ofstream, ifstream 和 fstream所有这些类的成员函数open 都包含了一个默认打开文件的方式，这三个类的默认方式各不相同： 类 参数的默认方式

ofstream ios::out | ios::trunc
ifstream ios::in
fstream ios::in | ios::out

只有当函数被调用时没有声明方式参数的情况下，默认值才会被采用。如果函数被调用时声明了任何参数，默认值将被完全改写，而不会与调用参数组合。
由 于对类ofstream, ifstream 和 fstream 的对象所进行的第一个操作通常都是打开文件，这些类都有一个构造函数可以直接调用open 函数，并拥有同样的参数。这样，我们就可以通过以下方式进行与上面同样的定义对象和打开文件的操作：ofstream file ("example.bin", ios::out | ios::app | ios::binary);两种打开文件的方式都是正确的。
你可以通过调用成员函数is_open()来检查一个文件是否已经被顺利的打开了：bool is_open();
它返回一个布尔(bool)值，为真（true）代表文件已经被顺利打开，假( false )则相反。
关闭文件(Closing a file)
当文件读写操作完成之后，我们必须将文件关闭以使文件重新变为可访问的。关闭文件需要调用成员函数close()，它负责将缓存中的数据排放出来并关闭文件。它的格式很简单：void close ();这个函数一旦被调用，原先的流对象(stream object)就可以被用来打开其它的文件了，这个文件也就可以重新被其它的进程(process)所有访问了。
为防止流对象被销毁时还联系着打开的文件，析构函数(destructor)将会自动调用关闭函数close。
文本文件(Text mode files)
类ofstream, ifstream 和fstream 是分别从ostream, istream 和iostream 中引申而来的。这就是为什么 fstream 的对象可以使用其父类的成员来访问数据。
一般来说，我们将使用这些类与同控制台(console)交互同样的成员函数(cin 和 cout)来进行输入输出。如下面的例题所示，我们使用重载的插入操作符

// writing on a text file
#include <fstream>
using namespace std;

int main()
{
    ofstream examplefile("example.txt");
    if (examplefile.is_open())
    {
        examplefile << "This is a line.\n";
        examplefile << "This is another line.\n";
        examplefile.close();
    }
    return 0;
}

从文件中读入数据也可以用与 cin的使用同样的方法：

// reading a text file
#include <iostream>
#include <fstream>
#include <cstdlib>
using namespace std;
int main ()
{
    char buffer[256];
    ifstream examplefile("example.txt");
    if (! examplefile.is_open())
    {
        cout << "Error opening file"; exit (1);
    }
    while (!examplefile.eof())
    {
        examplefile.getline(buffer,100);
        cout<<buffer<< endl;
    }
    return 0;
}
//This is a line.
//This is another line.

上面的例子读入一个文本文件的内容，然后将它打印到屏幕上。注意我们使用了一个新的成员函数叫做eof ，它是ifstream 从类 ios 中继承过来的，当到达文件末尾时返回true 。
状态标志符的验证(Verification of state flags)
除了eof()以外，还有一些验证流的状态的成员函数（所有都返回bool型返回值）：
bad()

如果在读写过程中出错，返回 true 。例如：当我们要对一个不是打开为写状态的文件进行写入时，或者我们要写入的设备没有剩余空间的时候。

fail()

除了与bad() 同样的情况下会返回 true 以外，加上格式错误时也返回true ，例如当想要读入一个整数，而获得了一个字母的时候。

eof()

如果读文件到达文件末尾，返回true。

good()

这是最通用的：如果调用以上任何一个函数返回true 的话，此函数返回 false 。

要想重置以上成员函数所检查的状态标志，你可以使用成员函数clear()，没有参数。
获得和设置流指针(get and put stream pointers)
所有输入/输出流对象(i/o streams objects)都有至少一个流指针：

ifstream， 类似istream, 有一个被称为get pointer的指针，指向下一个将被读取的元素。
ofstream, 类似 ostream, 有一个指针 put pointer ，指向写入下一个元素的位置。
fstream, 类似 iostream, 同时继承了get 和 put

我们可以通过使用以下成员函数来读出或配置这些指向流中读写位置的流指针：
tellg() 和 tellp()

这两个成员函数不用传入参数，返回pos_type 类型的值(根据ANSI-C++ 标准) ，就是一个整数，代表当前get 流指针的位置 (用tellg) 或 put 流指针的位置(用tellp).

seekg() 和seekp()

这对函数分别用来改变流指针get 和put的位置。两个函数都被重载为两种不同的原型：
seekg ( pos_type position );
seekp ( pos_type position );


使用这个原型，流指针被改变为指向从文件开始计算的一个绝对位置。要求传入的参数类型与函数 tellg 和tellp 的返回值类型相同。seekg ( off_type offset, seekdir direction );
seekp ( off_type offset, seekdir direction );使用这个原型可以指定由参数direction决定的一个具体的指针开始计算的一个位移(offset)。它可以是：

ios::beg 从流开始位置计算的位移
ios::cur 从流指针当前位置开始计算的位移
ios::end 从流末尾处开始计算的位移

流指针 get 和 put 的值对文本文件(text file)和二进制文件(binary file)的计算方法都是不同的，因为文本模式的文件中某些特殊字符可能被修改。由于这个原因，建议对以文本文件模式打开的文件总是使用seekg 和 seekp的第一种原型，而且不要对tellg 或 tellp 的返回值进行修改。对二进制文件，你可以任意使用这些函数，应该不会有任何意外的行为产生。
以下例子使用这些函数来获得一个二进制文件的大小：

// obtaining file size
#include <iostream>
#include <fstream>
using namespace std;

int main ()
{
    const char * filename = "example.txt";
    long l,m;
    ifstream file(filename, ios::in|ios::binary);
    l = file.tellg();
    file.seekg(0, ios::end);
    m = file.tellg();
    file.close();
    cout <<"size of "<< filename;
    cout <<" is "<< (m-l)<<" bytes.\n";
    return 0;
}
//size of example.txt is 40 bytes.

二进制文件(Binary files)
在二进制文件中，使用>，以及函数（如getline）来操作符输入和输出数据，没有什么实际意义，虽然它们是符合语法的。
文 件流包括两个为顺序读写数据特殊设计的成员函数：write 和 read。第一个函数 (write) 是ostream 的一个成员函数，都是被ofstream所继承。而read 是istream 的一个成员函数，被ifstream 所继承。类 fstream 的对象同时拥有这两个函数。它们的原型是：write ( char * buffer, streamsize size );
read ( char * buffer, streamsize size );这里 buffer 是一块内存的地址，用来存储或读出数据。参数size 是一个整数值，表示要从缓存（buffer）中读出或写入的字符数。

// reading binary file
#include <iostream>
#include <fstream>
using namespace std;
int main ()
{
    const char * filename = "example.txt";
    char * buffer;
    long size;
    ifstream file(filename, ios::in|ios::binary|ios::ate);
    size = file.tellg();
    file.seekg(0, ios::beg);
    buffer = new char [size];
    file.read(buffer, size);
    file.close();
    cout <<"the complete file is in a buffer";
    delete[] buffer;
    return 0;
}
//The complete file is in a buffer

缓存和同步(Buffers and Synchronization)
当我们对文件流进行操作的时候，它们与一个streambuf 类型的缓存(buffer)联系在一起。这个缓存（buffer）实际是一块内存空间，作为流(stream)和物理文件的媒介。例如，对于一个输出流， 每次成员函数put (写一个单个字符)被调用，这个字符不是直接被写入该输出流所对应的物理文件中的，而是首先被插入到该流的缓存（buffer）中。
当缓存被排放出来(flush)时，它里面的所有数据或者被写入物理媒质中（如果是一个输出流的话），或者简单的被抹掉(如果是一个输入流的话)。这个过程称为同步(synchronization)，它会在以下任一情况下发生：

当文件被关闭时: 在文件被关闭之前，所有还没有被完全写出或读取的缓存都将被同步。
当缓存buffer 满时:缓存Buffers 有一定的空间限制。当缓存满时，它会被自动同步。
控制符明确指明:当遇到流中某些特定的控制符时，同步会发生。这些控制符包括：flush 和endl。

明确调用函数sync(): 调用成员函数sync() (无参数)可以引发立即同步。这个函数返回一个int 值，等于-1 表示流没有联系的缓存或操作失败
在C++中，有一个stream这个类，所有的I/O都以这个“流”类为基础的，包括我们要认识的文件I/O，stream这个类有两个重要的运算符：
1、插入器(<<) 

向流输出数据。比如说系统有一个默认的标准输出流(cout)，一般情况下就是指的显示器，所以，cout
2、析取器(>>)
从流中输入数据。比如说系统有一个默认的标准输入流(cin)，一般情况下就是指的键盘，所以，cin>>x;就表示从标准输入流中读取一个指定类型(即变量x的类型)的数据。
在C++中，对文件的操作是通过stream的子类fstream(file stream)来实现的，所以，要用这种方式操作文件，就必须加入头文件fstream.h。下面就把此类的文件操作过程一一道来。
一、打开文件
在fstream类中，有一个成员函数open()，就是用来打开文件的，其原型是：void open(const char* filename,int mode,int access);参数：

filename： 要打开的文件名
mode： 要打开文件的方式
access： 打开文件的属性

打开文件的方式在类ios(是所有流式I/O类的基类)中定义，常用的值如下：

ios::app： 以追加的方式打开文件
ios::ate： 文件打开后定位到文件尾，ios:app就包含有此属性
ios::binary： 以二进制方式打开文件，缺省的方式是文本方式。两种方式的区别见前文
ios::in： 文件以输入方式打开
ios::out： 文件以输出方式打开
ios::nocreate： 不建立文件，所以文件不存在时打开失败
ios::noreplace：不覆盖文件，所以打开文件时如果文件存在失败
ios::trunc： 如果文件存在，把文件长度设为0

可以用“或”把以上属性连接起来，如ios::out|ios::binary
打开文件的属性取值是：
0：普通文件，打开访问
1：只读文件
2：隐含文件
4：系统文件
可以用“或”或者“+”把以上属性连接起来 ，如3或1|2就是以只读和隐含属性打开文件。
例如：以二进制输入方式打开文件c:config.sys

fstream file1;
file1.open("c:config.sys",ios::binary|ios::in,0);

如果open函数只有文件名一个参数，则是以读/写普通文件打开，即：

file1.open("c:config.sys");<=>file1.open("c:config.sys",ios::in|ios::out,0);

另外，fstream还有和open()一样的构造函数，对于上例，在定义的时侯就可以打开文件了：

fstream file1("c:config.sys");

特别提出的是，fstream有两个子类：ifstream(input file stream)和ofstream(outpu file stream)，ifstream默认以输入方式打开文件，而ofstream默认以输出方式打开文件。

ifstream file2("c:pdos.def");//以输入方式打开文件
ofstream file3("c:x.123");//以输出方式打开文件

所以，在实际应用中，根据需要的不同，选择不同的类来定义：如果想以输入方式打开，就用ifstream来定义；如果想以输出方式打开，就用ofstream来定义；如果想以输入/输出方式来打开，就用fstream来定义。
二、关闭文件
打开的文件使用完成后一定要关闭，fstream提供了成员函数close()来完成此操作，如：file1.close();就把file1相连的文件关闭。
三、读写文件
读写文件分为文本文件和二进制文件的读取，对于文本文件的读取比较简单，用插入器和析取器就可以了；而对于二进制的读取就要复杂些，下要就详细的介绍这两种方式
1、文本文件的读写
文本文件的读写很简单：用插入器(>)从文件输入。假设file1是以输入方式打开，file2以输出打开。示例如下：

file2"I Love You";//向文件写入字符串"I Love You"
int i;
file1>>i;//从文件输入一个整数值。

这种方式还有一种简单的格式化能力，比如可以指定输出为16进制等等，具体的格式有以下一些
操纵符 功能 输入/输出
dec 格式化为十进制数值数据 输入和输出
endl 输出一个换行符并刷新此流 输出
ends 输出一个空字符 输出
hex 格式化为十六进制数值数据 输入和输出
oct 格式化为八进制数值数据 输入和输出
setpxecision(int p) 设置浮点数的精度位数 输出
比如要把123当作十六进制输出：file1<<hex<<123;要把3.1415926以5位精度输出：file1<<setpxecision(5)<<3.1415926。
2、二进制文件的读写
①put()
put()函数向流写入一个字符，其原型是ofstream &put(char ch)，使用也比较简单，如file1.put(&#8216;c&#8217;);就是向流写一个字符&#8217;c&#8217;。
②get()
get()函数比较灵活，有3种常用的重载形式：
一种就是和put()对应的形式：ifstream &get(char &ch);功能是从流中读取一个字符，结果保存在引用ch中，如果到文件尾，返回空字符。如file2.get(x);表示从文件中读取一个字符，并把读取的字符保存在x中。
另一种重载形式的原型是： int get();这种形式是从流中返回一个字符，如果到达文件尾，返回EOF，如x=file2.get();和上例功能是一样的。
还 有一种形式的原型是：ifstream &get(char *buf,int num,char delim=&#8217;n&#8217;)；这种形式把字符读入由 buf 指向的数组，直到读入了 num 个字符或遇到了由 delim 指定的字符，如果没使用 delim 这个参数，将使用缺省值换行符&#8217;n&#8217;。例如：
file2.get(str1,127,&#8217;A&#8217;);//从文件中读取字符到字符串str1，当遇到字符&#8217;A&#8217;或读取了127个字符时终止。
③读写数据块
要读写二进制数据块，使用成员函数read()和write()成员函数，它们原型如下：

read(unsigned char *buf,int num);
write(const unsigned char *buf,int num);

read() 从文件中读取 num 个字符到 buf 指向的缓存中，如果在还未读入 num 个字符时就到了文件尾，可以用成员函数 int gcount();来取得实际读取的字符数；而 write() 从buf 指向的缓存写 num 个字符到文件中，值得注意的是缓存的类型是 unsigned char *，有时可能需要类型转换。
例：

unsigned char str1[]="I Love You";
int n[5];
ifstream in("xxx.xxx");
ofstream out("yyy.yyy");
out.write(str1,strlen(str1));//把字符串str1全部写到yyy.yyy中
in.read((unsigned char*)n,sizeof(n));//从xxx.xxx中读取指定个整数，注意类型转换
in.close();out.close();

四、检测 EOF
成员函数eof()用来检测是否到达文件尾，如果到达文件尾返回非0值，否则返回0。原型是int eof();
例：

if(in.eof())ShowMessage("已经到达文件尾！");

五、文件定位
和 C的文件操作方式不同的是，C++ I/O系统管理两个与一个文件相联系的指针。一个是读指针，它说明输入操作在文件中的位置；另一个是写指针，它下次写操作的位置。每次执行输入或输出时， 相应的指针自动变化。所以，C++的文件定位分为读位置和写位置的定位，对应的成员函数是 seekg()和 seekp()，seekg()是设置读位置，seekp是设置写位置。它们最通用的形式如下：

istream &seekg(streamoff offset,seek_dir origin);
ostream &seekp(streamoff offset,seek_dir origin);

streamoff定义于 iostream.h 中，定义有偏移量 offset 所能取得的最大值，seek_dir 表示移动的基准位置，是一个有以下值的枚举：

ios::beg： 文件开头
ios::cur： 文件当前位置
ios::end： 文件结尾

这两个函数一般用于二进制文件，因为文本文件会因为系统对字符的解释而可能与预想的值不同。
例：

file1.seekg(1234,ios::cur);//把文件的读指针从当前位置向后移1234个字节
file2.seekp(1234,ios::beg);//把文件的写指针从文件开头向后移1234个字节

C++ 文件操作详解，首发于文章 - 伯乐在线。</content>
</doc>
<doc>
	<docid>12</docid>
	<title>漫画算法：最小栈的实现</title>
	<link>http://blog.jobbole.com/106940/</link>
	<author></author>
	<content>





小灰回忆起当时的情景&#8230;&#8230;





题目：实现一个栈，带有出栈（pop），入栈（push），取最小元素（getMin）三个方法。要保证这三个方法的时间复杂度都是O（1）。


小灰的想法：
1.创建一个整型变量 min，初始值-1
2.当第一个元素进栈时，让min=0，即把唯一的元素当做最小值。
3.之后每当一个新元素近栈，让新元素和min指向位置的元素比较大小。如果Stack[min]大于新元素，则min等于新元素的下标；Stack[min]小于新元素，则不做改变。
4.当调用getMin方法的时候，直接返回min所指向位置的元素即可。

按这个思路，近栈、出栈、取最小值的时间复杂度都是O(1)，空间复杂度也是O(1)。




回忆到此结束&#8230;&#8230;







解法：
1.设原有的栈叫做栈A，此时创建一个额外的栈B，用于辅助原栈A。
2.当第一个元素进入栈A的时候，让新元素的下标进入栈B。这个唯一的元素是栈A的当前最小值。（考虑到栈中元素可能不是类对象，所以B栈存储的是A栈元素的下标）
3.每当新元素进入栈A时，比较新元素和栈A当前最小值的大小，如果小于栈A当前最小值，则让新元素的下标进入栈B，此时栈B的栈顶元素就是栈A当前最小值的下标。
4.每当栈A有元素出栈时，如果出栈元素是栈A当前最小值，则让栈B的栈顶元素也出栈。此时栈B余下的栈顶元素所指向的，是栈A当中原本第二小的元素，代替刚才的出栈元素成为了栈A的当前最小值。（备胎转正）
5.当调用getMin方法的时候，直接返回栈B的栈顶所指向的栈A对应元素即可。

这个解法中近栈、出栈、取最小值的时间复杂度都是O(1)，最坏情况空间复杂度是O(N)。

扩展题目：
实现一个队列，带有出队（deQueue），入队（enQueue），取最小元素（getMin）三个方法。要保证这三个方法的时间复杂度都尽可能小。

漫画算法：最小栈的实现，首发于文章 - 伯乐在线。</content>
</doc>
<doc>
	<docid>13</docid>
	<title>Go 中的锁源码实现：Mutex</title>
	<link>http://blog.jobbole.com/106981/</link>
	<author></author>
	<content>上一篇文章《当我们谈论锁，我们谈什么》 中我提到了锁，准确地说是信号量（semaphore, mutext是semaphore的一种）的实现方式有两种：wait的时候忙等待或者阻塞自己。//忙等待
wait(S) {
    while(S<=0)
        ;   //no-op
    S--
}
//阻塞
wait(semaphore *S) {
    S->value--;
    if (S->value < 0) {
        add this process to S->list;
        block()
    }
}忙等待和阻塞方式各有优劣：

忙等待会使CPU空转，好处是如果在当前时间片内锁被其他进程释放，当前进程直接就能拿到锁而不需要CPU进行进程调度了。适用于锁占用时间较短的情况，且不适合于单处理器。
阻塞不会导致CPU空转，但是进程切换也需要代价，比如上下文切换，CPU Cache Miss。

下面看一下golang的源码里面是怎么实现锁的。golang里面的锁有两个特性：
1.不支持嵌套锁
2.可以一个goroutine lock，另一个goroutine unlock
互斥锁
golang中的互斥锁定义在src/sync/mutex.go// A Mutex is a mutual exclusion lock.
// Mutexes can be created as part of other structures;
// the zero value for a Mutex is an unlocked mutex.
//
// A Mutex must not be copied after first use.
type Mutex struct {
    state int32
    sema  uint32
}

const (
    mutexLocked = 1 << iota // mutex is locked
    mutexWoken
    mutexWaiterShift = iota
)看上去也是使用信号量的方式来实现的。sema就是信号量，一个非负数；state表示Mutex的状态。mutexLocked表示锁是否可用（0可用，1被别的goroutine占用），mutexWoken=2表示mutex是否被唤醒，mutexWaiterShift=2表示统计阻塞在该mutex上的goroutine数目需要移位的数值。将3个常量映射到state上就是state:   |32|31|...|3|2|1|
         \__________/ | |
               |      | |
               |      | mutex的占用状态（1被占用，0可用）
               |      |
               |      mutex的当前goroutine是否被唤醒
               |
               当前阻塞在mutex上的goroutine数
1.Lock
下面看一下mutex的lock。func (m *Mutex) Lock() {
    // Fast path: grab unlocked mutex.
    if atomic.CompareAndSwapInt32(&m.state, 0, mutexLocked) {
        if race.Enabled {
            race.Acquire(unsafe.Pointer(m))
        }
        return
    }

    awoke := false
    iter := 0
    for {
        old := m.state
        new := old | mutexLocked
        if old&mutexLocked != 0 {
            if runtime_canSpin(iter) {
                // Active spinning makes sense.
                // Try to set mutexWoken flag to inform Unlock
                // to not wake other blocked goroutines.
                if !awoke && old&mutexWoken == 0 && old>>mutexWaiterShift != 0 &&
                    atomic.CompareAndSwapInt32(&m.state, old, old|mutexWoken) {
                    awoke = true
                }
                runtime_doSpin()
                iter++
                continue
            }
            new = old + 1<<mutexWaiterShift
        }
        if awoke {
            // The goroutine has been woken from sleep,
            // so we need to reset the flag in either case.
            if new&mutexWoken == 0 {
                panic("sync: inconsistent mutex state")
            }
            new &^= mutexWoken
        }
        if atomic.CompareAndSwapInt32(&m.state, old, new) {
            if old&mutexLocked == 0 {
                break
            }
            runtime_Semacquire(&m.sema)
            awoke = true
            iter = 0
        }
    }

    if race.Enabled {
        race.Acquire(unsafe.Pointer(m))
    }
}这里要解释一下atomic.CompareAndSwapInt32()，atomic包是由golang提供的low-level的原子操作封装，主要用来解决进程同步为题，官方并不建议直接使用。我在上一篇文章中说过，操作系统级的锁的实现方案是提供原子操作，然后基本上所有锁相关都是通过这些原子操作来实现。CompareAndSwapInt32()就是int32型数字的compare-and-swap实现。cas(&addr, old, new)的意思是if *addr==old, *addr=new。大部分操作系统支持CAS，x86指令集上的CAS汇编指令是CMPXCHG。下面我们继续看上面的lock函数。if atomic.CompareAndSwapInt32(&m.state, 0, mutexLocked) {
    if race.Enabled {
        race.Acquire(unsafe.Pointer(m))
    }
    return
}首先先忽略race.Enabled相关代码，这个是go做race检测时候用的，这个时候需要带上-race，则race.Enabled被置为true。Lock函数的入口处先调用CAS尝试去获得锁，如果m.state==0，则将其置为1，并返回。
继续往下看，首先将m.state的值保存到old变量中，new=old|mutexLocked。直接看能让for退出的第三个if条件，首先调用CAS试图将m.state设置成new的值。然后看一下if里面，如果m.state之前的值也就是old如果没有被占用则表示当前goroutine拿到了锁，则break。我们先看一下new的值的变化，第一个if条件里面new = old + 1<<mutexWaiterShift，结合上面的mutex的state各个位的意义，这句话的意思表示mutex的等待goroutine数目加1。还有awoke为true的情况下，要将m.state的标志位取消掉，也就是这句new &^= mutexWoken的作用。继续看第三个if条件里面，如果里面的if判断失败，则走到runtime_Semacquire()。
看一下这个函数runtime_Semacquire()函数，由于golang1.5之后把之前C语言实现的代码都干掉了，所以现在很低层的代码都是go来实现的。通过源码中的定义我们可以知道这个其实就是信号量的wait操作：等待*s>0，然后减1。编译器里使用的是sync_runtime.semacquire()函数。// Semacquire waits until *s > 0 and then atomically decrements it.
// It is intended as a simple sleep primitive for use by the synchronization
// library and should not be used directly.
func runtime_Semacquire(s *uint32)

//go:linkname sync_runtime_Semacquire sync.runtime_Semacquire
func sync_runtime_Semacquire(addr *uint32) {
    semacquire(addr, true)
}

func semacquire(addr *uint32, profile bool) {
    gp := getg()
    if gp != gp.m.curg {
        throw("semacquire not on the G stack")
    }

    // Easy case.
    if cansemacquire(addr) {
        return
    }

    // Harder case:
    //  increment waiter count
    //  try cansemacquire one more time, return if succeeded
    //  enqueue itself as a waiter
    //  sleep
    //  (waiter descriptor is dequeued by signaler)
    s := acquireSudog()
    root := semroot(addr)
    t0 := int64(0)
    s.releasetime = 0
    if profile && blockprofilerate > 0 {
        t0 = cputicks()
        s.releasetime = -1
    }
    for {
        lock(&root.lock)
        // Add ourselves to nwait to disable "easy case" in semrelease.
        atomic.Xadd(&root.nwait, 1)
        // Check cansemacquire to avoid missed wakeup.
        if cansemacquire(addr) {
            atomic.Xadd(&root.nwait, -1)
            unlock(&root.lock)
            break
        }
        // Any semrelease after the cansemacquire knows we're waiting
        // (we set nwait above), so go to sleep.
        root.queue(addr, s)
        goparkunlock(&root.lock, "semacquire", traceEvGoBlockSync, 4)
        if cansemacquire(addr) {
            break
        }
    }
    if s.releasetime > 0 {
        blockevent(s.releasetime-t0, 3)
    }
    releaseSudog(s)
}上面的代码有点多，我们只看和锁相关的代码。root := semroot(addr)   //seg 1

atomic.Xadd(&root.nwait, 1) // seg 2

root.queue(addr, s) //seg 3seg 1代码片段semroot()返回结构体semaRoot。存储方式是先对信号量的地址做移位，然后做哈希（对251取模，这个地方为什么是左移3位和对251取模不太明白）。semaRoot相当于和mutex.sema绑定。看一下semaRoot的结构：一个sudog链表和一个nwait整型字段。nwait字段表示该信号量上等待的goroutine数目。head和tail表示链表的头和尾巴，同时为了线程安全，需要使用一个互斥量来保护链表。这个时候细心的同学应该注意到一个问题，我们前面不是从Mutex跟过来的吗，相当于Mutex的实现了使用了Mutex本身？实际上semaRoot里面的mutex只是内部使用的一个简单版本，和sync.Mutex不是同一个。现在把这些倒推回去，runtime_Semacquire()的作用其实就是semaphore的wait(&s)：如果*s<0，则将当前goroutine塞入信号量s关联的goroutine waiting list，并休眠。func semroot(addr *uint32) *semaRoot {
    return &semtable[(uintptr(unsafe.Pointer(addr))>>3)%semTabSize].root
}

type semaRoot struct {
    lock  mutex
    head  *sudog
    tail  *sudog
    nwait uint32 // Number of waiters. Read w/o the lock.
}

// Prime to not correlate with any user patterns.
const semTabSize = 251

var semtable [semTabSize]struct {
    root semaRoot
    pad  [sys.CacheLineSize - unsafe.Sizeof(semaRoot{})]byte
}现在mutex.Lock()还剩下runtime_canSpin(iter)这一段，这个地方其实就是锁的自旋版本。golang对于自旋锁的取舍做了一些限制：1.多核; 2.GOMAXPROCS>1; 3.至少有一个运行的P并且local的P队列为空。golang的自旋尝试只会做几次，并不会一直尝试下去，感兴趣的可以跟一下源码。func sync_runtime_canSpin(i int) bool {
    // sync.Mutex is cooperative, so we are conservative with spinning.
    // Spin only few times and only if running on a multicore machine and
    // GOMAXPROCS>1 and there is at least one other running P and local runq is empty.
    // As opposed to runtime mutex we don't do passive spinning here,
    // because there can be work on global runq on on other Ps.
    if i >= active_spin || ncpu <= 1 || gomaxprocs <= int32(sched.npidle+sched.nmspinning)+1 {
        return false
    }
    if p := getg().m.p.ptr(); !runqempty(p) {
        return false
    }
    return true
}

func sync_runtime_doSpin() {
    procyield(active_spin_cnt)
}
Unlock
Mutex的Unlock函数定义如下// Unlock unlocks m.
// It is a run-time error if m is not locked on entry to Unlock.
//
// A locked Mutex is not associated with a particular goroutine.
// It is allowed for one goroutine to lock a Mutex and then
// arrange for another goroutine to unlock it.
func (m *Mutex) Unlock() {
    if race.Enabled {
        _ = m.state
        race.Release(unsafe.Pointer(m))
    }

    // Fast path: drop lock bit.
    new := atomic.AddInt32(&m.state, -mutexLocked)
    if (new+mutexLocked)&mutexLocked == 0 {
        panic("sync: unlock of unlocked mutex")
    }

    old := new
    for {
        // If there are no waiters or a goroutine has already
        // been woken or grabbed the lock, no need to wake anyone.
        if old>>mutexWaiterShift == 0 || old&(mutexLocked|mutexWoken) != 0 {
            return
        }
        // Grab the right to wake someone.
        new = (old - 1<<mutexWaiterShift) | mutexWoken
        if atomic.CompareAndSwapInt32(&m.state, old, new) {
            runtime_Semrelease(&m.sema)
            return
        }
        old = m.state
    }
}函数入口处的四行代码和race detection相关，暂时不用管。接下来的四行代码是判断是否是嵌套锁。new是m.state-1之后的值。我们重点看for循环内部的代码。if old>>mutexWaiterShift == 0 || old&(mutexLocked|mutexWoken) != 0 {
    return
}这两句是说：如果阻塞在该锁上的goroutine数目为0或者mutex处于lock或者唤醒状态，则返回。new = (old - 1<<mutexWaiterShift) | mutexWoken
if atomic.CompareAndSwapInt32(&m.state, old, new) {
    runtime_Semrelease(&m.sema)
    return
}这里先将阻塞在mutex上的goroutine数目减一，然后将mutex置于唤醒状态。runtime_Semrelease和runtime_Semacquire的作用刚好相反，将阻塞在信号量上goroutine唤醒。有人可能会问唤醒的是哪个goroutine，那么我们可以看一下goroutine wait list的入队列和出队列代码。func (root *semaRoot) queue(addr *uint32, s *sudog) {
    s.g = getg()
    s.elem = unsafe.Pointer(addr)
    s.next = nil
    s.prev = root.tail
    if root.tail != nil {
        root.tail.next = s
    } else {
        root.head = s
    }
    root.tail = s
}

func (root *semaRoot) dequeue(s *sudog) {
    if s.next != nil {
        s.next.prev = s.prev
    } else {
        root.tail = s.prev
    }
    if s.prev != nil {
        s.prev.next = s.next
    } else {
        root.head = s.next
    }
    s.elem = nil
    s.next = nil
    s.prev = nil
}如上所示，wait list入队是插在队尾，出队是从头出。
参考

《Go语言学习笔记》

Go 中的锁源码实现：Mutex，首发于文章 - 伯乐在线。</content>
</doc>
<doc>
	<docid>14</docid>
	<title>Linux 新的API signalfd、timerfd、eventfd使用说明</title>
	<link>http://blog.jobbole.com/106933/</link>
	<author></author>
	<content>三种新的fd加入linux内核的的版本：
signalfd：2.6.22
timerfd：2.6.25
eventfd：2.6.22
三种fd的意义：
lsignalfd
传统的处理信号的方式是注册信号处理函数；由于信号是异步发生的，要解决数据的并发访问，可重入问题。signalfd可以将信号抽象为一个文件描述符，当有信号发生时可以对其read，这样可以将信号的监听放到select、poll、epoll等监听队列中。
ltimerfd
可以实现定时器的功能，将定时器抽象为文件描述符，当定时器到期时可以对其read，这样也可以放到监听队列的主循环中。
leventfd
实现了线程之间事件通知的方式，也可以用于用户态和内核通信。eventfd的缓冲区大小是sizeof(uint64_t)；向其write可以递增这个计数器，read操作可以读取，并进行清零；eventfd也可以放到监听队列中，当计数器不是0时，有可读事件发生，可以进行读取。
三种新的fd都可以进行监听，当有事件触发时，有可读事件发生。
signalfd涉及API：


点击(此处)折叠或打开#include <sys/signalfd.h> 
int signalfd(int fd, const sigset_t *mask, int flags);


参数fd：如果是-1则表示新建一个，如果是一个已经存在的则表示修改signalfd所关联的信号；
参数mask：信号集合；
参数flag：内核版本2.6.27以后支持SFD_NONBLOCK、SFD_CLOEXEC；
成功返回文件描述符，返回的fd支持以下操作：read、select(poll、epoll)、close
l例子#include <sys/signalfd.h> 
  #include <signal.h> 
  #include <unistd.h> 
  #include <stdlib.h> 
  #include <stdio.h> 
 
  #define handle_error(msg) \ 
  do { perror(msg); exit(EXIT_FAILURE); } while (0) 
 
  int main(int argc, char *argv[]) 
  { 
    sigset_t mask; 
    int sfd; 
    struct signalfd_siginfo fdsi; 
    ssize_t s; 
 
    sigemptyset(&mask); 
    sigaddset(&mask, SIGINT); 
    sigaddset(&mask, SIGQUIT); 
 
    if (sigprocmask(SIG_BLOCK, &mask, NULL) == -1) 
        handle_error("sigprocmask"); 
 
    sfd = signalfd(-1, &mask, 0); 
    if (sfd == -1) 
        handle_error("signalfd"); 
 
    for (;;) { 
        s = read(sfd, &fdsi, sizeof(struct signalfd_siginfo)); 
        if (s != sizeof(struct signalfd_siginfo)) 
            handle_error("read"); 
 
        if (fdsi.ssi_signo == SIGINT) { 
           printf("Got SIGINT\n"); 
        } else if (fdsi.ssi_signo == SIGQUIT) { 
        printf("Got SIGQUIT\n"); 
        exit(EXIT_SUCCESS); 
        } else { 
        printf("Read unexpected signal\n"); 
        } 
     } 
  }L17-L21：将感兴趣的信号加入到sigset_t中；
L24：调用signalfd，把信号集与fd关联起来，第一个参数为-1表示新建一个signalfd，不是-1并且是一个合法的signalfd表示向其添加新的信号。
L29：阻塞等待信号的发生并读取。根据读取的结果可以知道发生了什么信号。
timerfd涉及的API

#include <sys/timerfd.h> 
int timerfd_create(int clockid, int flags); 
int timerfd_settime(int fd, int flags, const struct itimerspec *new_value,struct itimerspec *old_value); 
int timerfd_gettime(int fd, struct itimerspec *curr_value);

timerfd_create：创建一个timerfd；返回的fd可以进行如下操作：read、select(poll、epoll)、close
timerfd_settime：设置timer的周期，以及起始间隔
timerfd_gettime：获取到期时间。//函数参数中数据结构如下： 
struct timespec 
{ 
    time_t tv_sec; /* Seconds */ 
    long tv_nsec; /* Nanoseconds */ 
}; 
  struct itimerspec 
{ 
    struct timespec it_interval; /* Interval for periodic timer */ 
    struct timespec it_value; /* Initial expiration */ 
};

l例子

#include <sys/timerfd.h> 
  #include <sys/time.h> 
  #include <time.h> 
  #include <unistd.h> 
  #include <stdlib.h> 
  #include <stdio.h> 
  #include <stdint.h> /* Definition of uint64_t */ 
 
  #define handle_error(msg) \ 
  do { perror(msg); exit(EXIT_FAILURE); } while (0) 
 
  void printTime() 
  { 
      struct timeval tv; 
      gettimeofday(&tv, NULL); 
      printf("printTime: current time:%ld.%ld ", tv.tv_sec, tv.tv_usec); 
  } 
 
  int main(int argc, char *argv[]) 
  { 
      struct timespec now; 
      if (clock_gettime(CLOCK_REALTIME, &now) == -1) 
          handle_error("clock_gettime"); 
 
      struct itimerspec new_value; 
      new_value.it_value.tv_sec = now.tv_sec + atoi(argv[1]); 
      new_value.it_value.tv_nsec = now.tv_nsec; 
      new_value.it_interval.tv_sec = atoi(argv[2]); 
      new_value.it_interval.tv_nsec = 0; 
 
      int fd = timerfd_create(CLOCK_REALTIME, 0); 
      if (fd == -1) 
      handle_error("timerfd_create"); 
 
      if (timerfd_settime(fd, TFD_TIMER_ABSTIME, &new_value, NULL) == -1) 
          handle_error("timerfd_settime"); 
 
      printTime(); 
      printf("timer started\n"); 
 
      for (uint64_t tot_exp = 0; tot_exp < atoi(argv[3]);) 
      { 
          uint64_t exp; 
          ssize_t s = read(fd, &exp, sizeof(uint64_t)); 
          if (s != sizeof(uint64_t)) 
              handle_error("read"); 
 
          tot_exp += exp; 
          printTime(); 
          printf("read: %llu; total=%llu\n",exp, tot_exp); 
  } 
 
  exit(EXIT_SUCCESS); 
 }代码L25-L29：初始化定时器的参数，初始间隔与定时间隔。
L32：创建定时器fd，CLOCK_REALTIME：真实时间类型，修改时钟会影响定时器；CLOCK_MONOTONIC：相对时间类型，修改时钟不影响定时器。
L35：设置定时器的值。
L44：阻塞等待定时器到期。返回值是未处理的到期次数。比如定时间隔为2秒，但过了10秒才去读取，则读取的值是5。
编译运行：编译时要加rt库(g++ -lrt timerfd.cc -o timerfd)
[root@localhost appTest]# ./timerfd 5 2 10
printTime:  current time:1357391736.146196 timer started
printTime:  current time:1357391741.153430 read: 1; total=1
printTime:  current time:1357391743.146550 read: 1; total=2
printTime:  current time:1357391745.151483 read: 1; total=3
printTime:  current time:1357391747.161155 read: 1; total=4
printTime:  current time:1357391749.153934 read: 1; total=5
printTime:  current time:1357391751.157309 read: 1; total=6
printTime:  current time:1357391753.158384 read: 1; total=7
printTime:  current time:1357391755.150470 read: 1; total=8
printTime:  current time:1357391757.150253 read: 1; total=9
printTime:  current time:1357391759.149954 read: 1; total=10
[root@localhost appTest]#
第一个参数5为第一次定时器到期间隔，第二个参数2为定时器的间隔，第三个参数为定时器到期10次则退出。程序运行(5+2*10)S退出。
详细信息可以：man timerfd_create
eventfd涉及API：

#include <sys/eventfd.h> 
int eventfd(unsigned int initval, int flags);

创建一个eventfd，这是一个计数器相关的fd，计数器不为零是有可读事件发生，read以后计数器清零，write递增计数器；返回的fd可以进行如下操作：read、write、select(poll、epoll)、close。
这个函数会创建一个事件对象 (eventfd object), 用来实现，进程(线程)间的等待/通知(wait/notify) 机制. 内核会为这个对象维护一个64位的计数器(uint64_t)。并且使用第一个参数(initval)初始化这个计数器。调用这个函数就会返回一个新的文件描述符(event object)。2.6.27版本开始可以按位设置第二个参数(flags)。有如下的一些宏可以使用：
lEFD_NONBLOCK
功能同open(2)的O_NONBLOCK，设置对象为非阻塞状态，如果没有设置这个状态的话，read(2)读eventfd,并且计数器的值为0 就一直堵塞在read调用当中，要是设置了这个标志， 就会返回一个 EAGAIN 错误(errno = EAGAIN)。效果也如同 额外调用select(2)达到的效果。
lEFD_CLOEXEC
这个标识被设置的话，调用exec后会自动关闭文件描述符，防止泄漏。如果是2.6.26或之前版本的内核，flags 必须设置为0。
创建这个对象后，可以对其做如下操作：
1) write： 将缓冲区写入的8字节整形值加到内核计数器上。
2) read： 读取8字节值， 并把计数器重设为0. 如果调用read的时候计数器为0， 要是eventfd是阻塞的， read就一直阻塞在这里，否则就得到 一个EAGAIN错误。如果buffer的长度小于8那么read会失败， 错误代码被设置成 EINVAL。
3) poll select epoll
4) close: 当不需要eventfd的时候可以调用close关闭， 当这个对象的所有句柄都被关闭的时候，内核会释放资源。 为什么不是close就直接释放呢， 如果调用fork 创建
进程的时候会复制这个句柄到新的进程，并继承所有的状态。
l例子
#include <sys/eventfd.h>
 #include <unistd.h>
 #include <stdio.h>
 #include <stdint.h>
 #include <stdlib.h>
 #include <errno.h>
 #define handle_error(msg) \
    do { perror(msg); exit(1); } while (0)
int main( int argc, char **argv ){
     uint64_t u;
     ssize_t s;5 int j;
     if ( argc < 2 ) {
        fprintf(stderr, "input in command argument");
         exit(1);
     }
 
     int efd;
     if ( (efd = eventfd(0, EFD_NONBLOCK)) == -1 )
             handle_error("eventfd failed");
 
 
     switch (fork()) {
         case 0:
             for( j = 1; j < argc; j ++ ) {
                 printf("Child writing %s to efd\n", argv[j] );
             
                 u = strtoull(argv[j], NULL, 0); /* analogesly atoi */
                 s = write(efd, &u, sizeof(uint64_t));/*append u to counter */
                 if ( s != sizeof(uint64_t) )
                     handle_error("write efd failed");
 
             }
             printf("child completed write loop\n");
 
             exit(0);
         default:
             sleep (2);
             
             printf("parent about to read\n");
             s = read(efd, &u, sizeof(uint64_t));
             if ( s != sizeof(uint64_t) ) {
                 if (errno = EAGAIN) {
                     printf("Parent read value %d\n", s);
                     return 1;
                 }
                 handle_error("parent read failed");
             }
             printf("parent read %d , %llu (0x%llx) from efd\n",
                     s, (unsigned long long)u, (unsigned long long) u);
             exit(0);
 
         case -1:
             handle_error("fork ");
     }
     return 0;
}

Linux 新的API signalfd、timerfd、eventfd使用说明，首发于文章 - 伯乐在线。</content>
</doc>
<doc>
	<docid>15</docid>
	<title>一个屌丝程序猿的人生（39）</title>
	<link>http://blog.jobbole.com/106922/</link>
	<author></author>
	<content>
本系列：第 1 篇 、第 2 篇、第 3 篇、（4）、（5）、（6）、（7）、（8）、（9）、10）、（11）、（12）、（13）、（14）、（15）、（16）、（17）、（18）、（19）、（20）、（21）、（22）、（23）、（24）、（25）、（26）、（27）、（28）、（29）、（30）、（31）、（32）、（33）、（34）、（35）、（36）、（37）、（38）

有史晓玲这么一位颜值不错的妹子常伴左右，换作一般人还真的很难静下心来学习，但是林萧却很清楚，在眼前的情况下，撩妹和学习到底孰轻孰重。
更何况，这妹子可是怀了张亮的孩子的，就算是要撩妹，林萧也还没到饥不择食的地步，拿自己兄弟的妹子下手。
这事儿特么不地道啊！
于是，心无旁鹭的林萧，在结束了Java基础的学习之后，立即便开始了接下来的视频学习。
&#8230;&#8230;
紧接着Java基础的一章，是Java高级特性。
不知为何，程序猿总会对这些听起来很牛逼的词汇感兴趣，因此，一看到“高级特性”四个字，林萧立马就来了精神。
然而事实上，真正牛逼的东西，往往不是那么好理解的。至少对于一个初入Java领域的菜鸟来说，“高级特性”这四个字还是太过深奥了一些。
就说“高级特性”中最开始的内容吧，面向对象的三个重要特性，继承、封装以及多态。
其中继承其实已经是最好理解的一个了，原因是继承在现实中的例子实在是太多了，最典型的就是父子之间的关系。
儿子会继承老爸身上某一部分的特征，并且还会拥有老爸身上所没有的一些特点。这正是诠释着，子类可以继承父类的属性和方法，并且通常还会拥有父类所没有的一些属性和方法。
与此同时，儿子虽然可以继承老爸的一些特征，但也并不是所有特征都可以继承的。因此，这也正是诠释着，子类只能继承父类部分的属性和方法，一些隐藏比较深的，例如被private所修饰的，子类也是继承不了的。
知道继承的以上两点，对于一个新人来说，就基本上足够了。至于一些有关继承比较高级的用法，例如模板方法模式这种的，就只能在以后的路上，慢慢研究去了。
封装相较于继承，会比较难理解一些。不过对于很抽象很难理解的知识，林萧有自己的办法，那就是找相似的事物。
因为林萧相信，世间万物都是相通的，任何一个知识点，你总能在其它领域找到相似的。
就说谈恋爱这回事吧，都说没有最好的，只有最合适的，这句话，其实也同样可以适用于找工作，这就是一个最简单的道理相通的例子。
知道了这一点之后，如何找到一个合适的例子，就比较关键了。
关于封装的现实例子，其实说起来还是蛮多的。就说叫外卖这件事吧，其实卖家就充分体现了封装的思想。
那么卖家到底封装了哪些东西？
很显然，大致包括了买菜、洗菜、做饭、送饭等等一系列的动作，而你只需要付钱给外卖员，就可以吃到香喷喷的饭菜。你不需要知道如何买菜，如何做饭等等这些琐碎的事情，你要做的就一件事，就是掏钱！
这就是现实中封装的思想，也就是所谓的一站式服务。
对应到编程世界，其实也是一样的，一个类要尽可能把自己的服务细节隐藏起来，不对外部暴露，让外部的类只需要“付钱”，就可以方便的调用服务即可。
隐藏细节，这其实就是封装的核心思想了。
三大特性中，继承和封装都说完了，还剩下最后一个特性，也是最难理解的一个。
多态在很多人刚接触Java时，都难免会觉得懵逼，林萧也不例外。尽管他一直在试图理解“不同的类型有不同的表现”这句话，但其实他在当时，始终都没有理解到多态的本质。
多态的本质其实用一句话就可以概括，就是“编译时和运行时才决定对象的行为”，俗称为静态分派和动态分派。
很多人第一次看到这两个词的时候，应该比看到多态更加懵逼，但其实沉下心来去想想，这两个词其实非常好理解。
不过理解的前提是，你要非常清楚一个Java程序从编写到运行的过程，简单的说，这个过程就是编译和执行。
而静态分派和动态分派的本质就是，静态分派是编译期间就可以决定的，而动态分派则是执行期间才可以决定的。
要理解编译期和执行期其实也非常好理解，你可以把计算机看做是一个英国人，而你是一个中国人，程序就是你写的一篇文章。
那么计算机要想运行你的程序，也就相当于英国人要想读你的文章。
那就首先得有人把你的文章翻译成英文，这个过程就相当于编译的过程。这个负责翻译的人，在现实里就叫翻译员，而在程序世界里，就是编译器。
当文章被翻译成英文以后，英国人就开始阅读你的文章，也就相当于计算机开始运行你的程序。
而对于Java程序来说，编译器其实就是javac，而所谓的计算机，其实就是JVM。
理解这个最基本的编译和执行的过程，静态分派和动态分派就很好理解了。而理解了静态分派和动态分派，多态就非常好理解了。
当然了，这些知识，对于初学Java的林萧来说，当然是不太可能彻底理解的，不过这其实并无大碍，学习本身就是一个循序渐进的过程。
&#8230;&#8230;
说起来，林萧也算是够拼命的，自从开始培训以后，几乎就变成了足不出户的骨灰级宅男，与当初在家里玩游戏的时候，简直是如出一撤。
一天下来，除了上厕所和拿外卖以外，林萧几乎可以做到坐在椅子上一动不动。这种惊人的毅力和懒的程度，如果不是以前玩游戏早已经习惯了的话，说不定林萧还真坚持不下来。
但是，这种生活虽然看起来挺苦逼的，林萧却非常乐在其中。而且，游戏所带来的快感，与知识所带来的快感相比，始终缺少了一种充实感。
这种充实感，可以帮你驱散孤独，也会让你有种，未来掌握在自己手里的安全感。
此时此刻，林萧心中多少有些明白了，为什么有人说，成功的男人要学会享受孤独。因为享受孤独的过程，其实就是你充实自己的过程。
然而，就在林萧尽情享受孤独，疯狂得充实自己的时候，一个故人却再次打断了这种疯狂而又宁静的生活。
她，来了。
 

一个屌丝程序猿的人生（39），首发于文章 - 伯乐在线。</content>
</doc>
<doc>
	<docid>16</docid>
	<title>C++ 内存分配(new，operator new)详解</title>
	<link>http://blog.jobbole.com/106923/</link>
	<author></author>
	<content>本文主要讲述C++ new运算符和operator new, placement new之间的种种关联，new的底层实现，以及operator new的重载和一些在内存池，STL中的应用。
一 new 运算符和 operator new()：
new：指我们在C++里通常用到的运算符，比如A* a = new A;  对于new来说，有new和::new之分，前者位于std
operator new()：指对new的重载形式，它是一个函数，并不是运算符。对于operator new来说，分为全局重载和类重载，全局重载是void* ::operator new(size_t size)，在类中重载形式 void* A::operator new(size_t size)。还要注意的是这里的operator new()完成的操作一般只是分配内存，事实上系统默认的全局::operator new(size_t size)也只是调用malloc分配内存，并且返回一个void*指针。而构造函数的调用(如果需要)是在new运算符中完成的。
先简单解释一下new和operator new之间的关系：
关于这两者的关系，我找到一段比较经典的描述（来自于www.cplusplus.com 见参考文献）：
operator new can be called explicitly as a regular function, but in C++, new is an operator with a very specific behavior: An expression with the new operator, first calls function operator new (i.e., this function) with the size of its type specifier as first argument, and if this is successful, it then automatically initializes or constructs the object (if needed). Finally, the expression evaluates as a pointer to the appropriate type.
比如我们写如下代码：A* a = new A;我们知道这里分为两步：1.分配内存，2.调用A()构造对象。事实上，分配内存这一操作就是由operator new(size_t)来完成的，如果类A重载了operator new，那么将调用A::operator new(size_t )，如果没有重载，就调用::operator new(size_t )，全局new操作符由C++默认提供。因此前面的两步也就是：1.调用operator new 2.调用构造函数。这里再一次提出来是因为后面关于这两步会有一些变形，在关于placement new那里会讲到。
先举个简单例子//平台：Visual Stdio 2008  
#include<iostream>  
class A  
{  
public:  
     A()  
     {  
          std::cout<<"call A constructor"<<std::endl;  
     }  
  
     ~A()  
     {  
          std::cout<<"call A destructor"<<std::endl;  
     }  
}  
int _tmain(int argc, _TCHAR* argv[])  
{  
  
     A* a = new A;  
     delete a;  
  
     system("pause");  
     return 0;  
}下面我们跟踪一下A反汇编代码，由于Debug版本反汇编跳转太多，因此此处通过Release版本在A* a = new A;处设断点反汇编：
在Release版本中，构造函数和析构函数都是直接展开的。A* a = new A;  
01301022  push        1    ;不含数据成员的类占用一字节空间，此处压入sizeof(A)  
01301024  call        operator new (13013C2h) ;调用operator new(size_t size)  
01301029  mov         esi,eax ;返回值保存到esi  
0130102B  add         esp,4 ;平衡栈  
0130102E  mov         dword ptr [esp+8],esi ;  
01301032  mov         dword ptr [esp+14h],0   
0130103A  test        esi,esi ;在operator new之后，检查其返回值，如果为空(分配失败)，则不调用A()构造函数  
0130103C  je          wmain+62h (1301062h) ;为空 跳过构造函数部分  
0130103E  mov         eax,dword ptr [__imp_std::endl (1302038h)] ;构造函数内部，输出字符串  
01301043  mov         ecx,dword ptr [__imp_std::cout (1302050h)]   
01301049  push        eax    
0130104A  push        offset string "call A constructor" (1302134h)   
0130104F  push        ecx    
01301050  call        std::operator<<<std::char_traits<char> > (13011F0h)   
01301055  add         esp,8   
01301058  mov         ecx,eax   
0130105A  call        dword ptr [__imp_std::basic_ostream<char,std::char_traits<char> >::operator<< (1302040h)]   
01301060  jmp         wmain+64h (1301064h) ;构造完成，跳过下一句  
01301062  xor         esi,esi ;将esi置空，这里的esi即为new A的返回值  
01301064  mov         dword ptr [esp+14h],0FFFFFFFFh   
    delete a;  
0130106C  test        esi,esi ;检查a是否为空  
0130106E  je          wmain+9Bh (130109Bh) ;如果为空，跳过析构函数和operator delete  
01301070  mov         edx,dword ptr [__imp_std::endl (1302038h)] ;析构函数 输出字符串  
01301076  mov         eax,dword ptr [__imp_std::cout (1302050h)]   
0130107B  push        edx    
0130107C  push        offset string "call A destructor" (1302148h)   
01301081  push        eax    
01301082  call        std::operator<<<std::char_traits<char> > (13011F0h)   
01301087  add         esp,8   
0130108A  mov         ecx,eax   
0130108C  call        dword ptr [__imp_std::basic_ostream<char,std::char_traits<char> >::operator<< (1302040h)]   
01301092  push        esi  ;压入a   
01301093  call        operator delete (13013BCh) ;调用operator delete   
01301098  add         esp,4   
通过反汇编可以看出A* = new A包含了operator new(sizeof(A))和A()两个步骤(当然，最后还要将值返回到a)  
         delete a包含了~A()和operator delete(a)两个步骤。
二 operator new 的三种形式：
operator new有三种形式：
throwing (1)void* operator new (std::size_t size) throw (std::bad_alloc);nothrow (2)void* operator new (std::size_t size, const std::nothrow_t& nothrow_value) throw();placement (3)void* operator new (std::size_t size, void* ptr) throw();(1)(2)的区别仅是是否抛出异常，当分配失败时，前者会抛出bad_alloc异常，后者返回null，不会抛出异常。它们都分配一个固定大小的连续内存。
用法示例：A* a = new A; //调用throwing(1)
A* a = new(std::nothrow) A; //调用nothrow(2)（3）是placement new，它也是对operator new的一个重载，定义于中，它多接收一个ptr参数，但它只是简单地返回ptr。
其在new.h下的源代码如下：#define __PLACEMENT_NEW_INLINE  
inline void *__cdecl operator new(size_t, void *_P)  
        {return (_P); }  
#if     _MSC_VER >= 1200  
inline void __cdecl operator delete(void *, void *)  
    {return; }  
#endif  
#endif那么它究竟有什么用呢？事实上，它可以实现在ptr所指地址上构建一个对象(通过调用其构造函数)，这在内存池技术上有广泛应用。
它的调用形式为：new(p) A(); //也可用A(5)等有参构造函数。前面说到，new运算符都会调用operator new，而这里的operator new(size_t, void*)并没有什么作用，真正起作用的是new运算符的第二个步骤：在p处调用A构造函数。这里的p可以是动态分配的内存，也可以是栈中缓冲，如char buf[100]; new(buf) A();
我们仍然可以通过一个例子来验证：#include <iostream>  
class A  
{  
public:  
    A()  
    {  
        std::cout<<"call A constructor"<<std::endl;  
    }  
  
    ~A()  
    {  
        std::cout<<"call A destructor"<<std::endl;  
    }  
};  
int _tmain(int argc, _TCHAR* argv[])  
{  
  
    A* p = (A*)::operator new(sizeof(A)); //分配  
  
    new(p) A(); //构造  
      
    p->~A(); //析构  
  
    ::operator delete(p); //释放  
  
    system("pause");  
    return 0;  
}
上面的代码将对象的分配，构造，析构和释放分离开来，这也是new和delete运算符两句就能完成的操作。
先直接运行可以看到程序输出：
再分别注释掉new(a) A();和a->~A();两句，可以看到对应的构造和析构函数将不会被调用。
然后查看反汇编：
平台: Visual Studio 2008 Debug版A* a = (A*)::operator new(sizeof(A)); //分配  
00F9151D  push        1      
00F9151F  call        operator new (0F91208h) ;调用::operator new(size_t size)也就是throwing(1)版本  
00F91524  add         esp,4   
00F91527  mov         dword ptr [ebp-14h],eax ;返回地址放入[ebp-14h] 即为p  
  
    new(a) A(); //构造  
00F9152A  mov         eax,dword ptr [ebp-14h]   
00F9152D  push        eax    
00F9152E  push        1    ;压入p  
00F91530  call        operator new (0F91280h);调用operator new(size_t, void* p)即placement(3)版本 只是简单返回p  
00F91535  add         esp,8   
00F91538  mov         dword ptr [ebp-0E0h],eax ;将p放入[ebp-0E0h]  
00F9153E  mov         dword ptr [ebp-4],0   
00F91545  cmp         dword ptr [ebp-0E0h],0   ;判断p是否为空  
00F9154C  je          wmain+81h (0F91561h)     ;如果为空 跳过构造函数  
00F9154E  mov         ecx,dword ptr [ebp-0E0h] ;取出p到ecx  
00F91554  call        A::A (0F91285h)          ;调用构造函数 根据_thiscall调用约定 this指针通过ecx寄存器传递  
00F91559  mov         dword ptr [ebp-0F4h],eax ;将返回值(this指针)放入[ebp-0F4h]中  
00F9155F  jmp         wmain+8Bh (0F9156Bh)     ;跳过下一句  
00F91561  mov         dword ptr [ebp-0F4h],0   ;将[ebp-0F4h]置空 当前面判断p为空时执行此语句  
00F9156B  mov         ecx,dword ptr [ebp-0F4h] ;[ebp-0F4h]为最终构造完成后的this指针(或者为空) 放入ecx  
00F91571  mov         dword ptr [ebp-0ECh],ecx ;又将this放入[ebp-0ECh] 这些都是调试所用  
00F91577  mov         dword ptr [ebp-4],0FFFFFFFFh   
      
    a->~A(); //析构  
00F9157E  push        0      
00F91580  mov         ecx,dword ptr [ebp-14h] ;从[ebp-14h]中取出p  
00F91583  call        A::`scalar deleting destructor' (0F91041h) ;调用析构函数(跟踪进去比较复杂 如果在Release下，构造析构函数都是直接展开的)  
  
    ::operator delete(a); //释放  
00F91588  mov         eax,dword ptr [ebp-14h]   ;将p放入eax  
00F9158B  push        eax           ;压入p  
00F9158C  call        operator delete (0F910B9h);调用operator delete(void* )  
00F91591  add         esp,4 </span>从反汇编中可以看出，其实operator new调用了两次，只不过每一次调用不同的重载函数，并且placement new的主要作用只是将p放入ecx，并且调用其构造函数。
事实上，在指定地址上构造对象还有另一种方法，即手动调用构造函数：p->A::A(); 这里要加上A::作用域，否则编译器会报错：error C2273: “函数样式转换”: 位于“->”运算符右边时非法
用p->A::A();替换掉new(p) A();仍然能达到同样的效果，反汇编：A* a = (A*)::operator new(sizeof(A)); //分配  
010614FE  push        1      
01061500  call        operator new (1061208h)   
01061505  add         esp,4   
01061508  mov         dword ptr [a],eax   
  
    //new(a) A();   //构造  
    a->A::A();  
0106150B  mov         ecx,dword ptr [a]   
0106150E  call        operator new (1061285h)   
  
    a->~A(); //析构  
01061513  push        0      
01061515  mov         ecx,dword ptr [a]   
01061518  call        A::`scalar deleting destructor' (1061041h)   
  
    ::operator delete(a); //释放  
0106151D  mov         eax,dword ptr [a]   
01061520  push        eax    
01061521  call        operator delete (10610B9h)   
01061526  add         esp,4比之前的方法更加简洁高效(不需要调用placement new)。不知道手动调用构造函数是否有违C++标准或有什么隐晦，我在其他很多有名的内存池(包括SGI STL alloc)实现上看到都是用的placement new，而不是手动调用构造函数。
三 operator new 重载：
前面简单提到过 A* p = new A；所发生的事情：先调用operator new，如果类A重载了operator new，那么就使用该重载版本，否则使用全局版本::operatro new(size_t size)。那么类中可以重载operator new的哪些版本？全局operator new可以重载吗？全局和类中重载分别会在什么时机调用？
1.在类中重载 operator new
上面提到的throwing(1)和nothrow(2)的operator new是可以被重载的，比如：#include <iostream>  
class A  
{  
public:  
    A()  
    {  
        std::cout<<"call A constructor"<<std::endl;  
    }  
  
    ~A()  
    {  
        std::cout<<"call A destructor"<<std::endl;  
    }  
    void* operator new(size_t size)  
    {  
        std::cout<<"call A::operator new"<<std::endl;  
        return malloc(size);  
    }  
  
    void* operator new(size_t size, const std::nothrow_t& nothrow_value)  
    {  
        std::cout<<"call A::operator new nothrow"<<std::endl;  
        return malloc(size);  
    }  
};  
int _tmain(int argc, _TCHAR* argv[])  
{  
    A* p1 = new A;  
    delete p1;  
  
    A* p2 = new(std::nothrow) A;  
    delete p2;  
  
    system("pause");  
    return 0;  
}

如果类A中没有对operator new的重载，那么new A和new(std::nothrow) A;都将会使用全局operator new(size_t size)。可将A中两个operator new注释掉，并且在A外添加一个全局operator new重载：void* ::operator new(size_t size)  
{  
    std::cout<<"call global operator new"<<std::endl;  
    return malloc(size);  
}程序输出：

注意，这里的重载遵循作用域覆盖原则，即在里向外寻找operator new的重载时，只要找到operator new()函数就不再向外查找，如果参数符合则通过，如果参数不符合则报错，而不管全局是否还有相匹配的函数原型。比如如果这里只将A中operator new(size_t, const std::nothrow_t&)删除掉，就会报错：
error C2660: “A::operator new”: 函数不接受 2 个参数。
至于placement new，它本身就是operator new的一个重载，不需也尽量不要对它进行改写，因为它一般是搭配 new(p) A(); 工作的，它的职责只需简单返回指针。
对operator new的重载还可以添加自定义参数，如在类A中添加void* operator new(size_t size, int x, int y, int z)  
{  
    std::cout<<"X="<<x<<"  Y="<<y<<" Z="<<z<<std::endl;  
    return malloc(size);  
}这种重载看起来没有什么大作用，因为它operator new需要完成的任务只是分配内存，但是通过对这类重载的巧妙应用，可以让它在动态分配内存调试和检测中大展身手。这将在后面operator new重载运用技巧中，展现。
2.重载全局 operator new
全局operator new的重载和在类中重载并无太大区别，当new A;时，如果类A中没有重载operator new，那么将调用全局operator new函数，如果没有重载全局operator new，最后会调用默认的全局operator new。
3.类中operator new和全局 operator new 的调用时机
前面已经提到了在new时的调用顺序，但是这里提出来的原因是还存在一个全局的new运算符，也就是::new，这个运算符会直接调用全局operator new，并且也会调用构造函数。这可能让人很犯迷糊，只做了解即可。这里提到的调用时机都是指通过new运算符调用，没有讨论其他情况，比如主动调用。
四 operator new运用技巧和一些实例探索
1.operator new 重载运用于调试：
前面提到如何operator new的重载是可以有自定义参数的，那么我们如何利用自定义参数获取更多的信息呢，这里一个很有用的做法就是给operator new添加两个参数:char* file, int line,这两个参数记录new运算符的位置，然后再在new时将文件名和行号传入，这样我们就能在分配内存失败时给出提示：输出文件名和行号。
那么如何获取当前语句所在文件名和行号呢，windows提供两个宏：__FILE__和__LINE__。利用它们可以直接获取到文件名和行号，也就是 new(__FILE__, __LINE__) 由于这些都是不变的，因此可以再定义一个宏：#define new new(__FILE__, __LINE__)。这样我们就只需要定义这个宏，然后重载operator new即可。
源代码如下，这里只是简单输出new的文件名和行号。//A.h  
class A  
{  
public:  
    A()  
    {  
        std::cout<<"call A constructor"<<std::endl;  
    }  
  
    ~A()  
    {  
        std::cout<<"call A destructor"<<std::endl;  
    }  
  
    void* operator new(size_t size, const char* file, int line)  
    {  
        std::cout<<"call A::operator new on file:"<<file<<"  line:"<<line<<std::endl;  
        return malloc(size);  
        return NULL;  
    }  
  
};  
//Test.cpp  
#include <iostream>  
#include "A.h"  
#define new new(__FILE__, __LINE__)  
  
int _tmain(int argc, _TCHAR* argv[])  
{  
    A* p1 = new A;  
    delete p1;  
  
    A* p2 = new A;  
    delete p2;  
  
    system("pause");  
    return 0;  
}输出：


注意：需要将类的声明实现与new的使用隔离开来。并且将类头文件放在宏定义之前。否则在类A中的operator new重载中的new会被宏替换，整个函数就变成了： void* operator new(__FILE__, __LINE__)(size_t size, char* file, int line)
编译器自然会报错。
2.内存池优化
operator new的另一个大用处就是内存池优化，内存池的一个常见策略就是分配一次性分配一块大的内存作为内存池(buffer或pool)，然后重复利用该内存块，每次分配都从内存池中取出，释放则将内存块放回内存池。在我们客户端调用的是new运算符，我们可以改写operator new函数，让它从内存池中取出(当内存池不够时，再从系统堆中一次性分配一块大的)，至于构造和析构则在取出的内存上进行，然后再重载operator delete，它将内存块放回内存池。关于内存池和operator new在参考文献中有一篇很好的文章。这里就不累述了。
3.STL中的 new
在SGI STL源码中,defalloc.h和stl_construct.h中提供了最简单的空间配置器(allocator)封装，见《STL源码剖析》P48。它将对象的空间分配和构造分离开来，虽然在defalloc.h中仅仅是对::operator new和::operator delete的一层封装，但是它仍然给STL容器提供了更加灵活的接口
SGI STL真正使用的并不是defalloc.h中的分配器，而是stl_alloc.h中的SGI精心打造的&#8221;双层级配置器&#8221;，它将内存池技术演绎得淋漓尽致，值得细细琢磨。顺便提一下，在stl_alloc.h中并没有使用::operator new/delete 而直接使用malloc和free。具体缘由均可参见《STL源码剖析》。
五 delete 的使用
delete的使用基本和new一致，包括operator delete的重载方式这些都相似，只不过它的参数是void*，返回值为void。但是有一点需要注意，operator delete的自定义参数重载并不能手动调用。比如void* operator new(size_t size, int x)  
{  
    cout<<" x = "<<x<<endl;  
    return malloc(size);      
}  
void operator delete(void* p, int x)  
{  
    cout<<" x = "<<x<<endl;  
    free(p);  
}如下调用是无法通过的：A* p = new(3) A;//Ok
delete(3) p;//error C2541: “delete”: 不能删除不是指针的对象那么重载operator delete有什么作用？如何调用？事实上以上自定义参数operator delete 只在一种情况下被调用：当new运算符抛出异常时。
可以这样理解，只有在new运算符中，编译器才知道你调用的operator new形式，然后它会调用对应的operator delete。一旦出了new运算符，编译器对于你自定义的new将一无所知，因此它只会按照你指定的delete运算符形式来调用operator delete，而至于为什么不能指定调用自定义delete(也就是只能老老实实delete p)，这个就不知道了。
细心观察的话，上面operator new用于调试的例子代码中，由于我们没有给出operator new对应的operator delete。在VS2008下会有如下警告：
warning C4291: “void *A::operator new(size_t,const char *,int)”: 未找到匹配的删除运算符；如果初始化引发异常，则不会释放内存
六 关于 new 和内存分配的其他
 1.set_new_handler
还有一些零散的东西没有介绍到，比如set_new_handler可以在malloc(需要调用set_new_mode(1))或operator new内存分配失败时指定一个入口函数new_handler，这个函数完成自定义处理(继续尝试分配，抛出异常，或终止程序)，如果new_handler返回，那么系统将继续尝试分配内存，如果失败，将继续重复调用它，直到内存分配完毕或new_handler不再返回(抛出异常，终止)。下面这段程序完成这个测试：#include <iostream>  
#include <new.h>// 使用_set_new_mode和set_new_handler  
void nomem_handler()  
{  
    std::cout<<"call nomem_handler"<<std::endl;  
}  
int main()  
{  
    _set_new_mode(1);  //使new_handler有效  
    set_new_handler(nomem_handler);//指定入口函数 函数原型void f();  
    std::cout<<"try to alloc 2GB memory...."<<std::endl;  
    char* a = (char*)malloc(2*1024*1024*1024);  
    if(a)  
        std::cout<<"ok...I got it"<<std::endl;  
    free(a);  
    system("pause");  
}程序运行后会一直输出call nomem_handler 因为函数里面只是简单输出，返回，系统尝试分配失败后，调用nomem_handler函数，由于该函数并没有起到实际作用(让可分配内存增大)，因此返回后系统再次尝试分配失败，再调用nomem_handler，循环下去。
在SGI STL中的也有个仿new_handler函数:oom_malloc
2.new 分配数组
A* p = new A[3];中，会直接调用全局的operator new[](size_t size)，而不管A中是否有operator new[]的重载。而delete[]p却会优先调用A::operator delete[](void*)(如果A中有重载)。另外还要注意的是，在operator new[](size_t size)中传入的并不是sizeof(A)*3。而要在对象数组的大小上加上一个额外数据，用于编译器区分对象数组指针和对象指针以及对象数组大小。在VS2008下这个额外数据占4个字节，一个int大小。测试代码如下//A.h  
class A  
{  
public:  
    A()  
    {  
        std::cout<<"call A constructor"<<std::endl;  
    }  
  
    ~A()  
    {  
        std::cout<<"call A destructor"<<std::endl;  
    }  
  
    void* operator new(size_t size)  
    {  
        std::cout<<"call A::operator new[] size:"<<size<<std::endl;  
        return malloc(size);  
    }  
    void operator delete[](void* p)  
    {  
        std::cout<<"call A::operator delete[]"<<std::endl;  
        free(p);  
    }   
    void operator delete(void* p)  
    {  
        free(p);  
    }   
};//Test.cpp  
#include <iostream>  
#include "A.h"  
  
void* operator new[](size_t size)  
{  
    std::cout<<"call global new[] size: "<<size<<std::endl;  
    return malloc(size);  
}  
  
void operator delete[](void* p)  
{  
    std::cout<<"call global delete[] "<<std::endl;  
}  
int _tmain(int argc, _TCHAR* argv[])  
{  
    std::cout<<"sizeof A "<<sizeof(A)<<std::endl;  
    A* p1 = new A[3];  
    delete []p1;  
   
    system("pause");  
    return 0;  
}输出：


简单跟踪了一下：
operator new[]返回的是0x005b668 而最后new运算符返回给p的是0x005b66c。也就是说p就是数组的起始地址，这样程序看到的内存就是线性的，不包括前面的额外数据。

在内存中，可以看到前面的四个字节额外数据是0x00000003 也就是3，代表数组元素个数。后面三个cd是堆在Debug中的默认值(中文的cdcd就是&#8221;屯&#8221;，栈的初始值为cc，0xcccc中文&#8221;烫&#8221;)。再后面的0xfdfdfdfd应该是堆块的结束标志，前面我有博客专门跟踪过。

注：其实在malloc源码中也有内存池的运用，而且也比较复杂。最近在参考dlmalloc版本和STL空间适配器，真没有想到一个内存分配能涉及这么多的东西。
C++ 内存分配(new，operator new)详解，首发于文章 - 伯乐在线。</content>
</doc>
<doc>
	<docid>17</docid>
	<title>制作你的第一个 Atom 文本编辑器插件</title>
	<link>http://blog.jobbole.com/106919/</link>
	<author></author>
	<content>序言
这篇教程将会教你怎么制作你的第一个 Atom 文本编辑器的插件。我们将会制作一个山寨版的 Sourcerer，这是一个从 StackOverflow 查询并使用代码片段的插件。到教程结束时，你将会制作好一个将编程问题（用英语描述的）转换成获取自 StackOverflow 的代码片段的插件，像这样：

教程须知
Atom 文本编辑器是用 web 技术创造出来的。我们将完全使用 JavaScript 的 EcmaScript 6 规范来制作插件。你需要熟悉以下内容：

使用命令行
JavaScript 编程
Promises
HTTP

教程的仓库
你可以跟着教程一步一步走，或者看看 放在 GitHub 上的仓库，这里有插件的源代码。这个仓库的历史提交记录包含了这里每一个标题。
开始
安装 Atom
根据 Atom 官网 的说明来下载 Atom。我们同时还要安装上 apm（Atom 包管理器的命令行工具）。你可以打开 Atom 并在应用菜单中导航到 Atom > Install Shell Commands 来安装。打开你的命令行终端，运行apm -v 来检查 apm 是否已经正确安装好，安装成功的话打印出来的工具版本和相关环境信息应该是像这样的：apm -v
> apm  1.9.2
> npm  2.13.3
> node 0.10.40
> python 2.7.10
> git 2.7.4
生成骨架代码
让我们使用 Atom 提供的一个实用工具创建一个新的 package（软件包）来开始这篇教程。

启动编辑器，按下 Cmd+Shift+P（MacOS）或者 Ctrl+Shift+P（Windows/Linux）来打开命令面板Command Palette。
搜索“Package Generator: Generate Package”并点击列表中正确的条目，你会看到一个输入提示，输入软件包的名称：“sourcefetch”。
按下回车键来生成这个骨架代码包，它会自动在 Atom 中打开。

如果你在侧边栏没有看到软件包的文件，依次按下 Cmd+K Cmd+B（MacOS）或者 Ctrl+KCtrl+B（Windows/Linux）。

Command Palette可以让你通过模糊搜索来找到并运行软件包。这是一个执行命令比较方便的途径，你不用去找导航菜单，也不用刻意去记快捷键。我们将会在整篇教程中使用这个方法。
运行骨架代码包
在开始编程前让我们来试用一下这个骨架代码包。我们首先需要重启 Atom，这样它才可以识别我们新增的软件包。再次打开命令面板，执行 Window: Reload 命令。
重新加载当前窗口以确保 Atom 执行的是我们最新的源代码。每当需要测试我们对软件包的改动的时候，就需要运行这条命令。
通过导航到编辑器菜单的 Packages > sourcefetch > Toggle 或者在命令面板执行 sourcefetch:toggle 来运行软件包的 toggle 命令。你应该会看到屏幕的顶部出现了一个小黑窗。再次运行这条命令就可以隐藏它。

“toggle”命令
打开 lib/sourcefetch.js，这个文件包含有软件包的逻辑和 toggle 命令的定义。
 toggle() {
 console.log('Sourcefetch was toggled!');
 return (
   this.modalPanel.isVisible() ?
   this.modalPanel.hide() :
   this.modalPanel.show()
 );
}toggle 是这个模块导出的一个函数。根据模态面板的可见性，它通过一个三目运算符 来调用 show 和hide 方法。modalPanel 是 Panel（一个由 Atom API 提供的 UI 元素） 的一个实例。我们需要在 export default 内部声明 modalPanel 才可以让我们通过一个实例变量 this 来访问它。this.subscriptions.add(atom.commands.add('atom-workspace', {
  'sourcefetch:toggle': () => this.toggle()
}));上面的语句让 Atom 在用户运行 sourcefetch:toggle 的时候执行 toggle 方法。我们指定了一个 匿名函数() => this.toggle()，每次执行这条命令的时候都会执行这个函数。这是事件驱动编程（一种常用的 JavaScript 模式）的一个范例。
Atom 命令
命令只是用户触发事件时使用的一些字符串标识符，它定义在软件包的命名空间内。我们已经用过的命令有：

package-generator:generate-package
Window:reload
sourcefetch:toggle

软件包对应到命令，以执行代码来响应事件。
进行你的第一次代码更改
让我们来进行第一次代码更改——我们将通过改变 toggle 函数来实现逆转用户选中文本的功能。
改变 “toggle” 函数
如下更改 toggle 函数。toggle() {
  let editor
  if (editor = atom.workspace.getActiveTextEditor()) {
    let selection = editor.getSelectedText()
    let reversed = selection.split('').reverse().join('')
    editor.insertText(reversed)
  }
}
测试你的改动

通过在命令面板运行 Window: Reload 来重新加载 Atom。
通过导航到 File > New 来创建一个新文件，随便写点什么并通过光标选中它。
通过命令面板、Atom 菜单或者右击文本然后选中 Toggle sourcefetch 来运行sourcefetch:toggle 命令。

更新后的命令将会改变选中文本的顺序：

在 sourcefetch 教程仓库 查看这一步的全部代码更改。
Atom 编辑器 API
我们添加的代码通过用 TextEditor API 来访问编辑器内的文本并进行操作。让我们来仔细看看。let editor
if (editor = atom.workspace.getActiveTextEditor()) { /* ... */ }头两行代码获取了 TextEditor 实例的一个引用。变量的赋值和后面的代码被包在一个条件结构里，这是为了处理没有可用的编辑器实例的情况，例如，当用户在设置菜单中运行该命令时。let selection = editor.getSelectedText()调用 getSelectedText 方法可以让我们访问到用户选中的文本。如果当前没有文本被选中，函数将返回一个空字符串。let reversed = selection.split('').reverse().join('')
editor.insertText(reversed)我们选中的文本通过一个 JavaScript 字符串方法 来逆转。最后，我们调用 insertText 方法来将选中的文本替换为逆转后的文本副本。通过阅读 Atom API 文档，你可以学到更多关于 TextEditor 的不同的方法。
浏览骨架代码
现在我们已经完成第一次代码更改了，让我们浏览骨架代码包的代码来深入了解一下 Atom 的软件包是怎样构成的。
主文件
主文件是 Atom 软件包的入口文件。Atom 通过 package.json 里的条目设置来找到主文件的位置："main": "./lib/sourcefetch",这个文件导出一个带有生命周期函数（Atom 在特定的事件发生时调用的处理函数）的对象。

activate 会在 Atom 初次加载软件包的时候调用。这个函数用来初始化一些诸如软件包所需的用户界面元素的对象，以及订阅软件包命令的处理函数。
deactivate 会在软件包停用的时候调用，例如，当用户关闭或者刷新编辑器的时候。
serialize Atom 调用它在使用软件包的过程中保存软件包的当前状态。它的返回值会在 Atom 下一次加载软件包的时候作为一个参数传递给 activate。

我们将会重命名我们的软件包命令为 fetch，并移除一些我们不再需要的用户界面元素。按照如下更改主文件：'use babel';
import { CompositeDisposable } from 'atom'
export default {
  subscriptions: null,
  activate() {
    this.subscriptions = new CompositeDisposable()
    this.subscriptions.add(atom.commands.add('atom-workspace', {
      'sourcefetch:fetch': () => this.fetch()
    }))
  },
  deactivate() {
    this.subscriptions.dispose()
  },
  fetch() {
    let editor
    if (editor = atom.workspace.getActiveTextEditor()) {
      let selection = editor.getSelectedText()
      selection = selection.split('').reverse().join('')
      editor.insertText(selection)
    }
  }
};
“启用”命令
为了提升性能，Atom 软件包可以用时加载。我们可以让 Atom 在用户执行特定的命令的时候才加载我们的软件包。这些命令被称为 启用命令，它们在 package.json 中定义："activationCommands": {
  "atom-workspace": "sourcefetch:toggle"
},有一些软件包需要在 Atom 启动的时候被加载，例如那些改变 Atom 外观的软件包。在那样的情况下，activationCommands 会被完全忽略。
“触发”命令
菜单项
menus 目录下的 JSON 文件指定了哪些菜单项是为我们的软件包而建的。让我们看看menus/sourcefetch.json："context-menu": {
  "atom-text-editor": [
    {
      "label": "Toggle sourcefetch",
      "command": "sourcefetch:toggle"
    }
  ]
},这个 context-menu 对象可以让我们定义右击菜单的一些新条目。每一个条目都是通过一个显示在菜单的label 属性和一个点击后执行的命令的 command 属性来定义的。"context-menu": {
  "atom-text-editor": [
    {
      "label": "Fetch code",
      "command": "sourcefetch:fetch"
    }
  ]
},同一个文件中的这个 menu 对象用来定义插件的自定义应用菜单。我们如下重命名它的条目："menu": [
  {
    "label": "Packages",
    "submenu": [
      {
        "label": "sourcefetch",
        "submenu": [
          {
            "label": "Fetch code",
            "command": "sourcefetch:fetch"
          }
        ]
      }
    ]
  }
]
键盘快捷键
命令还可以通过键盘快捷键来触发。快捷键通过 keymaps 目录的 JSON 文件来定义：{
  "atom-workspace": {
    "ctrl-alt-o": "sourcefetch:toggle"
  }
}以上代码可以让用户通过 Ctrl+Alt+O（Windows/Linux） 或 Cmd+Alt+O（MacOS） 来触发 toggle 命令。
重命名引用的命令为 fetch："ctrl-alt-o": "sourcefetch:fetch"通过执行 Window: Reload 命令来重启 Atom。你应该会看到 Atom 的右击菜单更新了，并且逆转文本的功能应该还可以像之前一样使用。
在 sourcefetch 教程仓库 查看这一步所有的代码更改。
使用 NodeJS 模块
现在我们已经完成了第一次代码更改并且了解了 Atom 软件包的结构，让我们介绍一下 Node 包管理器（npm） 中的第一个依赖项模块。我们将使用 request 模块发起 HTTP 请求来下载网站的 HTML 文件。稍后将会用到这个功能来扒 StackOverflow 的页面。
安装依赖
打开你的命令行工具，切换到你的软件包的根目录并运行：npm install --save request@2.73.0
apm install这两条命令将 request 模块添加到我们软件包的依赖列表并将模块安装到 node_modules 目录。你应该会在package.json 看到一个新条目。@ 符号的作用是让 npm 安装我们这篇教程需要用到的特定版本的模块。运行 apm install 是为了让 Atom 知道使用我们新安装的模块。"dependencies": {
  "request": "^2.73.0"
}
下载 HTML 并将记录打印在开发者控制台
通过在 lib/sourcefetch.js 的顶部添加一条引用语句引入 request 模块到我们的主文件：import { CompositeDisposable } from 'atom'
import request from 'request'现在，在 fetch 函数下面添加一个新函数 download 作为模块的导出项：export default {  
  /* subscriptions, activate(), deactivate() */
  fetch() {
    ...
  },
  download(url) {
    request(url, (error, response, body) => {
      if (!error && response.statusCode == 200) {
        console.log(body)
      }
    })
  }
}这个函数用 request 模块来下载一个页面的内容并将记录输出到控制台。当 HTTP 请求完成之后，我们的回调函数会将响应体作为参数来被调用。
最后一步是更新 fetch 函数以调用 download 函数：fetch() {
  let editor
  if (editor = atom.workspace.getActiveTextEditor()) {
    let selection = editor.getSelectedText()
    this.download(selection)
  }
},fetch 函数现在的功能是将 selection 当作一个 URL 传递给 download 函数，而不再是逆转选中的文本了。让我们来看看这次的更改：

通过执行 Window: Reload 命令来重新加载 Atom。
打开开发者工具。为此，导航到菜单中的 View > Developer > Toggle Developer Tools。
新建一个文件，导航到 File > New。
输入一个 URL 并选中它，例如：http://www.atom.io。
用上述的任意一种方法执行我们软件包的命令：


开发者工具让 Atom 软件包的调试更轻松。每个 console.log 语句都可以将信息打印到交互控制台，你还可以使用 Elements 选项卡来浏览整个应用的可视化结构——即 HTML 的文本对象模型（DOM）。
在 sourcefetch 教程仓库 查看这一步所有的代码更改。
用 Promises 来将下载好的 HTML 插入到编辑器中
理想情况下，我们希望 download 函数可以将 HTML 作为一个字符串来返回，而不仅仅是将页面的内容打印到控制台。然而，返回文本内容是无法实现的，因为我们要在回调函数里面访问内容而不是在 download 函数那里。
我们会通过返回一个 Promise 来解决这个问题，而不再是返回一个值。让我们改动 download 函数来返回一个 Promise：download(url) {
  return new Promise((resolve, reject) => {
    request(url, (error, response, body) => {
      if (!error && response.statusCode == 200) {
        resolve(body)
      } else {
        reject({
          reason: 'Unable to download page'
        })
      }
    })
  })
}Promises 允许我们通过将异步逻辑封装在一个提供两个回调方法的函数里来返回获得的值（resolve 用来处理请求成功的返回值，reject 用来向使用者报错）。如果请求返回了错误我们就调用 reject，否则就用resolve 来处理 HTML。
让我们更改 fetch 函数来使用 download 返回的 Promise：fetch() {
  let editor
  if (editor = atom.workspace.getActiveTextEditor()) {
    let selection = editor.getSelectedText()
    this.download(selection).then((html) => {
      editor.insertText(html)
    }).catch((error) => {
      atom.notifications.addWarning(error.reason)
    })
  }
},在我们新版的 fetch 函数里，我们通过在 download 返回的 Promise 调用 then 方法来对 HTML 进行操作。这会将 HTML 插入到编辑器中。我们同样会通过调用 catch 方法来接收并处理所有的错误。我们通过用Atom Notification API 来显示警告的形式来处理错误。
看看发生了什么变化。重新加载 Atom 并在一个选中的 URL 上执行软件包命令：

如果这个 URL 是无效的，一个警告通知将会弹出来：

在 sourcefetch 教程仓库 查看这一步所有的代码更改。
编写一个爬虫来提取 StackOverflow 页面的代码片段
下一步涉及用我们前面扒到的 StackOverflow 的页面的 HTML 来提取代码片段。我们尤其关注那些来自采纳答案（提问者选择的一个正确答案）的代码。我们可以在假设这类答案都是相关且正确的前提下大大简化我们这个软件包的实现。
使用 jQuery 和 Chrome 开发者工具来构建查询
这一部分假设你使用的是 Chrome 浏览器。你接下来可以使用其它浏览器，但是提示可能会不一样。
让我们先看看一张典型的包含采纳答案和代码片段的 StackOverflow 页面。我们将会使用 Chrome 开发者工具来浏览 HTML：

打开 Chrome 并跳到任意一个带有采纳答案和代码的 StackOverflow 页面，比如像这个用 Python 写的 hello world 的例子或者这个关于 用 C 来读取文本内容的问题。
滚动窗口到采纳答案的位置并选中一部分代码。
右击选中文本并选择 检查。
使用元素侦察器来检查代码片段在 HTML 中的位置。

注意文本结构应该是这样的：<div class="accepted-answer">
  ...
    ...
      <pre>
        <code>
          ...snippet elements...
        </code>
      </pre>
    ...
  ...
</div>

采纳的答案通过一个 class 为 accepted-answer 的 div 来表示
代码块位于 pre 元素的内部
呈现代码片段的元素就是里面那一对 code 标签


现在让我们写一些 jQuery 代码来提取代码片段：

在开发者工具那里点击 Console 选项卡来访问 Javascript 控制台。
在控制台中输入 $('div.accepted-answer pre code').text() 并按下回车键。

你应该会看到控制台中打印出采纳答案的代码片段。我们刚刚运行的代码使用了一个 jQuery 提供的特别的 $函数。$ 接收要选择的查询字符串并返回网站中的某些 HTML 元素。让我们通过思考几个查询案例看看这段代码的工作原理：$('div.accepted-answer')
> [<div id="answer-1077349" class="answer accepted-answer" ... ></div>]上面的查询会匹配所有 class 为 accepted-answer 的 <div> 元素，在我们的案例中只有一个 div。$('div.accepted-answer pre code')
> [<code>...</code>]在前面的基础上改造了一下，这个查询会匹配所有在之前匹配的 <div> 内部的 <pre> 元素内部的 <code>元素。$('div.accepted-answer pre code').text()
> "print("Hello World!")"text 函数提取并连接原本将由上一个查询返回的元素列表中的所有文本。这也从代码中去除了用来使语法高亮的元素。
介绍 Cheerio
我们的下一步涉及使用我们创建好的查询结合 Cheerio（一个服务器端实现的 jQuery）来实现扒页面的功能。
安装 Cheerio
打开你的命令行工具，切换到你的软件包的根目录并执行：npm install --save cheerio@0.20.0
apm install
实现扒页面的功能
在 lib/sourcefetch.js 为 cheerio 添加一条引用语句：import { CompositeDisposable } from 'atom'
import request from 'request'
import cheerio from 'cheerio'现在创建一个新函数 scrape，它用来提取 StackOverflow HTML 里面的代码片段：fetch() {
  ...
},
scrape(html) {
  $ = cheerio.load(html)
  return $('div.accepted-answer pre code').text()
},
download(url) {
  ...
}最后，让我们更改 fetch 函数以传递下载好的 HTML 给 scrape 而不是将其插入到编辑器：fetch() {
  let editor
  let self = this
  if (editor = atom.workspace.getActiveTextEditor()) {
    let selection = editor.getSelectedText()
    this.download(selection).then((html) => {
      let answer = self.scrape(html)
      if (answer === '') {
        atom.notifications.addWarning('No answer found :(')
      } else {
        editor.insertText(answer)
      }
    }).catch((error) => {
      console.log(error)
      atom.notifications.addWarning(error.reason)
    })
  }
},我们扒取页面的功能仅仅用两行代码就实现了，因为 cheerio 已经替我们做好了所有的工作！我们通过调用load 方法加载 HTML 字符串来创建一个 $ 函数，然后用这个函数来执行 jQuery 语句并返回结果。你可以在官方 开发者文档 查看完整的 Cheerio API。
测试更新后的软件包
重新加载 Atom 并在一个选中的 StackOverflow URL 上运行 soucefetch:fetch 以查看到目前为止的进度。
如果我们在一个有采纳答案的页面上运行这条命令，代码片段将会被插入到编辑器中：

如果我们在一个没有采纳答案的页面上运行这条命令，将会弹出一个警告通知：

我们最新的 fetch 函数给我们提供了一个 StackOverflow 页面的代码片段而不再是整个 HTML 内容。要注意我们更新的 fetch 函数会检查有没有答案并显示通知以提醒用户。
在 sourcefetch 教程仓库 查看这一步所有的代码更改。
实现用来查找相关的 StackOverflow URL 的谷歌搜索功能
现在我们已经将 StackOverflow 的 URL 转化为代码片段了，让我们来实现最后一个函数——search，它应该要返回一个相关的 URL 并附加一些像“hello world”或者“快速排序”这样的描述。我们会通过一个非官方的google npm 模块来使用谷歌搜索功能，这样可以让我们以编程的方式来搜索。
安装这个 Google npm 模块
通过在软件包的根目录打开命令行工具并执行命令来安装 google 模块：npm install --save google@2.0.0
apm install
引入并配置模块
在 lib/sourcefetch.js 的顶部为 google 模块添加一条引用语句：import google from "google"我们将配置一下 google 以限制搜索期间返回的结果数。将下面这行代码添加到引用语句下面以限制搜索返回最热门的那个结果。google.resultsPerPage = 1
实现 search 函数
接下来让我们来实现我们的 search 函数：fetch() {
  ...
},
search(query, language) {
  return new Promise((resolve, reject) => {
    let searchString = `${query} in ${language} site:stackoverflow.com`
    google(searchString, (err, res) => {
      if (err) {
        reject({
          reason: 'A search error has occured :('
        })
      } else if (res.links.length === 0) {
        reject({
          reason: 'No results found :('
        })
      } else {
        resolve(res.links[0].href)
      }
    })
  })
},
scrape() {
  ...
}以上代码通过谷歌来搜索一个和指定的关键词以及编程语言相关的 StackOverflow 页面，并返回一个最热门的 URL。让我们看看这是怎样来实现的：let searchString = `${query} in ${language} site:stackoverflow.com`我们使用用户输入的查询和当前所选的语言来构造搜索字符串。比方说，当用户在写 Python 的时候输入“hello world”，查询语句就会变成 hello world in python site:stackoverflow.com。字符串的最后一部分是谷歌搜索提供的一个过滤器，它让我们可以将搜索结果的来源限制为 StackOverflow。google(searchString, (err, res) => {
  if (err) {
    reject({
      reason: 'A search error has occured :('
    })
  } else if (res.links.length === 0) {
    reject({
      reason: 'No results found :('
    })
  } else {
    resolve(res.links[0].href)
  }
})我们将 google 方法放在一个 Promise 里面，这样我们可以异步地返回我们的 URL。我们会传递由 google返回的所有错误并且会在没有可用的搜索结果的时候返回一个错误。否则我们将通过 resolve 来解析最热门结果的 URL。
更新 fetch 来使用 search
我们的最后一步是更新 fetch 函数来使用 search 函数：fetch() {
  let editor
  let self = this
  if (editor = atom.workspace.getActiveTextEditor()) {
    let query = editor.getSelectedText()
    let language = editor.getGrammar().name
    self.search(query, language).then((url) => {
      atom.notifications.addSuccess('Found google results!')
      return self.download(url)
    }).then((html) => {
      let answer = self.scrape(html)
      if (answer === '') {
        atom.notifications.addWarning('No answer found :(')
      } else {
        atom.notifications.addSuccess('Found snippet!')
        editor.insertText(answer)
      }
    }).catch((error) => {
      atom.notifications.addWarning(error.reason)
    })
  }
}让我们看看发生了什么变化：

我们选中的文本现在变成了用户输入的 query
我们使用 TextEditor API 来获取当前编辑器选项卡使用的 language
我们调用 search 方法来获取一个 URL，然后通过在得到的 Promise 上调用 then 方法来访问这个 URL

我们不在 download 返回的 Promise 上调用 then 方法，而是在前面 search 方法本身链式调用的另一个then 方法返回的 Promise 上面接着调用 then 方法。这样可以帮助我们避免回调地狱
在 sourcefetch 教程仓库 查看这一步所有的代码更改。
测试最终的插件
大功告成了！重新加载 Atom，对一个“问题描述”运行软件包的命令来看看我们最终的插件是否工作，不要忘了在编辑器右下角选择一种语言。

下一步
现在你知道怎么去 “hack” Atom 的基本原理了，通过 分叉 sourcefetch 这个仓库并添加你的特性 来随心所欲地实践你所学到的知识。
制作你的第一个 Atom 文本编辑器插件，首发于文章 - 伯乐在线。</content>
</doc>
<doc>
	<docid>18</docid>
	<title>QA，从 1.0 到 4.0</title>
	<link>http://blog.jobbole.com/106910/</link>
	<author></author>
	<content>迄今为止，敏捷开发方法在各个公司都有了长足的发展，曾经的测试人员慢慢的在向QA职能过渡，但依然很多人不了解QA和测试的区别是什么。
敏捷实践不断地演化过程，使项目中各个角色不断弱化，同时，对每个成员的要求也越来越高。“全功能团队”的提出，不单单是对开发的要求，对QA来说，想要在快速变革中具备竞争力，就现在所具备的技能来说，还是远远不够的。
简单聊聊我所经历的“QA发展史”

(图片来自ThoughtWorks UX设计师 高媛媛)
QA 1.0 —— 机械化流水线作业
在我实习的那年，软件领域还很少提及QA，伴随着瀑布模型的兴起、软件工程规模的不断扩大以及市场对软件质量要求的提高，催生出了“测试工程师”这样一个角色。那时他们的职能很单一，每天的工作就是在各种测试环境中按照详细设计的文档，编写测试用例，并逐条测试，检查功能完整性，发现软件中可能出现的功能缺陷，并进行追踪。
这个时期是软件测试的原始时期，对测试人员的技能要求不高，只要对文档理解透彻，做事细心，是很容易胜任的。此时的产出和交付物可度量，虽然如此，测试工程师只是执行者，能力和价值都无法最大化，却被每天重复的工作所累。
QA 2.0 —— 过程化带来不同的工作内容和价值体现
我毕业的时候，开始接触敏捷方法，团队规模从百人变成了仅有十人左右，信息平等取代了逐级传递，分散的信息源（ 客户的每一封邮件和每一句对话都可能是我们将要做的功能 ）取代了几十甚至百页的文档，测试不再仅是提出软件缺陷和编写、执行测试用例，而是成为了团队的数据库和字典。

当用户提出一个功能的时候，测试人员可以快速的进行需求分析，回顾并确定是否与此前的功能有冲突。当开发人员对某一块业务不了解的时候，测试人员也可以组织会议进行阐明。由于对业务和客户的深入了解，测试人员可以为客户提出建设性意见和功能，有时也会是做出决策的人。
高效、频繁的沟通，大大提升了QA的软技能。此时的测试人员已经过程化，对软件质量的理解，从“发现缺陷”提升到“对软件开发过程的质量控制和风险预估”，我们定义这样的测试工程师为QA。
QA 3.0 —— 自动化技能提高生产力
随着工程实践的日益成熟，QA的角色和工作日益复杂，这使得他们在大量重复、繁杂的工作与更有意义的角色之间频繁切换，这对软件质量也产生了一定影响。

QA从开始的手工测试、探索性测试等手段，逐渐发展成为可以利用工具和程序对测试进行快速的回归，对软件性能进行有效监控，无论是前端还是后端、web应用还是移动平台。这使得自己从繁杂的重复性工作中解脱出来，去做更有意义的事情。他们通过项目的积累以及团队成员的帮助，对测试技术有了一定的认识。
QA 4.0 —— 角色向多技能、服务化转型
记得几年前，前公司领导对我说，“不管开发换了什么技术栈，你做的自动化框架都可以继续使用，对你来说没有任何影响。”当时我也赞同，认为框架已经足够好，可以适用于任何场景和业务。
从某个角度来说确实是这样， 测试相对于开发技术的指数级发展，平稳的太多。不论是在互联网还是移动互联网时代，缓慢的发展速度给了我们一种太平盛世的错觉，似乎我们掌握的技术足够坐吃几年。
来到ThoughtWorks之后，我发现了类似的事情，不论是在交付还是咨询的过程中，会有意无意中推一些我们认为的最佳实践，当遭到客户质疑和challenge的时候，我们似乎很沮丧。
在北京出差的日子，有幸做了一次咨询，虽然只有几天，让我学到一件事，我们认为的最佳实践和方法，并不完全适应于所有场合，尤其是在我们这样的咨询公司，对客户实施怎样的方案，取决于客户的领域、产品/项目特性、用户群、技术水平、政治文化、技术栈以及目标和期望等等。

如果对于“最佳实践”过于坚持，也会影响客户关系和咨询效果。之后咨询同事讲的几个故事也似乎让我认识到，虽然我们对现在的工作和技能足够的熟练，但依然不够。
我们似乎还需要具备以下的能力：
1.尝试用不同的方法写“茴”
经验丰富的QA对于测试技术中的关键点都烂熟于心， 除了我们正在使用的方案和技术，尝试用不同的语言、框架去实现关键点和难点。这样的好处在于，我们可以通过深入的学习和使用，对流行方法、过程和框架进行比较，了解各自的优势和劣势，不但可以增强自身的技能，当面对不同客户的时候，也可以给出客观的分析，为客户提供精准服务，同时如果可以对客户现有的技术和方案、流程和方法提供有价值的意见，也可以提高在客户现场的生存率，轻松俘获客户。
2.If you cannot test it, dev it.
软件过程中，QA可以在需求分析和定义阶段介入，为项目提供不可估量的价值，但另一方面，QA技能实践（此处指Tech）是一个相对受限的领域，我们很难绕过未实现的代码和工程去做更多的事情。
你可能会说，“没有做过mobile的项目，如何去学习移动端的测试技能？”如果恰好你对行业的发展具有前瞻性和敏感度，例如你可能认为IoT和VR是一个趋势，你却没有机会去这样的项目中做QA。
那么我们是不是可以像开发一样，提升自己的学习能力和适应力，保持对技术的敏感度和热忱，了解不同技术领域，对该领域的开发、测试、构建和集成部署都有一定的了解？所以，如果你想比其他人走的快那么一点，go dev!

3.真正的全功能
QA的领域虽然相对受限，但幸运的是角色相对不受限。在日常的开发过程和项目积累过程中，不但能对业务有深刻的理解、对用户行为有独到的见解，而且对技术也有一定的认识。
在需求分析过程中，QA总是可以从技术和业务结合的角度扮演好一个BA的角色，成为一个优秀的PM，甚至我们可以在客户提出一些需求的时候尝试着从一个UX的角度去设计原型，如果具备前端的能力，也可以自己去Dev、UI，不断拓展自己的技能领域，使自己成为真正的全功能。
总结
真正的全功能，并不是单纯意义上让QA去做Dev，而是最大程度弱化角色的概念，逐步强调和培养技能多元化。如何把对需求的理解能力强化为业务分析能力，把质量控制能力强化为项目管理能力。强化自身的优势，跳出自己的舒适区，使自己能够轻松胜任。这样我们才不会在看到去QA和QA消亡之类的观点后，无所适从。
QA，从 1.0 到 4.0，首发于文章 - 伯乐在线。</content>
</doc>
<doc>
	<docid>19</docid>
	<title>C++ 虚函数和虚继承浅析</title>
	<link>http://blog.jobbole.com/106912/</link>
	<author></author>
	<content>本文针对C++里的虚函数，虚继承表现和原理进行一些简单分析，有不对的地方请指出。下面都是以VC2008编译器对这两种机制内部实现为例。

虚函数
　　一下是百度百科对于虚函数的解释：

定义：在某基类中声明为 virtual 并在一个或多个派生类中被重新定 义的成员函数[1]
语法：virtual 函数返回类型 函数名（参数表） { 函数体 }
用途：实现多态性，通过指向派生类的基类指针，访问派生类中同名覆盖成员函数

函数声明和定义和普通的类成员函数一样，只是在返回值之前加入了关键字“virtual”声明为虚函数。而虚函数是实现多态的重要手段，意思是只有对虚函数的调用才能动态决定调用哪一个函数，这是相对于普通成员函数而言的，普通的成员函数在编译阶段就能确定调用哪一个函数。举个栗子：

#include <stdio.h>

class A {
public:
    void fn() { printf("fn in A\n"); }
    virtual void v_fn() { printf("virtual fn in A\n"); }
};

class B : public A {
public:
    void fn() { printf("fn in B\n"); }
    virtual void v_fn() { printf("virtual fn in B\n"); }
};

int main() {
    A *a = new B();
    a->fn();
    a->v_fn();
    return 0;
}

基类A有两个成员函数fn和v_fn，派生类B继承自基类A，同样实现了两个函数，然后在main函数中用A的指针指向B的实例（向上转型，也是实现多态的必要手段），然后分别调用fn和v_fn函数。结果是“fn in A&#8221;和&#8221;virtual fn in B&#8221;。
这是因为fn是普通成员函数，它是通过类A的指针调用的，所以在编译的时候就确定了调用A的fn函数。而v_fn是虚函数，编译时不能确定，而是在运行时再通过一些机制来调用指针所指向的实例（B的实例）中的v_fn函数。假如派生类B中没有实现（完全一样，不是重载）v_fn这个函数，那么依然会调用基类类A中的v_fn；如果它实现了，就可以说派生类B覆盖了基类A中的v_fn这个虚函数。这就是虚函数的表现和使用，只有通过虚函数，才能实现面向对象语言中的多态性。
以上只是虚函数的表现和用途，下面来探讨它的实现机制。在此之前，先来看一个问题，还是以上的代码，基类A的大小为多少，也就是“printf(&#8220;%dn&#8221;, sizeof(A));&#8221;的输出会是多少呢？A中一个成员变量都没有，有人可能会说是0。额，0是绝对错误的，因为在C++中，即时是空类，它的大小也为1，这是另外的话题，不在本文讨论。当然1也是不对的，实际结果是4（32位系统），4刚好是一个int，一个指针（32位）的大小，派生类B的大小同样为4。这四个字节和实现多态，虚函数的机制有着很重要的关系。
其实用VC2008调试上面代码的时候，就会发现指针a所指向的实力中有一个成员常量（const），它的名字叫做vftable，全称大概叫做virtual function table（虚函数表）。它实际指向了一个数组，数组里面保存的是一系列函数指针，而上面的程序中，这个表只有一项，它就是派生类B中的v_fn函数入口地址。假如我们用一个A的指针指向一个A的实例呢？它同样有一个vftable，而它指向的表中也只有一项，这项保存的基类的v_fn函数入口地址。这用代码表示，就类似于下面这样：

void* vftable_of_A[] = {
    A::v_fn,
    ...
};

class A {
    const void* vftable = vftable_of_A;
    virtual void v_fn() {}
};

void* vftable_of_B[] = {
    B::v_fn,
    ...
};

class B {
    const void *vftable = vftable_of_B;
    vritual void v_fn() {}
};

上面vftable的类型之所以用void*表示，实际上一个类中所有虚函数的地址都被放到这个表中，不同虚函数对应的函数指针类型不尽相同，所以这个表用C++的类型不好表述，但是在机器级里都是入口地址，即一个32位的数字（32位系统），等到调用时，因为编译器预先知道了函数的参数类型，返回值等，可以自动做好处理。
这样我们就能更好的理解虚函数和多态了。第一个代码中，a指针虽然是A*类型的，但是它却调用了B中的v_fn，因为不管是A类，还是A的基类，都会有一个变量vftable，它指向的虚函数表中保存了正确的v_fn入口。所以a->v_fn()实际做的工作就是从a指向的实例中取出vftable的值，然后找到虚函数表，再从表中去的v_fn的入口，进行调用。不管a是指向A的实例，还是指向B的实例，a->fn()所做的步骤都是上面说的一样，只是A的实例和B的实例有着不同的虚函数表，虚函数表里也保存着可能不同的虚函数入口，所以最终将进入不同的函数调用中。通过表来达到不用判断类型，亦可实现多态的作用。还有一点指的提醒的是，因为虚函数表是一个常量表，在编译时，编译器会自动生成，并且不会改变，所以如果有多个B类的实例，每个实例中都会有一个vftable指针，但是它们指向的是同一个虚函数表。
上面一段中说到了，A和B的实例有着不同的虚函数表，但是虚函数表中只是可能保存着不同的v_fn，那是因为C++允许派生类不覆盖基类中的虚函数，意思就是假如派生类B中没有实现v_fn这个函数（不是重载），那么B的实例的虚函数表会保存着基类A中v_fn的入口地址。也就是说B类不实现v_fn函数，但是它同样提供了这个接口，实际上是调用基类A中的v_fn。假如某个类只是一个抽象类，抽象出一些列接口，但是又不能实现这些接口，而要有派生类来实现，那么就可以把这些接口声明为纯虚函数，包含有纯虚函数的类称为抽象类。纯虚函数是一类特殊的虚函数，它的声明方式如下：
class A {
public:
virtual 返回值 函数名（参数表）= 0；
};
在虚函数声明方式后加一个“=0”，并且不提供实现。抽象类不允许实例化（这样做编译器会报错，因为有成员函数没有实现，编译器不知道怎么调用）。纯虚函数的实现机制和虚函数类似，只是要求派生类类必须自己实现一个（也可以不实现，但是派生类也会是个抽象类，不能实例化）。
顺带提一下，java中的每一个成员函数都可以以理解为C++中的virtual函数，不用显式声明都可以实现重载，多态。而java的接口类似于C++中的抽象类，需要实现里面的接口。
虚继承
C++支持多重继承，这和现实生活很类似，任何一个物体都不可能单一的属于某一个类型。就像马，第一想到的就是它派生自动物这个基类，但是它在某系地方可不可以说也派生自交通工具这一个基类呢？所以C++的多重继承很有用，但是又引入了一个问题（专业术语叫做菱形继承？）。动物和交通工具都是从最根本的基类——“事物”继承而来，事物包含了两个最基本的属性，体积和质量。那么动物和交通工具都保存了基类成员变量——体积和质量的副本。而马有继承了这两个类，那么马就有两份体积和质量，这是不合理的，编译器无法确定使用哪一个，所以就会报错。JAVA中不存在这样的问题，因为JAVA不允许多重继承，它只可能实现多个接口，而接口里面只包含一些函数声明不包含成员变量，所以也不存在这样的问题。
这个问题用具体代码表述如下所示：

class A {
public:
    int a;
};

class B : public A {
};

class C : public A {
};

class D : public B, public C {
};

int main() {
    D d;
    d.a = 1;
    return 0;
}

这个代码会报错，因为d中保存了两份A的副本，即有两个成员变量a，一般不会报错，但是一旦对D中的a使用，就会报一个“对a的访问不明确”。虚继承就可以解决这个问题。在探讨虚函数之前，先来一个sizeof的问题。

#include <stdio.h>

class A {
public:
    int a;
};

class B : virtual public A {
};


int main() {
    printf("%d\n", sizeof(B));
    return 0;
}

B的大小是？首先回答0的是绝对错的，理由我之前都说了。1也是错的，不解释。4也是错的，如果B不是虚继承自A的，那么4就是对的。正确答案是8，B虚继承A了之后，比预想中的多了4个字节，这是怎么回事呢？这个通过调试是看不出来的，因为看不到类似于vftable的成员变量（实际上编译器生成了一个类似的东西，但是调试时看不到，但是在观察反汇编的时候，可以见到vbtable的字样，应该是virtual base table的意思）。
虚继承的提出就是为了解决多重继承时，可能会保存两份副本的问题，也就是说用了虚继承就只保留了一份副本，但是这个副本是被多重继承的基类所共享的，该怎么实现这个机制呢？编译器会在派生类B的实例中保存一个A的实例，然后在B中加入一个变量，这个变量是A的实例在实际B实例中的偏移量，实际上B中并不直接保存offset的值，而是保存的一个指针，这个指针指向一个表vbtable，vbtable表中保存着所有虚继承的基类在实例中的offset值，多个B的实例共享这个表，每个实例有个单独的指针指向这个表，这样就很好理解为什么多了4个字节了。用代码表示就像下面这样。

class A {
public:
    ...
};

int vbtable_of_B[] = {
　　offset(B::_a),
    ...
};


class B ：virtual public A{
private:
    const int* vbtable = vbtable_of_B;
    A _a;
};

每一个A的虚派生类，都会有自己的vbtable表，这个派生类的所有实例共享这个表，然后每个实例各自保存了一个指向vbtable表的指针。假如还有一个类C虚继承了A，那么编译器就会为它自动生成一个vbtable_of_C的表，然后C的实例都会有一个指向这个vbtable表的指针。
假如有多级的虚继承会发生什么情况，就像下面这段代码一样：

#include <stdio.h>

class A {
public:
    int a;
};

class B : virtual public A {
public:
　　int b;
};

class C : virtual public B {
};


int main() {
    printf("%d\n", sizeof(C));
    return 0; 
}

程序运行的结果是16，按照之前的理论，大概会这么想。基类A里有1个变量，4个字节。B类虚继承了A，所以它有一个A的副本和一个vbtable，还有自己的一个变量，那就是12字节。然后C类又虚继承了B类，那么它有一个B的副本，一个vbtable，16字节。但实际上通过调试和反汇编发现，C中保存分别保存了A和B的副本（不包括B类的vbtable），8字节。然后有一个vbtable指针，4字节，表里面包含了A副本和B副本的偏移量。最后还有一个无用的4字节（？），一共16字节。不仅是这样，每经过一层的虚继承，便会多出4字节。这个多出来的四字节在反汇编中没发现实际用途，所以这个有待探讨，不管是编译器不够智能，还是有待其它作用，虚继承和多重继承都应该谨慎使用。
还是以上面的例子，假如C类是直接继承B类，而不是使用虚继承，那么C类的大小为12字节。它里面是直接保存了A和B的副本（不包含B的vbtable），然后还有一个自己的vbtable指针，所以一共12字节，没有了上一段所说的最后的4个字节。
但是如果想下面一种继承，会是什么情况？

#include <stdio.h>

class A {
public:
    int a;
};

class B : virtual public A {
};

class C : virtual public A {
};

class D : public B, public C{
};


int main() {
    printf("%d\n", sizeof(D));
    return 0; 
}

D从B，C类派生出来，而B和C又同时虚继承了A。输出的结构是12，实际调试反汇编的时候发现，D中继承了B和C的vbtable，这就是8字节，而同时还保存了一个A的副本，4字节，总共12字节。它和上面的多重虚继承例子里的12字节是不一样的。之前一个例子中只有一个vbtable，一个A的实例，末尾还有一个未知的4字节。而这个例子中是有两个仅挨着的vbtable（都有效）和一个A的实例。

C++ 虚函数和虚继承浅析，首发于文章 - 伯乐在线。</content>
</doc>
<doc>
	<docid>20</docid>
	<title>Erlang之父Joe Armstrong确认将参加中国软件开发者大会</title>
	<link>http://www.iteye.com/news/31904</link>
	<author></author>
	<content>
          2016年11月18日-20日，由CSDN重磅打造的年终技术盛会，SDCC 2016中国软件开发者大会将在北京举行，大会面向国内外的中高端技术人员，聚焦最前沿技术及一线的实践经验，从而助力企业的技术升级和改造、全面提升技术人员的综合实力。



本次大会非常有幸地邀请到了Erlang最初的设计者和实现者Joe Armstrong来华，出席SDCC 2016·北京站的Keynote并带来精彩的主题分享，他也是Erlang OTP系统项目的首席架构师。他拥有瑞典皇家理工学院博士学位，是容错系统开发领域的世界级专家。此外，他还在开发旨在替代XML的标记语言ML9。现任职于爱立信公司。

Joe Armstrong最广为人知的是他发明了Erlang编程语言，并且创建了用于构建Erlang应用程序的框架——开放电信平台（Open Telecom Platform，OTP）。在现代语言的版图中，Erlang有点另类。同很多流行的语言相比，它既老又新。早在1986年，也就是Perl出现的前一年，Armstrong就已经开始了Erlang的工作，当时它只作为商用产品出售并且主要在爱立信公司内部使用。

这种情况直到1998年Erlang作为开源项目发布后才发生变化，那时Java和Ruby已经问世3年了。Erlang并非起源于Algol系列中的某个成员，而是源于逻辑编程语言Prolog。当时Erlang设计初衷也相当明确，它针对的软件是类似于电话交换机那样的高可用性、高可靠性系统。

但是几乎在不经意间，Erlang适合于构建电话交换机的那些特征也让它非常适合于编写并发软件，当程序员们开始努力应对多核系统未来的发展趋势时，并发性引起了他们的注意。

Armstrong本人也有点另类。他起初是一名物理工作者，在攻读物理学博士学位时因为用完积蓄而转向了计算机科学，找到一份研究员的工作，为英国人工智能领域奠基人之一的Donald Michie工作。在Michie的实验室，Armstrong接触了人工智能领域各个方面的杰作，成为英国机器人学会的创始成员并撰写了一些有关机器人视觉的论文。

由于Lighthill所做的那份非常有名的调查报告 ，人工智能的资金来源枯竭，Armstrong又回到了物理学领域，从事了5年多与物理学编程相关的工作。开始时他在欧洲非相干散射科学协会（EISCAT）工作，后来又到了瑞典空间研究中心，最后加入了爱立信计算机科学实验室，Erlang就是在那里发明的。

如果你想要获知他更多的情况，翻翻这本书吧编程人生 (豆瓣)。

顺便八卦一下Erlang。Erlang这们语言最牛的地方就是强大的并行处理能力和容错机制。Armstrong的博客有篇浅显易懂的文章介绍Erlang背后的思想。值得一读。文章标题也很骠悍：“Concurrency Is Easy”。端的豪气万丈。简单说，Erlang能够创建和管理大量的进程（不是操作系统级别的）。那些进程在不同的操作系统上有同样的行为，可以被垃圾回收，对运行地点透明（location transparent），不会破坏其它进程的运行。任意两个进程间完全独立，不共享任何状态，一切交流通过消息来传递，当然也就无需上锁。这样的设计思想造就了适合解决如下问题的Erlang：

系统高度并发 － 支持几十万个并行行为
实时处理
计算高度分布
系统要求高度可靠：每年的脱机时间以分钟算，甚至永不当机
系统高度复杂：代码量以百万行计
持续操作：以年计
系统要求持续在线更新。 

　　 
而要做到这些，需要：


并发 进程
错误隔离 隔离进程
错误侦查 发现什么东西当掉了
错误诊断 为什么当掉
在线代码更新 持续进化的系统(同时运行多个版本)
稳定存储 崩溃恢复 

　　 
系统彻底独立，遵循所谓的无共享语义是系统稳定和高并发的关键。至于句法么，可以到www.erlang.org上去看指南。

引用
注：文章大多数内容整理自网络，感谢图灵和人民邮电出版社的授权。

          
          感谢 mengyidan1988 投递这篇资讯
                      声明：本文系ITeye网站发布的原创资讯，严禁任何网站转载本文，否则必将追究法律责任！
                    
          
          
          
            已有 0 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>21</docid>
	<title>探讨增强现实（AR）基于模型的追踪技术</title>
	<link>http://www.iteye.com/news/31903</link>
	<author></author>
	<content>
          引用
作者简介： 古鉴，于2015年加入暴风魔镜任 AR 合伙人，负责 AR 类产品及交互技术的研发工作。
本文为作者投稿，非经作者同意，请勿转载。欢迎技术投稿、约稿、给文章纠错，请发送邮件至mobile@csdn.net。

最近增强现实越来越被大家熟悉起来。最近看到一个案例（http://www.pps.tv/w_19rtditek5.html），见图一。 现代汽车发布电子用车手册，其亮点是融入了 AR 技术。车主只需拿起手机或平板对准车子，该 AR 系统即能自动识别这是车子的哪一部分，并给出相关的信息。目前，该用车手册涵盖了空气滤清器、智能巡航控制、警示标志、机油、制动液等方面的信息，同时还包含基于增强现实技术的视频、指南。



图一 现代汽车运用增强现实技术
很多人问我这是用到了什么样的 AR 技术，是不是 SLAM？没错，这里面的确用到了 SLAM 相关技术。更确切地说，应该是 SLAM 框架中的一个重要技术环节：基于模型的追踪技术（Model Based Tracking）。

今天，就让我们来讨论一下这个基于模型的追踪技术技术，我先从整体介绍一下增强现实中的追踪技术。增强现实技术的核心组成部分主要是(图二)：显示技术、输入交互技术和识别追踪技术。



图二 增强现实的核心技术
显示部分主要是光学成像相关技术，该部分与硬件相关的比较多，这里就不做展开介绍了。输入交互的技术主要涉及语音交互、手势交互等。这个我们会在下一期重点讨论。我认为有了交互技术及空间感知的追踪技术，即可实现了 AR 的基本功能。在这里，我们重点介绍追踪技术。

追踪技术是计算机科学中一个广阔而重要的领域，它在教育、娱乐、工业和制造领域具有广泛的应用。伴随着成像设备和硬件计算能力的发展，当前基于计算机视觉的物体跟踪技术取得了重大的发展。

追踪技术主要分为：

光学追踪；
内部传感器追踪；
超声波追踪；
磁力追踪；
机械装置追踪。
光学追踪是相对来说成本最低，比较精确的解决方案。它的难点主要在于视觉算法的门槛较高。AR 中的光学追踪分为几种（见图三），在 VR 中 OutsideIn 的方式用的比较多，比如 Oculus、HTC Vive、PS VR用的都是 Outside 进行的位置追踪。但是在AR中，基本都是 InsideOut，比如说 Hololens。当然最近 Oculus 的一体机用的也是 InsideOut 的追踪方式。

InsideOut 比 OutsideIn 更加方便，成本更加低。但目前，OutsideIn 的方式更为精确一些。如在一些工业场景，多使用反光材质的标示点或特殊材质进行 Outsidein 光学追踪。

标识追踪现在用的比较多，比如说大家看到的识字卡片、绘本等都是所谓的标识追踪，其最早的代表就是 ARToolkit 的。但该基于 Marker 的技术需要预先在 AR 场景中粘贴一定的标识，在用户体验和适应性方面具有较大的局限性。为此，基于无标识的 AR 技术是目前发展的方向。其中，无标识别追踪是实现该技术核心问题之一，像是 SLAM 等都是属于这个类别。我们这次主要介绍红色部分的 Model-Based Tracking。



图三 光学追踪分类
Model-Based Tracking 的主要技术框架如下图四所示，下面将该过程分为建模（Models)、视觉信息处理(Visual processing)、跟踪(Tracking)三个方面，并依次对其进行展开。



图四 基于模型跟踪的三个主要任务：Models，Visual processing，Tracking
1. 模型建立（Models）
模型（Models）的建立主要从目标、传感器和环境三个方面展开。首先对于跟踪目标，需要该目标一些先验信息（prior knowledge)，如被跟踪物体的形状、外观、姿态等特性。其次是需要从传感器的角度去考虑，比如如果使用未标定的单目相机，其获取的信息多以 pixel 为单位，若使用立体相机或深度相机，其构建的尺寸信息是 Metric 的，因此不同传感器的选择对被跟踪物体的表征形式会有不同影响。最后是从环境的角度，在基于视觉的跟踪过程中，环境对跟踪的准确性具有很关键的作用，比如在室内光照、环境相对稳定的情况下，重建所得模型就具有更加稳定，能很大程度提高基于模型跟踪的准确性。

当前，在模型建立（models）阶段，主要可分为 Online 和 Offline 两种模型构建技术。Online 在线建模方式：以 SLAM 为代表，其在追踪的同时建立模型，其中追踪和建模是两个相互依赖的过程，其彼此间的误差会相互传递并累加，为保证较好追踪精度，一般需要借助 Bundle Adjustment 等优化方法调整。但目前，该优化方法计算量较大。目前在 PC 端有较好的实现，在移动端尚存在计算能力相对不足的问题。Offline 离线建模方式：将建模和跟踪分开，规避了 SLAM 过程中建模的不确定性，同时克服了建模所需大量运算量的问题，因此该方法能提高跟踪的精度和效率。

目前 Offline 离线建模主要是有两种情况：

（1）已知目标物体的 CAD 形状，如模型在设计制造过程中已有的 CAD 先验知识，在工业应用及企业级别的应用场合，因此多以本情况为主。 
（2）通过传感器（如结构光扫描、RGBD、ToF等）逆向获取目标物体的模型信息，然后以此模型作为一个先验知识。在实际的生活应用中，个人参与者不大可能获取已有物体的 CAD 形状，因此只能借助简单的传感器离线获取模型，因此这种离线获取模型的方式更加适用于消费者。



图五 不同的物体跟踪形式（当前主要以Shape和Appearance为主）
2. 视觉信息处理 (Visual processing)
视觉信息处理主要是提取和关联传感器所获取的目标物体信息，以更新整个跟踪系统的状态。在实际的应用中，我们需要根据被跟踪目标的特性，选择稳定的跟踪线索（如颜色、边界、运动规律、背景信息等等）。

具体在实际应用过程中，是根据构建的模型信息，确定被预处理的对象。如在工业的 AR 等跟踪过程中，多以 CAD 的形状和拓扑信息为处理对象，该视觉信息处理过程即为对形状和拓扑信息的采样提取，和数据关联等。在 2D/3D 领域，多以其各自对应的纹理特征和形状特征为研究对象，整个视觉信息的处理过程是以提取、优化获取构建模型的稳定特征为目标。

目前，该方法可进一步细分为：基于颜色的信息处理；基于关键特征的信息处理；基于轮廓的信息处理和基于模板的信息处理四部分。

3. 跟踪 (Tracking)
当被传感器获取的数据经预处理输入后，即可实现物体的定位跟踪，该过程具体可以细分为检测和跟踪两个阶段。在检测阶段，需要以被跟踪物体的先验信息作为初始值，利用觉信息处理技术获取的模型信息，与输入的数据进行匹配，确定其在空间的姿态，实现对目标的跟踪。

目前针对移动端 6DoF（Degree of Freedom）的跟踪，主流是采用视觉和 IMU 进行融合，实现更稳定的跟踪（如 Tango）。

以上就是我们极少的 model-Based tracking. 以后我们会逐一介绍增强现实技术是如何实现的。
          
          感谢 mengyidan1988 投递这篇资讯
                      声明：本文系ITeye网站发布的原创资讯，严禁任何网站转载本文，否则必将追究法律责任！
                    
          
          
          
            已有 0 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>22</docid>
	<title>训练神经网络的五大算法</title>
	<link>http://www.iteye.com/news/31902</link>
	<author></author>
	<content>
          引用
原文： 5 algorithms to train a neural network 
作者： Alberto Quesada 译者： KK4SBB 
责编：何永灿，关注人工智能，投稿请联系 heyc@csdn.net 或微信号 289416419

神经网络模型的每一类学习过程通常被归纳为一种训练算法。训练的算法有很多，它们的特点和性能各不相同。



问题的抽象
人们把神经网络的学习过程转化为求损失函数f的最小值问题。一般来说，损失函数包括误差项和正则项两部分。误差项衡量神经网络模型在训练数据集上的拟合程度，而正则项则是控制模型的复杂程度，防止出现过拟合现象。

损失函数的函数值由模型的参数（权重值和偏置值）所决定。我们可以把两部分参数合并为一个n维的权重向量，记为w。下图是损失函数f(w)的图示。



如上图所示，w*是损失函数的最小值。在空间内任意选择一个点A，我们都能计算得到损失函数的一阶、二阶导数。一阶导数可以表示为一个向量：

ᐁif(w) = df/dwi (i = 1,…,n)

同样的，损失函数的二阶导数可以表示为海森矩阵（ Hessian Matrix ）：

Hi,jf(w) = d2f/dwi·dwj (i,j = 1,…,n)

多变量的连续可微分函数的求解问题一直被人们广泛地研究。许多的传统方法都能被直接用于神经网络模型的求解。

一维优化方法
尽管损失函数的值需要由多个参数决定，但是一维优化方法在这里也非常重要。这些方法常常用于训练神经网络模型。

许多训练算法首先计算得到一个训练的方向d，以及速率η来表示损失值在此方向上的变化，f(η)。下图片展示了这种一维函数。



f和η*在η1和η2所在的区间之内。

由此可见，一维优化方法就是寻找到某个给定的一维函数的最小值。黄金分段法和Brent方法就是其中两种广泛应用的算法。这两种算法不断地缩减最小值的范围，直到η1和η2两点之间的距离小于设定的阈值。

多维优化方法
我们把神经网络的学习问题抽象为寻找参数向量w*的问题，使得损失函数f在此点取到最小值。假设我们找到了损失函数的最小值点，那么就认为神经网络函数在此处的梯度等于零。

通常情况下，损失函数属于非线性函数，我们很难用训练算法准确地求得最优解。因此，我们尝试在参数空间内逐步搜索，来寻找最优解。每搜索一步，重新计算神经网络模型的参数，损失值则相应地减小。

我们先随机初始化一组模型参数。接着，每次迭代更新这组参数，损失函数值也随之减小。当某个特定条件或是终止条件得到满足时，整个训练过程即结束。

现在我们就来介绍几种神经网络的最重要训练算法。



1. 梯度下降法（Gradient descent）
梯度下降方法是最简单的训练算法。它仅需要用到梯度向量的信息，因此属于一阶算法。

我们定义f(wi) = fi and ᐁf(wi) = gi。算法起始于W0点，然后在第i步沿着di = -gi方向从wi移到wi+1，反复迭代直到满足终止条件。梯度下降算法的迭代公式为：

wi+1 = wi - di·ηi, i=0,1,…

参数η是学习率。这个参数既可以设置为固定值，也可以用一维优化方法沿着训练的方向逐步更新计算。人们一般倾向于逐步更新计算学习率，但很多软件和工具仍旧使用固定的学习率。

下图是梯度下降训练方法的流程图。如图所示，参数的更新分为两步：第一步计算梯度下降的方向，第二步计算合适的学习率。



梯度下降方法有一个严重的弊端，若函数的梯度变化如图所示呈现出细长的结构时，该方法需要进行很多次迭代运算。而且，尽管梯度下降的方向就是损失函数值减小最快的方向，但是这并不一定是收敛最快的路径。下图描述了此问题。



当神经网络模型非常庞大、包含上千个参数时，梯度下降方法是我们推荐的算法。因为此方法仅需要存储梯度向量（n空间），而不需要存储海森矩阵（n2空间）

2.牛顿算法（Newton’s method）
因为牛顿算法用到了海森矩阵，所以它属于二阶算法。此算法的目标是使用损失函数的二阶偏导数寻找更好的学习方向。

我们定义f(wi) = fi, ᐁf(wi) = gi and Hf(wi) = Hi。用泰勒展开式估计函数f在w0值

f = f0 + g0 · (w - w0) + 0.5 · (w - w0)2 · H0

H0是函数f在w0的海森矩阵值。在f(w)的最小值处g = 0，我们得到了第二个等式

g = g0 + H0 · (w - w0) = 0

因此，将参数初始化在w0，牛顿算法的迭代公式为

wi+1 = wi - Hi-1·gi, i = 0,1,…

Hi-1·gi 被称为牛顿项。值得注意的是，如果海森矩阵是一个非正定矩阵，那么参数有可能朝着最大值的方向移动，而不是最小值的方向。因此损失函数值并不能保证在每次迭代都减小。为了避免这种问题，我们通常会对牛顿算法的等式稍作修改：

wi+1 = wi - (Hi-1·gi) ·ηi, i=0,1,…

学习率η既可以设为固定值，也可以动态调整。向量d = Hi-1·gi被称为牛顿训练方向。

下图展示的是牛顿法的流程图。参数的更新也分为两步，计算牛顿训练方向和合适的学习率。



此方法训练神经网络模型的效率被证明比梯度下降法更好。由于共轭梯度法不需要计算海森矩阵，当神经网络模型较大时我们也建议使用。

4. 柯西-牛顿法（Quasi-Newton method）
由于牛顿法需要计算海森矩阵和逆矩阵，需要较多的计算资源，因此出现了一个变种算法，称为柯西-牛顿法，可以弥补计算量大的缺陷。此方法不是直接计算海森矩阵及其逆矩阵，而是在每一次迭代估计计算海森矩阵的逆矩阵，只需要用到损失函数的一阶偏导数。

海森矩阵是由损失函数的二阶偏导数组成。柯西-牛顿法的主要思想是用另一个矩阵G来估计海森矩阵的逆矩阵，只需要损失函数的一阶偏导数。柯西-牛顿法的更新方程可以写为：

wi+1 = wi - (Gi·gi)·ηi, i=0,1,…

学习率η既可以设为固定值，也可以动态调整。海森矩阵逆矩阵的估计G有多种不同类型。两种常用的类型是Davidon–Fletcher–Powell formula (DFP)和Broyden–Fletcher–Goldfarb–Shanno formula (BFGS)。

柯西-牛顿法的流程图如下所示。参数更新的步骤分为计算柯西-牛顿训练方向和计算学习率。



许多情况下，这是默认选择的算法：它比梯度下降法和共轭梯度法更快，而不需要准确计算海森矩阵及其逆矩阵。

5. Levenberg-Marquardt算法
Levenberg-Marquardt算法又称为衰减的最小 平方法，它针对损失函数是平方和误差的形式。它也不需要准确计算海森矩阵，需要用到梯度向量和雅各布矩阵。

假设损失函数f是平方和误差的形式：

f = ∑ ei2, i=0,…,m

其中m是训练样本的个数。

我们定义损失函数的雅各布矩阵由误差项对参数的偏导数组成，

Ji,jf(w) = dei/dwj (i = 1,…,m & j = 1,…,n)

m是训练集中的样本个数，n是神经网络的参数个数。雅各布矩阵的规模是m·n

损失函数的梯度向量是：

ᐁf = 2 JT·e

e是所有误差项组成的向量。

最后，我们可以用这个表达式来估计计算海森矩阵。

Hf ≈ 2 JT·J + λI

λ是衰减因子，以确保海森矩阵是正的，I是单位矩阵。

此算法的参数更新公式如下：

wi+1 = wi - (JiT·Ji+λiI)-1·(2 JiT·ei), i=0,1,…

若衰减因子λ设为0，相当于是牛顿法。若λ设置的非常大，这就相当于是学习率很小的梯度下降法。

参数λ的初始值非常大，因此前几步更新是沿着梯度下降方向的。如果某一步迭代更新失败，则λ扩大一些。否则，λ随着损失值的减小而减小，Levenberg-Marquardt接近牛顿法。这个过程可以加快收敛的速度。

下图是Levenberg-Marquardt算法训练过程的流程图。第一步计算损失值、梯度和近似海森矩阵。然后衰减参数和衰减系数。



由于Levenberg-Marquardt算法主要针对平方和误差类的损失函数。因此，在训练这类误差的神经网络模型时速度非常快。但是这个算法也有一些缺点。首先，它不适用于其它类型的损失函数。而且，它也不兼容正则项。最后，如果训练数据和网络模型非常大，雅各布矩阵也会变得很大，需要很多内存。因此，当训练数据或是模型很大时，我们并不建议使用Levenberg-Marquardt算法。

内存使用和速度的比较
下图绘制了本文讨论的五种算法的计算速度和内存需求。如图所示，梯度下降法往往是最慢的训练方法，它所需要的内存也往往最少。相反，速度最快的算法一般是Levenberg-Marquardt，但需要的内存也更多。柯西-牛顿法较好地平衡了两者。

总之，如果我们的神经网络模型有上千个参数，则可以用节省存储的梯度下降法和共轭梯度法。如果我们需要训练很多网络模型，每个模型只有几千个训练数据和几百个参数，则Levenberg-Marquardt可能会是一个好选择。其余情况下，柯西-牛顿法的效果都不错。
          
          感谢 mengyidan1988 投递这篇资讯
                      声明：本文系ITeye网站发布的原创资讯，严禁任何网站转载本文，否则必将追究法律责任！
                    
          
          
          
            已有 0 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>23</docid>
	<title>Facebook 用 Rust 重写 Mercurial 服务器</title>
	<link>http://www.iteye.com/news/31898</link>
	<author></author>
	<content>
          Linus Torvalds创建的Git分布式版本控制系统是市场占有率最高的版本控制系统：Git占了七成，另一个开源分布式版本控制系统Mercurial 则占了约13.5%。大多数开发者偏爱Git而对Mercurial 持怀疑态度，但如今他们正改变对Mercurial态度。

在本月早些时候举行的Mercurial开发者会议上， Facebook演示了Mercurial在可扩展性方面的优势：它开发了一个扩展高效缓存blame查询，查询速度提升了4-10x，以前要花10秒的blame查询现在只要1秒；Facebook正在用 Rust重写Mercurial 服务器，将支持可插拔的键值存储；Facebook演示了增强工作流的功能hg absorb；Facebook、Google和 Unity正在Mercurial上实验虚拟文件系统，加快处理大量文件和改进自动操作。

来自：Solidot
          
          感谢 mengyidan1988 投递这篇资讯
                    
          
          
          
            已有 0 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>24</docid>
	<title>苹果终于任命AI研究负责人，Ruslan其人其事</title>
	<link>http://www.iteye.com/news/31897</link>
	<author></author>
	<content>
          昨日，CMU卡内基梅隆大学副教授Ruslan Salakhutdinov发推宣布加入苹果，业内纷纷表示“苹果AI有救了”。



Ruslan Salakhutdinov拥有多伦多大学机器学习方向博士学位，师从人工智能之父Geoffrey Hinton。说起Geoffrey Hinton大家不会陌生，正是他自1960年代起，一步步将深度学习从边缘学科变成现如今AI技术的核心，进而引发此次深度学习热潮的掀起。Geoffrey Hinton麾下的人工智能圈子可谓把控着当今各巨头的深度学习命脉，从谷歌、微软、Facebook的人工智能负责人，到国内我们所熟知的百度首席科学家吴恩达，都出自Geoffrey Hinton组建的人工智能团队NCAP。

Ruslan当然也在这个圈子里，能成为Geoffrey的学生，Ruslan必然有很高的学术水平，根据其个人简历所述，Ruslan共发表论文105篇。据谷歌学术统计，其论文共被引15667次，h指数高达43。其发表的Restricted Boltzmann machines for collaborative filtering、Dropout: a simple way to prevent neural networks from overfitting、Bayesian probablisitic matrix factorization using MCMC三篇论文均有很高的科研价值。

2009年Ruslan从多伦多大学取得博士学位后，在麻省理工大学开始了他的博士后生涯，研究方向为：大脑与认知科学、计算机科学和人工智能。两年后（2011），Ruslan重回多伦多大学，任教于计算机科学系和统计科学系，职称为助理教授。今年2月，Ruslan前往卡内基梅隆大学（拥有世界上最好的计算机科学学院）任机器学习专业副教授。仅8个月后便拥有除副教授之外的第二重身份——苹果人工智能研究总监，其职业发展速度不可谓不快。

加入苹果后，Ruslan第一件事便是招兵买马。要加入他的团队，需要至少为计算机科学方向博士，且有高性能的机器学习系统的设计与研发经验，并且要求对深度学习、计算机视觉、机器学习、增强学习、系统优化以及数据挖掘均有造诣。团队要求如此之高，不禁让人好奇，苹果在AI方向究竟要有何动作。



一直以来，业界普遍认为苹果的人工智能弱于谷歌、微软、Facebook等公司，主要原因有二。一方面是因为众所周知的苹果喜欢保密的脾气。为了防止竞争者的效仿，苹果一般不会将研发成果发表。因此业内有声音认为，如此高产的Ruslan教授加入苹果后，其研发成果将会被苹果保密，这将是全世界深度学习行业的损失。第二方面是苹果尊重用户隐私的产品卖点，使其不能像其他行业巨头一样将用户的信息储存到云端并在云端强大的服务器上进行学习。因此苹果选择将AI学习系统放在个人的设备上，毫无疑问，这大大削弱了苹果AI的竞争力。

但苹果表示，对此他们已经有了很好的解决方案。其基于个人设备的深度学习系统，大小仅为200MB左右（取决于所储存的私人信息的多少）。然而神经网络的训练需要大量的数据样本，既然苹果称自己不会将用户的隐私数据存储至云端，那么该神经网络要如何训练？对此，苹果表示，其人工智能的训练包括两部分，在云端的针对大量不敏感信息的训练，以及在个人设备上的针对个人敏感信息的个性化训练。为此，苹果于今年六月推出了Differential Privacy技术，称为差别隐私保护方法。其具体原理为，在敏感数据中加入噪音，从而使系统不能识别用户的ID等信息，换句话说，系统在训练时不知道样本的出处，从而达到数据保护的目的。

此方法是否会行之有效？苹果尊重用户隐私权的做法是不是作茧自缚？抑或高人自有妙计？笔者相信苹果自有其一套行事准则。

近年来苹果通过收购深度学习公司获得了大量深度学习方面的人才，至今仍在广纳贤士的进程中。在苹果网站上进行职位搜索，能发现129个在招职位与深度学习有关。且苹果对于深度学习的招聘并非只看中经验，而是着重于候选人的基本素质，如数理及统计能力、计算机相关能力等。手握大量人才，苹果没有像其他各巨头一样成立深度学习研发中心，而是将深度学习专家分散至各个产品中，让深度学习的思想落地，这也成就了iOS10为用户带来的多重人工智能体验。

苹果的人工智能体现在细节中，而非某个重磅的人工智能产品。例如Siri的声音、Siri的理解能力、从主界面右滑会出现系统猜测的用户将要打开的APP等。底层一些的有通过深度学习算法延长设备的续航时间、iPad Pro手写功能防止手掌对纸张的滑动等等，都体现了苹果深度学习的应用。

对于此次苹果雇佣Ruslan， Hacker News上的一些评论很有意思。
引用

vonnik：Ruslan早在十年前就是人工智能方面的大粗腿了啊，他可是Geoff Hinton的学生啊，2006年那篇RBMs的论文上面就有他的名字啊。苹果这次赚大了。
joe_the_user：一个响当当的名字或许在华尔街或者媒体圈有用，但是在科技界，用处可能不大。原因在于：1. 机器学习发展速度可能是当今世界上所有学科里面最快的，因此在机器学习领域，人们关心的不是你之前十年做过多少研究有过多少成果，而是你现在用这个理论做了什么。2. 正是因为开放科研结果，机器学习才能发展到当下的程度，而苹果一直秉持着科研结果保密的态度，这使其对其他科研学者的吸引力大大降低。仅仅雇佣一位大牛对于其AI事业并不会有大的推动作用。
system16：从Swift看来，苹果保密的态度似乎已经有了放宽。我觉得他们从地图和Siri上面学到了教训，当一个产品需要大量的数据支撑，来为用户提供更好的交互体验的时候，闭关锁国是不行的。
Grandalf：我用iPhone五年了，但是我从来没用过Siri，前两天我的iPhone坏了，为了等iPhone7，我用了一个月的安卓。用安卓的时候我被OK Google惊艳到了，所以当我拿到IPhone7的时候我开始用Siri。用OK Google，你让它完成的事情它能做得很好，但是Siri就连播放音乐有的时候都做不到。
IBM：我觉得Siri表现得很好，用Siri放音乐你甚至不用碰手机一下。现在大家都在把Siri和Google Now作比较，我觉得这不公平。苹果自身没有搜索引擎，所以苹果没有强大的数据库来训练Siri来回答一些常识性问题。而且二者的定位也不一样，Siri是私人助手，是帮你做事情的。而且现在苹果也开放了一些Siri API，所以日后的功能肯定会越来越多。



          
          感谢 mengyidan1988 投递这篇资讯
                      声明：本文系ITeye网站发布的原创资讯，严禁任何网站转载本文，否则必将追究法律责任！
                    
          
          
          
            已有 1 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>25</docid>
	<title>【知识库专访】蒋守壮：Hive性能优化实战分享</title>
	<link>http://www.iteye.com/news/31896</link>
	<author></author>
	<content>
          【编者按】Hive作为Hadoop家族的重要一员，具有学习成本低，开发者可通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用。在攒库中，Hive也不负众望，得到了非常高的票数。为此，CSDN知识库特邀社区专家蒋守壮（博客：http://blog.csdn.net/jiangshouzhuang）绘制了Hive技术图谱，帮助广大开发者更加系统、全面的学习直播技术。

Hive知识库发布，速来关注！

与此同时，我们还采访了蒋守壮老师，分享他的技术成长之路以及对Hive技术的解读与思考。



CSDN社区专家、知识库特邀编辑蒋守壮
结缘大数据技术
CSDN:请简单地介绍一下自己。

蒋守壮：首先非常感谢CSDN能够给我这次被专访的机会，可以让我重新审视自己的职业发展历程，也希望能够帮助一些同行的朋友们。目前就职万达网络科技集团有限公司，是一名大数据分析师和大数据平台架构师。

我是电子专业出身，但自己对软件行业非常感兴趣，所以大学里一边学习本专业课程，一边到图书馆或活跃在相关技术网站上学习计算机专业课程。虽然累点苦点，但是为自己职业生涯打下了扎实的基础。

从毕业后至今，我已经在IT圈跌打滚爬5年多了，经历了很多，有苦有乐，这些都丰富了自己的阅历。工作以来，我一直热爱分享和交流技术，热衷于写博客（CSDN），参与开源社区（Apache一些顶级项目），也乐于在Github上开源自己的软件产品。慢慢地，我从一名菜鸟，成长为资深工程师和架构师，但是我要学习的东西太多了，我会一直坚持下去。

CSDN:你是如何与计算机结缘，踏上大数据开发之路的，能否跟我们分享一下。

蒋守壮：其实我与计算机结缘的原因，可能会和一些朋友相似，是因为黑客，觉得很酷，也很有挑战性的。这是一个启蒙阶段，于是我开始漫长的探索，学习汇编，C语言，C++，Java；学习计算机原理，Linux内核；学习网络，TCP/IP；学习密码学；学习各种主流数据库；学习脚本，Perl，Python，Shell等等。为了测试一些病毒，因为不可能用自己笔记本的操作系统测试，于是就开始研究虚拟化技术，使用虚拟机搭建各种Linux操作系统进行模拟测试，就这样，走上了一条不归路（是技术研究不归路，不是进局里了）。在这个探索的过程中，发现自己对Linux和数据库方面特别感兴趣，于是2011年毕业后就从事了Linux和数据库方面的工作。

从2012年底开始，我开始接触MPP（大规模并行处理）架构的关系型数据库，使用和维护过Vertica和Greenplum等MPP架构的数据库。在电信行业分析TB和PB级别海量数据时性能还是非常不错的。但是随着业务的发展，处理非结构化和半结构化数据的需求迫在眉睫。我一直关注和参与Apache开源社区，见证了Hadoop生态圈的快速发展，Hadoop非常方便处理非结构化和半结构化数据，于是我们的大数据平台架构开始融合Hadoop，形成混合架构。随着Hadoop生态圈家族不断壮大，支持各种场景的组件出现，SQL支持也非常完善，于是大数据平台架构就逐渐以Hadoop为核心来构建。尤其这两年来，Spark，Flink等通用并行框架不断发展壮大，集成批处理，流计算，实时分析，机器学习和图计算，为企业级构建大数据平台提供更多的选择。

CSDN:分享一下你最近使用的框架，它们都解决了哪些方面的问题。

蒋守壮：前段时间，我使用Kafka，Spark Streaming和HBase来进行实时数据计算分析，将保险用户相关的数据发送到Kafka消息队列，后端将从Kafka消费数据，并使用Spark Streaming进行流数据实时分析，然后将处理后的数据写入HBase集群中，最后用户从HBase中获取分析好的数据。

现阶段，我们基于现有的Hadoop大数据平台，集成Apache Kylin组件，Kylin可以对Hive中的表进行关联后多维度处理，并将结果写入HBase，其与Hadoop生态圈整合非常完善，非常适合用来做海量数据规模下的OLAP分析，进行实时或准实时查询上百亿数据。另外，Kylin和BI工具也可以很好的集成，比如Tableau，Saiku等。

Hive痛点直击&经验分享
CSDN：hive的使用场景有哪些？适合实时性强的分析场景使用么？

蒋守壮：Hive是建立在Hadoop上的数据仓库基础构架，它的最佳使用场合是大数据集的批处理作业，一般延迟性比较高，并不能够在大规模数据集上实现低延迟快速的查询，所以不适合实时性强的分析场景。不过目前Hive底层的计算框架除了支持原生的MapReduce，还支持Tez和Spark，这对提升Hive的查询处理性能帮助很大，从Hive 2.0版本开始，推荐使用Tez或Spark作为Hive的计算引擎。

如果希望实时查询分析，可以结合Impala，Presto，Drill等开源的交互式、实时的查询引擎使用，它们能够访问Hive中的表进行数据查询分析。

CSDN：hive的技术特点有哪些？ 类sql操作啊，内置大量用户函数udf等等。

蒋守壮：Hive应该是第一个出现的SQL on Hadoop的产品，技术特点如下：

类SQL查询方式，支持标准SQL也比较全面；
支持索引，加快数据查询；
元数据保存在关系型数据库中，比如MySQL，可以减少查询过程中执行语义检查的时间；
支持多种数据存储格式类型，比如Text，Sequence，RCFile，Parquet和ORC等，针对不同的场景进行选择；
数据存储在HDFS分布式文件系统中，实现冗余高可用；
内置常用的基本函数，以及窗口分析型函数，同时支持用户自定义UDF，UDAF，UDTF函数；
底层计算引擎支持MapReduce，Tez和Spark，根据需要进行动态选择。

CSDN：Hive创建的内部表和外部表有何异同？

蒋守壮：这个其实和很多传统数据库中的内部表和外部表一样，没有什么差别。

我将从表的创建和删除两方面简单介绍一下：

Hive 创建内部表时，后面执行导入操作时会将用户数据移动到表所在的数据仓库指向的路径；
若创建外部表时，只会记录表对应的用户数据所在的路径，不对用户数据的位置做任何改变。
在删除表的时候，内部表的元数据和用户数据会被一起删除；
而外部表只会删除元数据，不删除用户数据。这样外部表相对来说更加安全些，数据组织也更加灵活，方便共享源数据。


CSDN：Hive的优化技巧有哪些？比如如何处理数据倾斜、大表与小表join时，如何优化性能。

蒋守壮：Hive优化其实涉及到几方面，其中一方面是计算引擎方面的优化，比如你使用MapReduce作为计算引擎，那么就需要优化MapReduce；如果你选择Spark作为计算引擎，那么需要对Spark进行优化。

这里我仅从Hive这一层面介绍该如何优化，基本内容如下：

(1)表设计层面优化

合理利用中间结果集，避免查过就丢的资源浪费，减低Hadoop的IO负载
合理设计表分区，包括静态分区和动态分区
尽量不使用复杂或低效函数，比如count(distinct)，可以使用其他方式实现
选择合适的表存储格式和压缩格式
如果某些逻辑使用系统函数可能嵌套好几层，那么可以使用自定义函数实现
适当使用索引

(2)语法和参数层面优化

合理控制mapper和reducer数
设置map和reduce的内存大小
合并小文件
避免数据倾斜，解决数据倾斜问题

处理数据倾斜的方法其实有很多，不论是Group by还是Join时出现数据倾斜，其实都是数据热点的问题，即某些Key值太多，导致都分发到一个节点执行，那么我们可以将数据量比较大的Key拿出来单独处理，最后再合并到结果集中。如果出现数据倾斜的Key值对结果无关紧要，比如空值，那么我们可以过滤处理，或者将空值加上随机数，进行分发到集群的所有节点并行处理。当然也可以利用Hive自带的参数进行优化，设置当分组或关联的Key值超过多少数量时，进行单独处理，即额外启动一个MapReduce作业处理。 
这方面的具体优化过程，请参考我的技术博客。

减少Job数
Join优化

尽量将小表放到join的左边。小表和大表join时，如果差一个以及以上数量级并且小表数据量很小，可以使用mapjoin方式，将小表全部读入内存中，在map阶段进行表关联匹配。大表和大表进行关联时，要注意数据倾斜的问题。如果两个表以相同Key进行分桶，以及表的桶个数是倍数关系，可以使用bucket join，加快关联查询。

避免笛卡尔积
提前裁剪数据，减少处理的数据量，避免资源浪费

(3)Hive Job优化

并行化执行——每个查询被Hive转化成多个阶段，有些阶段关联性不大，则可以并行化执行，减少执行时间。
本地化执行
JVM重利用——JVM重利用可以是Job长时间保留slot，直到作业结束，这在对于有较多任务和较多小文件的任务是非常有意义的，减少执行时间。
推测执行——所谓的推测执行，就是当所有的task都开始运行之后，Job Tracker会统计所有任务的平均进度，如果某个task所在的节点配置内存比较低或者CPU负载很大，导致任务执行比总体任务的平均执行要慢，此时Job Tracker就会在其他节点启动一个新的相同的任务，原有任务和新任务哪个先执行完就把其他节点的另外一个任务kill掉。
Hive中间结果压缩数据——中间压缩就是处理Hive查询的多个job之间的数据，对于中间压缩，最好选择一个节省CPU耗时的压缩方式


CSDN：分享一下Hive数据仓库经验。

蒋守壮：由于Hive的类SQL和类数据库功能，它向非编程人员开放了大数据Hadoop生态系统，Hive也推动了Hadoop的普及和发展。

企业使用Hive来构建数据仓库，一是可以节约成本，二是基于SQL开发，将传统数据库迁移到Hadoop平台上分析也相关方便，三是支持和Hive集成的Hadoop生态圈的产品也丰富，满足架构扩展。

在使用Hive来构建企业级数据仓库时要注意以下几点：

根据不同业务数据来源，在Hive创建不同的数据库，方便分类管理；
表的文件存储格式尽量采用Parquet或ORC，不仅降低存储量，还优化了查询，压缩，表关联等性能；
Hive的计算引擎，推荐使用Tez或Spark；
实现用户权限的控制，针对不同项目设置相应的用户，相互之间权限独立，实现数据安全，也可以根据需要，授予相应表权限。


学习心得
CSDN：关于技术学习您有什么心得？我们上线了知识库系统化学习的方法，您会怎么应用呢？

蒋守壮：其实我学习任何一门新技术的过程都比较相似，可以总结为以下几点：

俗话说，工欲善其事，必先利其器，首先我一般都会根据官方文档将环境搭建起来，然后运行官方的示例，先从整体上感受一下。
阅读该新技术比较好的书籍或博客文档，最好结合官方文档一起阅读，一定要记得做笔记或写博客。与此同时，可以多加入一些技术群或公众号。
当对该技术的原理，架构，基本操作都熟练时，就开始寻找好的项目进行实战，现在Github上开源项目很多，可以学习别人的思路或开发过程。
源码分析，这个是加深理解该技术的关键步骤。如果你只希望达到应用的水平，可以不用分析源码；但是如果你希望能够优化或者定制该技术某些方面，那么分析源码是必经的过程。
对于源码分析，一定不要漫无目的的查看源码，因为现在很多开源软件的代码比较多，这样你将很容易进入黑洞，不可自拔，回头一看，可能啥都不知道了。源码分析先挑选简单的模块分析，搭建好调测平台，进行代码跟踪，这样可以增加自信心。假如我要分析Spark源码，我首先查看Spark启动、客户端访问部分的源码，看一下RDD底层源码如何实现，也就是你要选择一个主题去分析源码并跟踪调测，久而久之，我就会将很多模块内容串联起来，也慢慢加深对Spark源码的理解。
最后，对修改的源码进行重新编译打包，然后部署环境进行验证，体会激动的时刻。

上面的过程是交互循环的，不是一蹴而就的，你将需要经历时间的磨练，而且在这过程中，你会遇到很多问题，一定要对每个问题刨根问底，从根本上解决，并且坚持做好笔记或写博客，方便后续查看。

另外，CSDN推出的知识库，给我们提供了系统学习的方法。如果我要学习一门新技术，比如Docker，我可以进入Docker知识库，里面列出了Docker的每个知识点，并且每个知识点下面都会有该领域的专家精选的博客文章。此外，还可以把其它平台上看到的干货添加到自己的个人图谱当中，创建你的专属知识库。选择自己感兴趣的知识点进行系统学习，效率非常高。

CSDN：听说你在10月份即将推出自己的第一本技术书籍，请简单介绍一下。

蒋守壮：Apache的顶级项目Apache Kylin，这是一个开源的分布式分析引擎，是由eBay研发并贡献给开源社区，其提供Hadoop之上的SQL查询接口及多维分析（OLAP）能力以支持超大规模数据。书的名字暂定为《基于Apache Kylin构建企业级大数据分析平台》，预计10月份底左右出版，该书比较全面地介绍Apache Kylin的各方面，以及集成到现有的大数据平台中进行多维数据分析。

关注Kylin项目很久了，也见证了第一个由中国团队完整贡献到Apache的顶级项目。尤其这一年多，Kylin快速发展，功能和稳定性不断提升，我也积极地参与Kylin开源社区，同时也希望借助自己的微薄力量来推广Kylin的使用，帮助更多的朋友认识和使用Kylin来解决企业中基于Hadoop的多维数据分析的需求。

CSDN：你最近关注的技术有哪些？

蒋守壮：这段时间研究的技术，大体上有三方面，这里我就简单描述一下，具体大家可以访问官网查询更多内容：

第一方面：自动化运维工具Ansible。Ansible算是比较新的自动运维工具，基于Python开发，集合了众多运维工具的优点，实现了批量系统配置，批量程序部署和批量运行命令等功能。研究这方面内容，主要是因为项目组已经实现基于Ansible快速部署高可用并且安全的大数据平台，提升用户的体验效果和满意度。

第二方面：Flink实时流处理框架。个人一直觉得Flink在实时流方面的架构非常优秀，而且只需要很少的配置就能够实现高吞吐率和低延迟。将来希望在项目中更多地实战Flink实时流处理应用。

第三方面：Docker容器技术。深入研究Docker在分布式和大数据架构中的应用，比如基于Docker构建企业的私用镜像仓库，方便企业能够快速部署大数据平台，也可以将企业中的一些应用实现Docker微服务化。

推荐阅读：

【知识库专访】亲加CTO郝飞：直播技术架构解密与优化之道
前端开发人员必须了解的七大技能图谱
来吧 主流编程语言图谱+知识库都在这了
主流编程语言图谱+知识库（二）

          
          感谢 mengyidan1988 投递这篇资讯
                      声明：本文系ITeye网站发布的原创资讯，严禁任何网站转载本文，否则必将追究法律责任！
                    
          
          
          
            已有 0 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>26</docid>
	<title>Java 社区论坛 - Sym 1.6.0 发布</title>
	<link>http://www.iteye.com/news/31894-release-sym-160</link>
	<author></author>
	<content>
          
简介
Sym 是一个用 Java 写的现代化的社区论坛，欢迎来体验！（如果你需要搭建一个企业内网论坛，请使用 SymX）

非常详细的 Sym 功能点脑图

如果你在搭建或者二次开发时碰到问题，欢迎加 Q 群 17370164 进行讨论

Sym 的诞生是有如下几点原因：
（正版）

好用的 Java 开源论坛系统难以寻找
很多系统界面上仍然保持着老式风格，远远没有跟上前端发展的脚步
很多系统没有创新、好玩的特性，缺少现代化的用户体验
我们正在探索新的论坛模式，实现独奏（Solo）与协奏（Symphony）相结合的社区新体验


（野版）

做最 NB 的开源论坛系统
作者技痒，炫技之作


基本理念

实时交互
在浏览帖子时，传统论坛都是需要刷新页面来查看回帖的，而 Sym 则是基于 WebSocket 技术进行回帖推送，看帖时不需要刷新页面也可以看到其他人回帖。

互联
Sym 提供了 API 进行帖子、回帖的同步（B3log 构思），目前 Solo、Typecho、Z-BlogPHP、WordPress 均已经提供插件来进行内容同步，欢迎大家进行接入！

HTML5
Sym 使用了很多 HTML5 提供的技术特性，比如

通过使用本地存储防止编辑帖子/回帖时内容丢失
使用了音频特性来进行帖子/回帖音频录制、播放
复制/粘贴上传图片
CSS3 动画

通过使用这些技术，Sym 可以让用户在分享、交流时更加便捷、舒服

安装
需求：Maven3+、MySQL5.5+、Jetty9+/Tomcat9+


下载源码
解压后修改 src/main/resources/local.properties 中的数据库配置，并创建数据库

可能需要修改 latke.properties 中的端口为容器端口

可能需要修改 init.properties 中的管理员账号
使用 mvn install 进行构建
将构建好的 war 包部署到容器中，数据库表会在第一次启动时自动建立

注意：

没有数据库建表 SQL 脚本，手动建库后，表会在第一次启动时自动生成
生产环境建议使用反向代理，并需要配置好 WebSocket 代理
Tomcat 用 9 以上版本，最好是使用最新版本


配置

图片上传默认是上传服务器本地，要使用七牛可配置 symphony.properties 中的 qiniu.* 属性
将 WEB-INF/cron.xml 中注释掉的部分打开
邮件发送使用的是 SendCloud，需要配置 symphony.properties 中的 sendcloud.* 属性
用户注册时需要验证邮箱的，所以必须先配置好 SendCloud

如果遇到问题，可以参考一下这篇帖子。

案例

黑客派
宽客网
贵州IT
超级产品经理
Titandb 学习主站

如果你也搭建好了，欢迎通过 Pull Request 将你的站点加到这个列表中 :-p

商用授权
如果需要将 Sym 用于商用（比如公司搭建对外社区），则必须付费，报价 ￥4000，请联系我（Q845765）进行细节咨询。

开源授权
请仔细查看并遵循使用条款，尊重我们的劳动成果。
商用授权和开源授权在功能上没有任何区别，但商用授权后可以去除页脚版权部分。如果在未获得商用授权前私自去除版权部分，必将追究法律责任。

感悟
在实现 B3log 构思的这几年：

我们见证了 xAE（GAE/BAE/SAE/etc）的兴起与没落。2009 年选择了 GAE 作为服务器，并开始实现 Latke 框架来解决跨云平台，直到告别 GAE，不得不感叹技术更迭之快
感受到了自造轮子的优缺点，并且可以肯定一点：对于一个想要长久的产品来说，自制技术框架优势远大于劣势
一个好玩的产品或说是细节特性然并卵，需要做的是一个能够持续提供用户价值的产品/特性
虽然直到目前 B3log 系产品用户不多，但我们已经初步证明了：Java 用来实现博客、论坛没有什么不好的
使用开源软件，了解开源思想，融入开源
如果你想做个程序员相关的论坛，请三思
你怎么看待社群、社区这两个词？
UGC 社区价值生态


贡献
Sym 的主要作者是 Daniel 与 Vanessa，所有贡献者可以在这里看到。
我们非常期待你加入到这个项目中，无论是使用反馈还是代码补丁，都是对 Sym 一份满满的爱 ❤️

Terms

This software is open sourced under the Apache License 2.0
You can not get rid of the "Powered by B3log 开源 • Sym" from any page, even which you made
If you want to use this software for commercial purpose, please mail to support@liuyun.io for a commercial license request
Copyright © b3log.org, all rights reserved


鸣谢
Sym 的诞生离不开以下开源项目：


jQuery：前端 JavaScript 工具库

CodeMirror：前端 Markdown 编辑器内核

Highlight.js：前端代码高亮库

emojify.js：前端 Emoji 处理库

APlayer：前端 HTML5 音乐播放器

ECharts：前端 JavaScript 交互式图表库

MathJax：前端数学公式渲染引擎

SoundRecorder：前端 HTML5 录音库

ZeroClipboard：前端剪贴板支持

JavaScript MD5：前端 JavaScript MD5 库

ReconnectingWebSocket：前端 WebSocket 重连库

to-markdown：前端 HTML 转换 Markdown

UAParser.js：前端 User-Agent 解析库

Sass：前端 CSS 处理工具

jsoup：Java HTML 解析器

pegdown：Java Markdown 处理库

Apache Commons：Java 工具库集

Jodd：Java 工具库集

emoji-java：Java Emoji 处理库

User-Agent-Utils：Java User-Agent 解析库

Druid：Java 数据库连接池

FreeMarker：好用的 Java 模版引擎

Latke：Java Web 框架

NetBeans：全宇宙暂时排名第三的 IDE



功能图解
首页

帖子

（右边红色回帖按钮的位置应该在右下角，截图软件不给力..）
个人设置

发布编辑

Markdown 编辑器，支持 GFM 语法
LaTeX 数学公式
复制粘贴时自动转换为 Markdown
Chrome 下可以直接粘贴图片，其他浏览器支持拖拽
除了使用文字，也可以在帖子内进行录音
支持 Emoji
使用本地存储保障数据在未提交时不丢



移动端
移动端使用单独的模版进行渲染，解决通过一套模版自适应不能达成的效果和体验。


          
          感谢 88250 投递这篇资讯
                      声明：本文系ITeye网站发布的原创资讯，严禁任何网站转载本文，否则必将追究法律责任！
                    资讯来源：黑客派
          
          
          
            已有 0 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>27</docid>
	<title>【知识库专访】阿里孙佰贵：深度学习十问十答</title>
	<link>http://www.iteye.com/news/31893</link>
	<author></author>
	<content>
          【编者按】深度学习，作为人工智能研究中一个新的领域，其动机在于建立可以模拟人脑进行去分析学习，模仿人脑的逻辑去解释数据的神经网络。为了帮助大家能够更加系统化的学习该门课程，知识库特邀请了CSDN社区专家、知识库特邀编辑孙佰贵精心绘制了深度学习知识图谱。

点击进入深度学习知识库

与此同时，我们还采访了孙佰贵（博客地址：http://blog.csdn.net/sunbaigui）老师，分享他是如何与深度学习结缘以及技术层面的理解与思考。



孙佰贵
CSDN：请先简单地介绍一下自己。

孙佰贵：我叫孙佰贵，本科毕业于宁波大学，本科期间曾参加ACM编程竞赛取得最好成绩是省赛银奖，地区赛铜奖。 
硕士毕业于浙大CAD&CG国家重点实验室，硕士期间主要研究方向是计算机视觉（图像分割、视频分割、视频2D转3D）。 
硕士毕业后来到阿里巴巴工作至今，主要研究和应用方向为深度学习，包括图像分类、图像搜索、广告点击率预测，曾发表一篇ACMMM长文，论文地址http://arxiv.org/abs/1609.06018。在本科大一做ACM编程竞赛开始就坚持着写博客，在csdn上已经陆续写了7、8年。

CSDN：能否分享一下，你是如何与深度学习结缘，并从事这个领域的。

孙佰贵：在本科大三的时候接触到人工智能课程，对里面的手写数字识别尤其感兴趣，当时手写数字识别是用BP（深度学习的基石）网络去做，感觉非常酷。于是打算考研进修，因为觉得人机交互是人工智能普及的必经之路，所以读研期间选择了计算机视觉。

刚入职阿里巴巴工作时也找了图像方向的岗位，当时进到组里的时候，我对组里的各个业务方向还有技术方向都详细研究了下，由于我对人工智能一直有强烈的兴趣所以就选择了当时组里比较新的深度学习方向，然后我当时就成了阿里第二批做深度学习的人，第一批做的人很少，大概就几个，我师兄就是其中一个。

CSDN：在深度学习领域，目前有很多开源的工具和库，开发者该如何选择适合他们的深度学习框架？

孙佰贵：通常一聊到开源，大家首先会看这个开源代码写的性能是否足够好，代码风格是否足够好，支持的特性是否特别前沿。但是往往以上几点都有可能成为误区，一个真正好的开源是要有足够大的开发者人气的，并且它必须是易扩展的，然后还要是易移植的，其实只要有前两点，那这个开源库是一定会不停的更新，这点非常重要，很多开源一时热，但没有开发者基础，往往很容易就冷掉了。

好的开源也要是易移植的，因为工业上用开源的话，一般都会用到线上，这就要求这个开源必须是非常容易移植的，caffe在工业界比较受欢迎。

以上都是基于工业上的需求去选的，一般做学术的话可能会更加要求少量编程，所以一般theano，torch在学术界很受欢迎。

CSDN：如何利用高性能计算加速深度学习算法？

孙佰贵：现在深度学习的加速也是一个非常火热的研究方向，在硬件方面一般采用FPGA或者GPU进行加速，在算法方面一般采用并行计算去加速。

CSDN：目前Intel和Nvidia两家都砸重金押注深度学习，并在处理器、软件栈、生态建设等展开全方位布局，您对此有何评价？

孙佰贵：从商业上来讲，当前这两家在处理器、软件栈、生态建设等方面的布局其实可以看做是一个问题，那就是要卖处理器。早前，Intel专注于CPU，Nvidia专注于GPU分工比较明确，最早Intel在GPU领域有集成显卡，一般情况下也能满足个人的需求，所以导致Nvidia的市场份额一直都不大。近几年工业界在深度学习方面的突飞猛进式的研究与业务拓展，迎来了GPU的高需求期，Nvidia凭借其在GPU独立显卡领域的优势迅速提升了其销售额，也使得Nvidia的市值在最近提升了很多。

从深度学习生态上来讲，Intel在深度学习上的发力，使得工业界之后有更多的选择，也能加速深度学习硬件层面上的突破式创新，对于开发者来讲多了一种选择，这无疑是一种福音。

CSDN：从工程应用角度，您认为一个深度学习平台的易用性和效率哪方面更重要？

孙佰贵：这两者缺一不可，线上的性能效率往往直接决定了该应用能不能上线；然而易用性往往决定了迭代速度；在互联网里，应用的迭代速度与效率都非常重要。

CSDN：目前Google，微软，Facebook，百度分别开源了自己的深度学习平台，形成百花齐放，百家争鸣的局面，您认为哪个更适合初学者，哪个更适合生产部署？

孙佰贵：这三家公司开源的深度学习平台，都带有一些局限性，有些门槛太高，有些专注于某个特定领域，我认为最适合初学者的还是非商业公司的开源，比如caffe，torch等。

从易部署角度来讲，Google的TF比较好。

CSDN：阿里哪些典型业务使用了深度学习，其有效性体现在哪些方面？

孙佰贵：阿里有很多语音，图像，视频相关的都使用了深度学习，有效性在于深度学习在这些方向上的效果都远高于传统算法，这些方向能放大深度学习擅长特征表达学习的优势，因为原先在这些方向上特征的抽取往往都是人工制定的，人工制定的特征存在一个问题，那就是它并不是适用于所有的数据场景的，深度学习可以在任一一个数据场景中学习出专属于这个数据集的特征描述，这是深度学习的优势所在。

CSDN：深度学习理论方面有哪些由于当前技术水平限制还不能用于工程实现？

孙佰贵：由于深度学习强依赖于硬件、集群、大数据，这使得深度学习是少数的工业界成果不输于学术界成果的领域。深度学习是一个重实践的领域，一般情况下，深度学习理论都是在实验上验证过而沉淀下来的，所以往往是先由工程后有理论。

CSDN：技术路线方面，对于DL的入门和进阶，您有什么建议？我们上线了知识库系统化学习的方法，您会怎样应用？

孙佰贵：DL入门的话一般挑一些权威的survey或者书籍，如下作者写的一些资料比较权威：Yoshua Bengio、Geoffrey Hinton、Yann LeCun、Andrew Ng，然后挑一个开源跟着教程去跑例子，比较推荐torch、theano、caffe。

DL进阶的话一定要选择一个开源去做详细的代码解读，比较推荐caffe，然后再去了解各种深度学习相关的领域以及竞赛，再就是一定要去arxiv网站上定期查看DL的相关文章，了解DL最新研究状况与动态。

技术学习一般都是由点及面，上线的知识库可能会很大，除了了解该技术全面的知识点外，还需要挑这里面的某个细分领域去做仔细的研究与学习=>>进入深度学习知识库

推荐阅读：


【知识库专访】亲加CTO郝飞：直播技术架构解密与优化之道
CSDN技术主题月深度学习框架 精华集锦
前端开发人员必须了解的七大技能图谱
来吧 主流编程语言图谱+知识库都在这了
主流编程语言图谱+知识库（二）

更多内容请关注CSDN知识库。该产品汇集了领域专家们精心绘制的各重点技术领域的知识图谱，及由特邀编辑（领域专家）从海量数据中层层筛选出的精华内容和资源（学习视频、实践Demo、图书）。知识图谱可以帮助开发者全局把控该技术领域，而精选内容更能让开发者从技术细节加深了解该领域的每个核心技术点。扫描以下二维码，获取更多CSDN知识库内容。



个人知识图谱已上线，快来订制你的专属知识库吧=>>你的个人图谱上线了，快来领取！
          
          感谢 mengyidan1988 投递这篇资讯
                      声明：本文系ITeye网站发布的原创资讯，严禁任何网站转载本文，否则必将追究法律责任！
                    
          
          
          
            已有 0 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>28</docid>
	<title>JEECMS v8 发布，java 开源 CMS 系统</title>
	<link>http://www.iteye.com/news/31890</link>
	<author></author>
	<content>
          JEECMSv8 是java开源的站群管理系统，支持栏目模型、内容模型交叉自定义、以及具备支付和财务结算的内容电商为一体的内容管理系统。 

对于不懂技术的用户来说，只要通过后台的简单设置即可自定义出集新闻管理、图库管理、视频管理、下载系统、文库管理、政务公开、作品管理、产品发布、供求信息、房屋租售、招聘信息、网络问卷调查及留言板于一体的综合性且不失个性化门户网站。  
对于技术达人来说，jeecms不仅提供全部源码，而且在研发之初即全面的考虑了二次开发的高效性和代码的高移植性，是一款非常容易上手和二次开发的产品。  
JEECMSv8版本是一款集PC互联网、移动互联网和微信网站于一体的网站群管理系统，jeecmsV7不仅可以通过H5自适应的方式实现手机网页模板，还可以采用v7专享的移动端模板，实现了同一个网站PC端和移动端所展现的风格或者内容一致或完全不一样的效果，让网站更适应用户的浏览体验。  
JEECMSv8版本新增作者投稿、投稿管理、投稿佣金收益管理和内容赞赏功能，集成了支付宝和微信多种场景多终端的支付方案，为下一波内容电商从业人员提供技术动力。 


JEECMSv8支持PC网站、手机网站、微信网站、手机APP和可移动触摸大屏，一套系统即可完成五端一体化管理的网站建设。  

jeecmsV8新增功能：   
新增功能  
1、Spring3升级Spring4  hibernate3升级hibernate4  
2、文章打赏模块，支持微信、支付宝支付（电脑端扫码支付，手机端移动支付）支持作者提现，平台提现打款以及相关数据统计等  
3、应用中心作为插件功能，可快速查看官方发布最新功能模块、系统更新包、BUG修复包等。  
4、热词标签，热词新增推荐功能  
5、新增内容查询缓存时间，在不需要即时发布更新的情况下可以提升查询效率   
6、新增工作流是否允许跨级  
7、编辑器升级  
8、新增模型自定义字段也可以自定义上传的字段，模型字段增加是否必填选项控制  
9、模型区分全站、本站模型区分  
10、访问量统计新增月访问量、周访问量  
11、内容新增推荐级别功能  
12、评论调整每条评论要有点赞和回复功能，管理员以外的人也能回复，可配置是否开放评论 、允许重复评论  
13、投票选项后台新增上传图片功能  
14、文库更改模式，无需转换成swf，支持手机阅读  
15、更改后台首页数据调用  
16、栏目标签获取内容数量（总数、当月发布数、当日发布数、这周发布数、今年发布数）  
17、新增内容操作记录  
18、新增管理员查询条件 
19、网站前后台全面升级  
20、繁简体一键切换 

BUG修复  

修复 tomcat8下，后台配置资源管理错误
修复Tomcat8资源预览时，如预览图片时，路径出错  
修复静态化环境则采用根首页设置的话，手机静态首页会覆盖电脑端静态首页  
修复调查选项切换就不能添加调查项目  
修复主栏目会自动生成前台静态页 添加的副栏目的栏目页不因为增加有信息而自动生成静态页  
修复在后台登陆了，退出，然后在会员前台登陆后又跳转到后台了  
修复共享管理点击分页到所有内容列表的下一页了  
优化栏目修改处增加个模型选项，可以让用户调整  
优化数据统计完善  
修复发布时间不能修改问题  
修复部门管理权限 留言审核没有区分问题  
插件功能相关优化调整  
优化新增调整栏目名称自动生成栏目访问路径  
修复当新增加文章标题或描述内容超出字段长值时，系统直接报异常错误问题  
修复模型管理，帮助位置，输入2位以上数字，保存，页面报错问题  
修复内容类型编辑页面中，ID未实现判重校验问题  
修复投票管理-添加某一投票标题，点击提交 系统报错问题
修复后台页面中，列表中页数中输入11位数字回车后页面报错问题
前台演示地址：http://demo.jeecms.com/   （手机直接访问和PC端效果不一样哦）
后台演示地址：http://demo.jeecms.com/jeeadmin/jeecms/index.do
后台用户名：test    密码：test

系统安装包、源码下载地址：http://www.jeecms.com/download.jhtml
          
          感谢 jeecms 投递这篇资讯
                      声明：本文系ITeye网站发布的原创资讯，严禁任何网站转载本文，否则必将追究法律责任！
                    资讯来源：JEECMS
          
          
          
            已有 3 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>29</docid>
	<title>Noms：灵感来自Git的数据库</title>
	<link>http://www.iteye.com/news/31892</link>
	<author></author>
	<content>
          引用
原文：Noms – A versioned, forkable, syncable database 
作者：Noms 翻译：赖信涛 责编：仲培艺




Noms是一个类似于Git的去中心化的数据库。支持Fork，混合版本，同步等。目前已经在GitHub上开源，并且有两个编程语言的实现。一个是Go，一个是JavaScript。在Github上，还提供了很多实用工具以及应用范例。

关于Noms
Nom相比于其它数据库，主要有以下不同：

Content-addressed 如果你想要往Noms存放数据，不必担心要插入的数据是否存在。重复的数据在Noms中会被自动忽略。在Noms里，没有更新，只有插入；
Append-only 如果你想向Noms提交什么数据，不必担心覆盖的问题。你添加的记录都有历史版本。默认情况下，Noms永远不会删除数据，你可以查看数据库的整个历史版本，比较两次提交的不同，或者回退到之前的任何一个版本；
Typed 任何数据值，数据表和数据库的版本，都有一个类型（type），当你添加数据的时候，会自动生成。如果你对自己足够自信，也可以自己实现Noms对类型的处理；
Decentralized 如果我给你一份数据库，你和我都可以同时修改数据，互不影响。一段时间之后可以合并在一起。

Setup
Noms支持MacOSX和Linux。你可以自己从源代码变异Windows版本，一般情况下都能正常工作，但是并不受官方支持。


下载Noms最新版本
解压tar -xzf noms-*.tar.gz
运行

./noms ds http://demo.noms.io/cli-tour

./noms log http://demo.noms.io/cli-tour::sf-film-locations

Explore



讨论
niftich说：像这样的开源技术非常好。这可以用来做分布式的文件编辑应用。或者其它想要做分布式多个示例并支持以后合并的应用。

aboodman（作者）说： 以下介绍一些资源：一个原型查询语言，以及如何在Noms中创建索引的示例（https://www.youtube.com/watch?v=fv6_T5yaWns）；如何合并以及处理潜在的冲突（https://www.youtube.com/watch?v=–7dgoJBdjU）
          
          感谢 mengyidan1988 投递这篇资讯
                      声明：本文系ITeye网站发布的原创资讯，严禁任何网站转载本文，否则必将追究法律责任！
                    
          
          
          
            已有 0 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>30</docid>
	<title>ITeye新闻热点月刊：2016年9月总第103期发布了！</title>
	<link>http://www.iteye.com/news/31887</link>
	<author></author>
	<content>
          ITeye新闻热点月刊总第103期（2016年9月版）发布了！制作精美，内容丰富，为您总结一个月最精彩的技术新闻。 

【点击下载ITeye 2016年9月新闻热点月刊】 




>>精彩内容推荐：
微信小程序，大多数人都搞错的八个问题
小程序目前被炒得沸沸扬扬，无数媒体和企业借机获取阅读流量。 这再次证明一
点，微信想让什么火，真的就能让什么火。但四处传的消息很多是失真的，本文列
出8个多数人都搞错的问题。

浅谈RxJava与2.0的新特性
为什么 RxJava 如此受到 Android 开发者们的欢迎，不外乎两个原因。异步，链
式操作。

【点击下载ITeye 2016年9月新闻热点月刊】  

【查看所有ITeye新闻热点月刊】
          
          感谢 mengyidan1988 投递这篇资讯
                      声明：本文系ITeye网站发布的原创资讯，严禁任何网站转载本文，否则必将追究法律责任！
                    
          
          
          
            已有 0 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>31</docid>
	<title>2016年收入最高的5个编程语言</title>
	<link>http://www.iteye.com/news/31886</link>
	<author></author>
	<content>
          引用
译文链接：http://www.codeceo.com/article/top-5-paying-language-2016.html
英文原文：Top 5 Highest Paying Programming Languages of 2016
翻译作者：码农网 – 小峰


电子书网站Packt公布了其2016年技术提高报告。它提供的统计数据基于超过11,000名IT专业人士参与的关于2016年收入最高的编程语言的调研。它同时还介绍了流行的web框架和主题。

高效的程序员是基于IT的企业结构的中坚力量。企业雇用擅长于不同编程语言的人，并支付他们薪水，这样人员才不会被竞争对手撬走。



现在有许许多多的编程语言，数以千计的开发人员在用这些编程语言工作。但是，在薪水方面，学习有些编程语言的人超过学习其他语言的人。Packt，一个电子书网站，发布了2016年技术提高的调查报告，涉及来自世界各地的11500名开发人员。

该报告描述了领先的IT专业人员和开发人员所使用的编程语言的趋势。在受访者中，来自美国的人数最多，其次是英国。

最流行的编程语言是：

JavaScript
Python
Java
C
SQL




榜首是客户端脚本语言JavaScript。之所以对JavaScript的需求会升级，是因为越来越多的业务应用被转移到web浏览器。Python是一种通用语言，它可以用于从渗透测试到web开发。Python被广泛应用于各种组织，如D-LINK，惠普，飞利浦等。

而既老旧又年轻的C语言也在名单中。C语言主要用于年轻的程序员在学校上手编码的时候。所以，很明显，在企业部门的话C语言专家可能得不到太高的薪水。

然而，这些语言从财务角度来看都不是最好的。尽管它们很成功，但是开发人员并不能从这些语言上赚到最高的薪水。

更赚钱的编程语言都并不大受欢迎。平均年薪薪酬最高的语言是：




Bash——$ 100,000
Perl——$ 95,000
Scala——$ 90,000
SQL——$ 62,000
Delphi——$ 60,000

专家程序员使用诸如bash和Perl语言。因此，当决定职位薪资结构的时候，这些语言更受企业喜欢。谷歌的Go（$ 50,000）语言俨然已经获得了关注，比更受欢迎的用于为苹果生态系统编码app的Swift表现更佳。Golang被设计得更让C语言老将喜欢，其代码一直以来保持着可读性和简洁。

Golang被设计得更让C语言老将喜欢，其代码一直以来保持着可读性和简洁。它被接受主要是因为速度，快速的编码会话非常便捷。软件工程师以及应用软件开发是这种语言两个选择最多的工作。选择后端web开发的也相当多。

流行的Web开发框架：



Web框架中，AngularJS，是图表的佼佼者。然而，相当大百分比的开发人员已经开始或计划转移到几个月前发布的AngularJS 2。那些不盯着AngularJS 2的准备迎接更新的Facebook造的ReactJS。

热门研究主题：

除了编程语言和框架这些东西之外，调查也提及了开发者社区中感兴趣的流行领域。



机器学习和大数据自去年以来一直是IT行业的热门话题。我们最近看到基于IBM Watson的机器学习如何想出对一种罕见的血液癌症类型进行有效的治疗。

详细信息请阅读2016年技能提升报告。

有什么补充，欢迎告诉我们。
          
          感谢 mengyidan1988 投递这篇资讯
                    
          
          
          
            已有 2 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>32</docid>
	<title>Firefox发布了更多实验特性</title>
	<link>http://www.iteye.com/news/31885</link>
	<author></author>
	<content>
          引用
原文：Firefox Launches More Experimental Features 
作者：ARINDAM 翻译：赖信涛 责编：仲培艺

最近，又有三个实验性的项目作为Mozilla Test Pilot项目的一部分发布了。Mozilla Test Pilot主要给用户提供一些尚处在开发中，未正式发布的先进特性，在未来有可能加入到官方的安装包中。

在今年年初，Mozilla发布了项目的一部分，例如：

全局搜索（Universal Search）
标签中心（Tab Center）
不再有404页面（No More 404s）
Activity流（Activity Stream）

除此之外，最近又有三个特性加入其中：

页面截图（Page Shot）
小视频窗口（Min Vid）
上网痕迹保护（Tracking Protection）

下面是新特性速览，你可以安装来体验（文末有安装教程）。
Page Shot
在浏览网页时，你可以使用Mozilla的网页截图获取截图图片，而不必再调用系统的截图功能。只需要点击工具栏上的截图按钮，还可以选择截图的区域，并通过社交网络分享。截图的保存使用了Pagesshot.net的保存服务。



小视频窗口
小视频窗口特性可以让你在一个较小的窗口下，在浏览其它标签页的同时继续观看视频。这个特性目前兼容youtube和Vimeo视频，并计划在将来支持更多的视频网站。






上网痕迹保护
现在的上网痕迹保护只在Firefox的无痕浏览模式下有。现在这项实验性功能支持所有浏览和所有网站。你可以根据你的喜好对某些网站开启或关闭这项功能。

其他特性
标签页中心（Tab Center）

窗口的左侧会有一个带搜索功能的标签列表。这对宽屏显示器来说非常有用。tab窗口还支持自动隐藏。



活动流

你最近的活动历史会以一种全新的方式，带有分享功能，显示在下面。不仅仅像以前那样仅仅只有链接，它还会显示一些描述，缩略图以及分享等一些你可能需要的功能。



如何体验这些功能
在Firefox中打开https://testpilot.firefox.com/experiments/，安装Test Pilot扩展。然后安装你想要体验的特性。

安装成功之后，你将在工具栏看见这些按钮。点击按钮可以快速开关这些功能。






这些功能非常惊艳。但是请注意，它们依然处于开发中，所以你可能遇到一些bug，也可能让浏览器的性能变慢。但是它们仍然是值得体验的。

讨论
lewisl9029说：尝试了这些新特性之后，Firefox马上成了我最喜欢的浏览器！我非常喜欢Tab Center、Page Shot、Min Vid，和Tracking Protection这些功能！马上在我所有设备上的Firefox安装了这些扩展。希望Firefox能提供不同设备的Firefox之间的同步功能，现在，我不得不挨个安装他们。

SNvD7vEJ说：我希望Activity Stream可以将活动历史用某种树的形式展示，当你从网页进入某个链接，然后又退回来，进入另一个链接。这些动作会用树的形式展示出来，就更好了——或者说，他们已经有这个功能了，只是我不知道而已？
          
          感谢 mengyidan1988 投递这篇资讯
                      声明：本文系ITeye网站发布的原创资讯，严禁任何网站转载本文，否则必将追究法律责任！
                    
          
          
          
            已有 2 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>33</docid>
	<title>阿里云开放AliSQL数据库源码下载</title>
	<link>http://www.iteye.com/news/31882</link>
	<author></author>
	<content>
          10月14日，在2016杭州云栖大会上，AliSQL正式开放了源代码的下载，即日可在官网下载使用。



AliSQL是阿里巴巴基于于MySQL官方版本的一个分支，应用于阿里巴巴集团业务以及阿里云数据库服务。目前由阿里云数据库团队维护。

AliSQL版本在强度和广度上都经历了极大的考验。最新的AliSQL版本从其他开源分支比如：Percona，MariaDB，WebScaleSQL等社区得到帮助，也从阿里巴巴MySQL领域的经验和解决方案得到经验。

据阿里云资深总监李津表示，在AliSQL上赋予了300多个改进。

据悉，AliSQL增加更多监控指标，并针对电商秒杀、物联网大数据压缩、金融数据安全等场景提供个性化的解决方案。可帮助中小企业和开发者提升数据运营能力。

另有专家表示，“在通用基准测试场景下，AliSQL版本比MySQL官方版本有着70%的性能提升。在秒杀场景下，性能提升100倍。”

本文来自：开源中国
          
          感谢 mengyidan1988 投递这篇资讯
                    
          
          
          
            已有 0 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>34</docid>
	<title>高性能 TCP/UDP/HTTP 通信框架 HP-Socket v4.0.1 发布</title>
	<link>http://www.iteye.com/news/31880</link>
	<author></author>
	<content>
          

　　HP-Socket 是一套通用的高性能 TCP/UDP/HTTP 通信框架，包含服务端组件、客户端组件和 Agent 组件，广泛适用于各种不同应用场景的 TCP/UDP/HTTP 通信系统，提供 C/C++、C#、Delphi、E（易语言）、Java、Python 等编程语言接口。HP-Socket 对通信层实现完全封装，应用程序不必关注通信层的任何细节；HP-Socket 提供基于事件通知模型的 API 接口，能非常简单高效地整合到新旧应用程序中。
　　为了让使用者能方便快速地学习和使用 HP-Socket，迅速掌握框架的设计思想和使用方法，特此精心制作了大量 Demo 示例（如：PUSH 模型示例、PULL 模型示例、PACK 模型示例、性能测试示例以及其它编程语言示例）。HP-Socket 目前运行在 Windows 平台，将来会实现跨平台支持。
　　《HP-Socket v4.0 开发指南》
----------------------------------------------------------------
通用性

HP-Socket 的唯一职责就是接收和发送字节流，不参与应用程序的协议解析等工作。
HP-Socket 与应用程序通过接口进行交互，并完全解耦。任何应用只要实现了 HP-Socket 的接口规范都可以无缝整合 HP-Socket。

易用性

易用性对所有通用框架都是至关重要的，如果太难用还不如自己重头写一个来得方便。因此，HP-Socket 的接口设计得非常简单和统一。
HP-Socket 完全封装了所有底层通信细节，应用程序不必也不能干预底层通信操作。通信连接被抽象为 Connection ID，Connection ID 作为连接的唯一标识提供给应用程序来处理不同的连接。
HP-Socket 提供 PUSH / PULL / PACK 等接收模型， 应用程序可以灵活选择以手工方式、 半自动方式或全自动方式处理封解包， PULL / PACK 接收模型在降低封解包处理复杂度的同时能大大减少出错几率。

高性能


Client 组件：基于 Event Select 通信模型，在单独线程中执行通信操作，避免与主线程或其他线程相互干扰。每个组件对象管理一个 Socket 连接。

Server 组件：基于 IOCP 通信模型，并结合缓存池、私有堆（Private Heap）等技术，支持超大规模连接，在高并发场景下实现高效内存管理。

Agent 组件：对于代理服务器或中转服务器等应用场景，服务器自身也作为客户端向其它服务器发起大规模连接，一个 Agent 组件对象同时可管理多个 Socket 连接；Agent 组件与 Server 组件采用相同的技术架构，可以用作代理服务器或中转服务器的客户端部件。

伸缩性
　　应用程序能够根据不同的容量要求、通信规模和资源状况等现实场景调整 HP-Socket 的各项性能参数（如：工作线程的数量、缓存池的大小、发送模式和接收模式等），优化资源配置，在满足应用需求的同时不必过度浪费资源。
　　 (项目主页：点击这里，下载地址：点击这里)

*** v4.0.1更新 ***
 > 增加 HTTP 系列通信组件：
-----------------



新增 HTTP Server 组件 1) 组件类：CHttpServer、CHttpsServer 2) 实现接口：ITcpServer / IComplexHttpResponder 3) 监听器接口：IHttpServerListener
新增 HTTP Agent 组件 1) 组件类：CHttpAgent、CHttpsAgent 2) 实现接口：ITcpAgent / IComplexHttpRequester 3) 监听器接口：IHttpAgentListener
新增 HTTP Client 组件 1) 组件类：CHttpClient、CHttpsClient 2) 实现接口：ITcpClient / IHttpRequester 3) 监听器接口：IHttpServerListener
新增 HTTP 示例 Demo 1) TestEcho-Http （源代码） 2) TestEcho-Http-4C （4C LIB / 4C DLL）

Http 监听器 1) 监听器事件：
OnMessageBegin()	: 【可选】开始解析
OnRequestLine()		: 【可选】请求行解析完成（仅用于 HTTP 服务端）
OnStatusLine()		: 【可选】状态行解析完成（仅用于 HTTP 客户端）
OnHeader()		: 【可选】请求头通知
OnHeadersComplete()	: 【必须】请求头完成通知
OnBody()		: 【必须】请求体报文通知
OnChunkHeader()		: 【可选】Chunked 报文头通知
OnChunkComplete()	: 【可选】Chunked 报文结束通知
OnMessageComplete()	: 【必须】完成解析通知
OnUpgrade()		: 【可选】升级协议通知
OnParseError()		: 【必须】解析错误通知
2) 监听器事件返回值（EnHttpParseResult）：
HPR_OK		: 继续解析
HPR_SKIP_BODY	: 跳过当前请求 BODY（仅用于 OnHeadersComplete 事件）
HPR_UPGRADE	: 升级协议（仅用于 OnHeadersComplete 事件）
HPR_ERROR	: 终止解析，断开连接


 > 组件接口调整：
-----------------

IServer 和 IAgent 组件的所有监听器回调方法增加‘事件源’参数，如：OnShutdown() -> OnShutdown(T* pSender)
IClient 组件的所有监听器回调方法增加‘连接ID’参数，如：OnHandShake(IClient* pClient) -> OnHandShake(T* pSender, CONNID dwConnID)
IServer 和 IAgent 接口增加接口方法： Get/SetMaxConnectionCount() 用于设置最大连接数，最大连接数默认：10000
OnHandShake() 事件触发规则调整：非 SSL 组件在 OnConnect() 事件后也触发 OnHandShake() 事件，使 SSL 组件和 SSL 组件处理流程一致
HPSocket4C 增加 PACK 组件监听器的创建、销毁方法，新版本必须使用下列方法创建、销毁 PACK 组件监听器： 1) Create_HP_TcpPackServerListener / Destroy_HP_TcpPackServerListener2) Create_HP_TcpPackAgentListener / Destroy_HP_TcpPackAgentListener3) Create_HP_TcpPackClientListener / Destroy_HP_TcpPackClientListener
SSL 组件支持 SNI 1) SSL 初始化方法 HP_SSL_Initialize()，增加 SNI 回调函数指针参数 2) 新增方法 HP_SSL_AddServerContext()，用于 加载 SNI 主机证书

 > 其他更新：
-----------------

IServer 和 IAgent 组件采用 Ring Pool 取代 R/W Lock + Map 维护活动连接，提升读写和并发性能
更新所有 Demo 示例代码


*** v3.5.1更新 ***
 > 增加 SSL 系列通信组件：
-----------------

新增 SSL PUSH 组件：CSSLServer、CSSLAgent、CSSLClient
新增 SSL PULL 组件：CSSLPullServer、CSSLPullAgent、CSSLPullClient
新增 SSL PACK 组件：CSSLPackServer、CSSLPackAgent、CSSLPackClient
SSL Server 实现 ITcpServer 接口，SSL Agent 实现 ITcpAgent 接口，SSL Client 实现 ITcpClient 接口
启动 SSL 通信组件前需要调用 HP_SSL_Initialize() 函数初始化 SSL 全局环境参数
通信结束后调用 HP_SSL_Cleanup() 函数清理 SSL 全局运行环境

新增 SSL 相关示例 Demo：
1) TestEcho-SSL		（源代码）
2) TestEcho-SSL-Pack	（DLL / 4C DLL）
3) TestEcho-SSL-4C	（4C LIB）
4) TestEcho-SSL-PFM	（LIB）



> 组件接口调整：
-----------------

ITcpServerListener 接口增加 SSL 握手成功事件：OnHandShake(CONNID dwConnID)
ITcpAgentListener 接口增加 SSL 握手成功事件：OnHandShake(CONNID dwConnID)
ITcpClientListener 接口增加 SSL 握手成功事件：OnHandShake(IClient* pClient)
枚举类型 EnSocketError 增加‘SSL 环境未就绪’错误代码 SE_SSL_ENV_NOT_READY 
增加枚举类型：EnSSLSessionMode（SSL 工作模式），EnSSLVerifyMode（SSL 验证模式） 
HPSocket-SSL DLL 主要头文件：SocketInterface-SSL.h，HPSocket-SSL.h
HPSocket4C-SSL DLL 主要头文件：HPSocket4C-SSL.h
Tcp Pack 系列组件可设置的最大包长调整为 4194303/0x3FFFFF 字节
Tcp Pack 系列组件的有效包头标识取值范围调整为 0 ~ 1023/0x3FF

> 增加静态库工程：
-----------------

新增项目工程 HPSocketLIB 和 HPSocketLIB4C 用于编译 HPSocket 和 HPSocket4C 静态库
静态库与动态库的使用方式一致（请参考示例 Demo：TestEcho-SSL-4C 和 TestEcho-SSL-PFM）
使用 HPSocket 或 HPSocket4C 静态库时，需要在工程属性中定义预处理宏 -> HPSOCKET_STATIC_LIB
静态库目标文件不包含在发布包中（因为太大），如果需要请自行编译


*** v3.4.4更新 ***
 > 增加 Tcp Pack 系列通信组件：
-----------------

Tcp Pack 系列组件保证每个 OnReceive 事件都向应用程序提供一个完整数据包
Tcp Pack 系列组件是 PUSH/PULL 模式的结合体，应用程序不必处理分包（如：PUSH）与数据抓取（如：PULL）
Tcp Pack 系列组件提供 Get/SetMaxPackSize() 和 Get/SetPackHeaderFlag() 方法，用来设置最大包长和包头标识
CTcpPackServer 实现 ITcpServer 接口，CTcpPackAgent 实现 ITcpAgent 接口，CTcpPackClient 实现 ITcpClient 接口

> 组件接口调整：
-----------------

OnClose/OnError 合并为一个通信事件： OnClose(CONNID dwConnID, EnSocketOperation enOperation, int iErrorCode)
枚举类型 EnSocketOperation 增加一个枚举值： SO_CLOSE = 5，标识关闭 Socket 操作
IServer 和 IAgent 接口删除接口方法： Get/SetRecvPolicy()
IServer 和 IAgent 接口删除接口方法： Get/SetMaxShutdownWaitTime()

> Bug 修复：
-----------------

修复 TCP Pack Agent “异步连接失败导致程序崩溃” Bug
修复 vc-common-src 公共代码包的 CCASQueue 可能导致无限循环 Bug



          
          感谢 guaishou 投递这篇资讯
                      声明：本文系ITeye网站发布的原创资讯，严禁任何网站转载本文，否则必将追究法律责任！
                    
          
          
          
            已有 0 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>35</docid>
	<title>Facebook 发布新 Node 模块管理器 Yarn，或将取代 npm 客户端</title>
	<link>http://www.iteye.com/news/31881</link>
	<author></author>
	<content>
          引用
本文为掘金投稿，译文出自 : 掘金翻译计划（翻译不易，欢迎 Star 支持）。
原文链接 : Yarn: A new package manager for JavaScript
原文作者 : SEBASTIAN MCKENZIE，CHRISTOPH POJER，JAMES KYLE
译者 : 达仔
校对者: 根号三

在 JavaScript 社区中，工程师们互相分享成千上万的代码，帮助我们节省大量编写基础组件、类库或框架的时间。每个代码包可能都依赖于其他代码，而代码间的依赖关系则由包管理器负责维护。目前最流行的 JavaScript 包管理器是 npm 客户端，在 npm 仓库中提供了多达 30 万的软件包。据统计，已有超过 500 万的工程师使用 npm 仓库，其软件包下载量达到了 50 亿次/月。

在Facebook中，我们多年来一直在使用npm客户端并取得了成功，但随着代码仓库与团队人数的增长，我们在一致性、安全性以及性能方面遇到了挑战。在尝试解决每个方面的问题后，我们最终决定着手打造一套新的客户端解决方案，以帮助我们更可靠地管理依赖。我们把这个客户端工具称为 Yarn —— 更加快速、可靠、安全的 npm 客户端的替代品。

我们在此荣幸地宣布，我们与Exponent、Google和Tilde进行了合作，并开源 Yarn 项目。工程师在使用 Yarn 时，依然可以访问 npm 仓库，但 Yarn 能够更快速地安装软件包和管理依赖关系，并且可以在跨机器或者无网络的安全环境中保持代码的一致性。Yarn 提高了开发效率，并解决了共享代码时面临的一些问题，使得工程师们可以专注在构建新产品以及新特性上。

JavaScript包管理方式在Facebook的演变
在包管理工具出现之前，JavaScript 工程师们通常依赖的项目并不多，因此会把依赖直接存储在工程目录或上传到 CDN 上。在 Node.js 出现后不久，第一个主流的 JavaScript 包管理工具 npm 被引入进来，并很快成为了最受欢迎的包管理工具之一。从此，新的开源项目不断涌现，工程师们比起以前更加乐于分享代码了。

在 Facebook 中，我们有很多项目都要依赖 npm 仓库上的代码，比如 React。但随着内部规模的扩大，我们面临着以下挑战：在跨平台与跨用户之间安装依赖时的代码一致性问题、在安装依赖时花费太长时间、以及 npm 客户端自动执行某些依赖库的代码所导致的安全性问题。我们尝试过寻找这些问题的解决方案，但在这个过程中通常又会引起一些新的问题。

尝试修改npm客户端
在开始阶段，我们遵循了最佳实践，在代码仓库中只跟踪了 package.json 文件的变化，并要求工程师手动运行 npm install 命令安装依赖。这种模式在开发人员的电脑上没有问题，但在持续集成环境中遇到了困难，因为出于安全与可靠性的考虑，持续集成环境需要进行沙箱隔离，不能进行联网，因此也无法安装依赖。

接下来，我们尝试在代码仓库中跟踪整个node_modules目录的文件变化。虽然这种方式有效，却使得一些简单操作变得复杂化了。比如，对 babel 更新一个次要版本号时，会产生多达 800,000 行的提交记录，此外由于 lint 规则的存在，引起无效的 utf-8 字节序列、windows 换行符、非 png 压缩图片等问题时，将会导致工程师经常需要花费一整天的时间合并 node_modules 目录的文件。而我们负责源码控制的团队也指出，跟踪 node_modules 目录会引入过多的元数据。比如 React Native 的 package.json 文件目前只列出了68项依赖，但在运行 npm install 后，node_modules 目录整整包含了 121,358 个文件。

最后，为了有效组织 Facebook 逐渐增长的工程师人数以及管理需要安装的代码量，我们尝试修改 npm 客户端。我们决定压缩整个 node_modules 目录，并上传到内部 CDN，然后我们的工程师与持续集成系统都能从 CDN 上下载并解压文件，从而保证了代码一致性。这样我们就可以从源码控制系统中删除数以万计的文件了，但不足之处是工程师现在不仅在拉代码时需要联网了，构建也同样需要联网。

我们还试图为 npm 的 shrinkwrap 功能寻求优化方案，这个工具是用来锁定依赖版本号的。但 Shrinkwrap 功能的文件默认不会生成，如果开发者忘记了生成这一步骤，文件就不会被同步更新，因此我们编写了一个工具，以确定 Shrinkwrap 的文件内容和 node_modules 目录中的文件相符。这些文件由大量的 JSON 块组成，并且键名是无序的，因此每次更改通常会导致 Shrinkwrap 文件的内容大幅变化，难以进行代码审查。为减缓这一问题，我们还需要借助一个额外的脚本，对所有条目进行排序。

最后，通过 npm 升级单个依赖包时，基于 语义化版本号 规则，npm 通常会连同其他无关依赖一起更新。这使得每次更新都会比预期产生更多的变化，工程师们认为这样把 node_modules 提交上传到 CDN 的过程，难以达到预期的效果。

构建新客户端
与其围绕 npm 客户端继续构建基础设施，不如从整体上再次回顾这些问题。伦敦办公室的 Sebastian McKenzie 提出，如果我们建立一个新客户端工具以代替 npm 客户端，从而解决我们的核心问题呢？这一构思很快得到了我们的认同，团队对于这个主意也感到非常兴奋。

在开发过程中，我们与业界的工程师们进行了交流讨论，发现他们也面临着类似的问题，也尝试过许多类似的解决方案，通常只能把这些问题逐一解决。很明显，有必要把整个 JavaScript 社区正在面临的问题集合起来，然后我们就可以开发一个主流的解决方案了。在此感谢 Exponent、 Google 与 Tilde 的工程师们的协助，我们共同建立了 Yarn 客户端，并在每一个主流 JS 框架以及 Facebook 外的使用场景中测试验证了 Yarn 的性能。今天（2016-10-11），我们很荣幸把这个工具开源分享到社区中。

介绍 Yarn
Yarn 是一个新的包管理器，用于替代现有的 npm 客户端或者其他兼容 npm 仓库的包管理工具。Yarn 保留了现有工作流的特性，优点是更快、更安全、更可靠。

任何包管理器的主要功能都是安装某些软件包，软件包即用于特定功能的某段代码，通常是从一个全局的仓库安装到工程师的本地环境。每个软件包可以依赖于其他包，也可以不依赖。一个典型的项目结构的依赖树通常会包含数十个、数百个甚至上千个软件包。

这些依赖包通常是带版本号的，通过语义化版本控制（semver）安装。Semver 定义的版本号反映了每个新版本更改的类型，到底是进行了不兼容的API改动（MAJOR），还是添加了向后兼容的新特性（MINOR），还是进行了向后兼容的 bug 修复（PATCH）。然而，semver 依赖于软件包的开发者不能犯错误——如果依赖关系没有加锁，可能会引入一些破坏性更改或者产生新的 bug。

结构
在Node生态系统中，依赖通常安装在项目的 node_modules 文件夹中。然而，这个文件的结构和实际依赖树可能有所区别，因为重复的依赖可以合并到一起。npm 客户端把依赖安装到 node_modules 目录的过程具有不确定性。这意味着当依赖的安装顺序不同时，node_modules 目录的结构可能会发生变化。这种差异可能会导致类似“我的机子上可以运行，别的机子不行”的情况，并且通常要花费大量时间定位与解决。

Yarn通过lockfiles文件以及一个确定性的、可靠的安装算法，解决了版本问题和npm 的不确定性问题。Lockfile文件把安装的软件包版本锁定在某个特定版本，并保证 node_modules 目录在所有机器上的安装结果都是相同的。Lockfile 还使用简洁的有序键名的格式，保证了每次的文件变化最小化，进行代码审查也更为简单。

安装过程分为以下三个步骤：

处理： Yarn 通过向代码仓库发送请求，并递归查找每个依赖项，从而解决依赖关系。
抓取： 接下来，Yarn 会查找全局的缓存目录，检查所需的软件包是否已被下载。如果没有，Yarn 会抓取对应的压缩包，并放置在全局的缓存目录中，因此 Yarn 支持离线安装，同一个安装包不需要下载多次。依赖也可以通过 tarball 的压缩形式放置在源码控制系统中，以支持完整的离线安装。
生成： 最后，Yarn 从全局缓存中把需要用到的所有文件复制到本地的 node_modules 目录中。

通过清晰地细分这些步骤，以及确定性的算法支持，使得 Yarn 支持并行操作，从而最大化地利用资源，并加速安装进程。在一些 Facebook 的项目上，Yarn 甚至可以把安装过程降低一个数量级，从几分钟到只需几秒钟。Yarn 还使用了互斥锁，以确保多个 CLI 实例同时运行时不会互相冲突与影响。

纵观整个过程，Yarn 对于软件包安装加上了严格的限制。你可以对哪个生命周期脚本作用于哪个软件包进行控制。软件包的 checksum 也会存储在 lockfile 中，以确保每一次安装都可以得到同一个包。

特性
Yarn 除了让安装过程变得更快与更可靠，还添加了一些额外的特性，从而进一步简化依赖管理的工作流。

同时兼容npm与bower工作流，并支持两种软件仓库混合使用
可以限制已安装模块的协议，并提供方法输出协议信息
提供一套稳定的公有JS API，用于记录构建工具的输出信息
可读、最小化、美观的 CLI 输出信息


Yarn 用于生产环境
我们已经在 Facebook 中把 Yarn 用于生产环境，并且效果非常理想。Yarn 有效地管理了许多 JavaScript 项目的包依赖关系。在每次迁移时，构建都可以离线进行，因此加速了工作流程。我们基于 React Native 在不同条件下进行安装时间测试，比较了 Yarn 与 npm 的性能，具体参见这里。



起步
最简单的起步方法是：

npm install -g yarnpkg
yarn

yarn CLI 代替了原有开发工作流中 npm CLI 的作用，用法可能是单纯的替代，也可能是一个新的、相似的命令：

npm install → yarn

不需要带参数，yarn 命令会读取 package.json 文件，然后从 npm 仓库中抓取软件包，并放置到 node_modules 目录中。等价于运行 npm install。

npm install --save <name> → yarn add <name>

我们避免了 npm install <name> 命令中安装“不可见的依赖”的行为，并分离出一个新命令。运行 yarn add <name> 等价于运行 npm install --save <name>。

未来
目前已经有许多成员一起参与到 Yarn 的构建中，以解决我们的共同问题，我们也希望 Yarn 未来能真正成为一个大众化的社区项目。Yarn 目前已经在GitHub开源 ，我们也已经准备好向 Node 社区进行推广：使用 Yarn、分享构思、编写文档、互相支持，并帮助构建一个很棒的社区来进行长期维护。我们相信 Yarn 已经拥有一个良好的开局，如果有你的帮助，Yarn的未来将会更加美好。
          
          感谢 mengyidan1988 投递这篇资讯
                      声明：本文系ITeye网站发布的原创资讯，严禁任何网站转载本文，否则必将追究法律责任！
                    
          
          
          
            已有 0 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>36</docid>
	<title>15 个开源的顶级人工智能工具</title>
	<link>http://www.iteye.com/news/31877</link>
	<author></author>
	<content>
          AI是科技研究中最热门的方向之一。像 IBM、谷歌、微软、Facebook 和亚马逊等公司都在研发上投入大量的资金、或者收购那些在机器学习、神经网络、自然语言和图像处理等领域取得了进展的初创公司。考虑到人们对此感兴趣的程度，我们将不会惊讶于斯坦福的专家在人工智能报告中得出的结论：“越来越强大的人工智能应用，可能会对我们的社会和经济产生深远的积极影响，这将出现在从现在到 2030 年的时间段里。”

在最近的一篇文章中，我们概述了 45 个十分有趣或有前途的人工智能项目。在本文中，我们将聚焦于开源的人工智能工具，详细的了解下最著名的 15 个开源人工智能项目。

以下这些开源人工智能应用都处于人工智能研究的最前沿。

1. Caffe



它是由贾扬清在加州大学伯克利分校的读博时创造的，Caffe 是一个基于表达体系结构和可扩展代码的深度学习框架。使它声名鹊起的是它的速度，这让它受到研究人员和企业用户的欢迎。根据其网站所言，它可以在一天之内只用一个 NVIDIA K40 GPU 处理 6000 万多个图像。它是由伯克利视野和学习中心（BVLC）管理的，并且由 NVIDIA 和亚马逊等公司资助来支持它的发展。

2. CNTK



Computational Network Toolkit的缩写，CNTK是一个微软的开源人工智能工具。不论是在单个 CPU、单个 GPU、多个 GPU 或是拥有多个 GPU 的多台机器上它都有优异的表现。微软主要用它做语音识别的研究，但是它在机器翻译、图像识别、图像字幕、文本处理、语言理解和语言建模方面都有着良好的应用。

3. Deeplearning4j



Deeplearning4j是一个 java 虚拟机（JVM）的开源深度学习库。它运行在分布式环境并且集成在 Hadoop 和 Apache Spark 中。这使它可以配置深度神经网络，并且它与 Java、Scala 和 其他 JVM 语言兼容。

这个项目是由一个叫做 Skymind 的商业公司管理的，它为这个项目提供支持、培训和一个企业的发行版。

4. DMTK



DMTK是分布式机器学习工具Distributed Machine Learning Toolkit的缩写，和 CNTK 一样，是微软的开源人工智能工具。作为设计用于大数据的应用程序，它的目标是更快的训练人工智能系统。它包括三个主要组件：DMTK 框架、LightLDA 主题模型算法和分布式（多义）字嵌入算法。为了证明它的速度，微软声称在一个八集群的机器上，它能够“用 100 万个主题和 1000 万个单词的词汇表（总共 10 万亿参数）训练一个主题模型，在一个文档中收集 1000 亿个符号，”。这一成绩是别的工具无法比拟的。

5. H20



相比起科研，H2O更注重将 AI 服务于企业用户，因此 H2O 有着大量的公司客户，比如第一资本金融公司、思科、Nielsen Catalina、PayPal 和泛美都是它的用户。它声称任何人都可以利用机器学习和预测分析的力量来解决业务难题。它可以用于预测建模、风险和欺诈分析、保险分析、广告技术、医疗保健和客户情报。

它有两种开源版本：标准版 H2O 和 Sparking Water 版，它被集成在 Apache Spark 中。也有付费的企业用户支持。

6. Mahout



它是 Apache 基金会项目，Mahout是一个开源机器学习框架。根据它的网站所言，它有着三个主要的特性：一个构建可扩展算法的编程环境、像 Spark 和 H2O 一样的预制算法工具和一个叫 Samsara 的矢量数学实验环境。使用 Mahout 的公司有 Adobe、埃森哲咨询公司、Foursquare、英特尔、领英、Twitter、雅虎和其他许多公司。其网站列了出第三方的专业支持。

7. MLlib



由于其速度，Apache Spark 成为一个最流行的大数据处理工具。MLlib是 Spark 的可扩展机器学习库。它集成了 Hadoop 并可以与 NumPy 和 R 进行交互操作。它包括了许多机器学习算法如分类、回归、决策树、推荐、集群、主题建模、功能转换、模型评价、ML 管道架构、ML 持久、生存分析、频繁项集和序列模式挖掘、分布式线性代数和统计。

8. NuPIC



由Numenta公司管理的NuPIC是一个基于分层暂时记忆Hierarchical Temporal Memory，HTM理论的开源人工智能项目。从本质上讲，HTM 试图创建一个计算机系统来模仿人类大脑皮层。他们的目标是创造一个 “在许多认知任务上接近或者超越人类认知能力” 的机器。

除了开源许可，Numenta还提供NuPic的商业许可协议，并且它还提供技术专利的许可证。

9. OpenNN



作为一个为开发者和科研人员设计的具有高级理解力的人工智能，OpenNN是一个实现神经网络算法的 c++ 编程库。它的关键特性包括深度的架构和快速的性能。其网站上可以查到丰富的文档，包括一个解释了神经网络的基本知识的入门教程。OpenNN的付费支持由一家从事预测分析的西班牙公司Artelnics提供。

10. OpenCyc



由 Cycorp 公司开发的OpenCyc提供了对Cyc知识库的访问和常识推理引擎。它拥有超过 239,000 个条目，大约 2,093,000 个三元组和大约 69,000 owl：这是一种类似于链接到外部语义库的命名空间。它在富领域模型、语义数据集成、文本理解、特殊领域的专家系统和游戏 AI 中有着良好的应用。该公司还提供另外两个版本的 Cyc：一个可免费的用于科研但是不开源，和一个提供给企业的但是需要付费。

11. Oryx 2



构建在Apache Spark和Kafka之上的Oryx 2是一个专门针对大规模机器学习的应用程序开发框架。它采用一个独特的三层 λ 架构。开发者可以使用 Orys 2 创建新的应用程序，另外它还拥有一些预先构建的应用程序可以用于常见的大数据任务比如协同过滤、分类、回归和聚类。大数据工具供应商 Cloudera 创造了最初的 Oryx 1 项目并且一直积极参与持续发展。

12. PredictionIO



今年的二月，Salesforce 收购了PredictionIO，接着在七月，它将该平台和商标贡献给 Apache 基金会，Apache 基金会将其列为孵育计划。所以当 Salesforce 利用 PredictionIO 技术来提升它的机器学习能力时，成效将会同步出现在开源版本中。它可以帮助用户创建带有机器学习功能的预测引擎，这可用于部署能够实时动态查询的 Web服务。

13. SystemML



最初由 IBM 开发，SystemML现在是一个Apache大数据项目。它提供了一个高度可伸缩的平台，可以实现高等数学运算，并且它的算法用 R 或一种类似 python 的语法写成。企业已经在使用它来跟踪汽车维修客户服务、规划机场交通和连接社会媒体数据与银行客户。它可以在 Spark 或 Hadoop 上运行。

14. TensorFlow



TensorFlow是一个谷歌的开源人工智能工具。它提供了一个使用数据流图进行数值计算的库。它可以运行在多种不同的有着单或多 CPU 和 GPU 的系统，甚至可以在移动设备上运行。它拥有深厚的灵活性、真正的可移植性、自动微分功能，并且支持 Python 和 c++。它的网站拥有十分详细的教程列表来帮助开发者和研究人员沉浸于使用或扩展他的功能。

15. Torch



Torch将自己描述为：“一个优先使用 GPU 的拥有机器学习算法广泛支持的科学计算框架”，它的特点是灵活性和速度。此外，它可以很容易的通过软件包用于机器学习、计算机视觉、信号处理、并行处理、图像、视频、音频和网络等方面。它依赖一个叫做 LuaJIT 的脚本语言，而 LuaJIT 是基于 Lua 的。
          
          感谢 mengyidan1988 投递这篇资讯
                    
          
          
          
            已有 1 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>37</docid>
	<title>2016收入最高的5门编程语言 Java和C没在前五</title>
	<link>http://www.iteye.com/news/31876</link>
	<author></author>
	<content>
          


电子书网站Packt公布了其2016年技术提高报告。它提供的统计数据基于超过 11,000 名 IT 专业人士参与的关于 2016 年收入最高的编程语言的调研。它同时还介绍了流行的 web 框架和主题。

高效的程序员是基于 IT 的企业结构的中坚力量。企业雇用擅长于不同编程语言的人，并支付他们薪水，这样人员才不会被竞争对手撬走。

现在有许许多多的编程语言，数以千计的开发人员在用这些编程语言工作。但是，在薪水方面，学习有些编程语言的人超过学习其他语言的人。Packt，一个电子书网站，发布了 2016 年技术提高的调查报告，涉及来自世界各地的 11500 名开发人员。

该报告描述了领先的 IT 专业人员和开发人员所使用的编程语言的趋势。在受访者中，来自美国的人数最多，其次是英国。

最流行的编程语言是：
引用
Python

JavaScript

Java

PHP

SQL




客户端脚本语言 JavaScript 与榜首 Python 不相上下。之所以对 JavaScript 的需求会升级，是因为越来越多的业务应用被转移到 web 浏览器。Python 是一种通用语言，它可以用于从渗透测试到 web 开发。Python 被广泛应用于各种组织，如D-LINK，惠普，飞利浦等。

而既老旧又年轻的C语言也在名单中。C语言主要用于年轻的程序员在学校上手编码的时候。所以，很明显，在企业部门的话C语言专家可能得不到太高的薪水。

然而，这些语言从财务角度来看都不是最好的。尽管它们很成功，但是开发人员并不能从这些语言上赚到最高的薪水。

更赚钱的编程语言都并不大受欢迎。平均年薪薪酬最高的语言是：



引用
Bash——$ 100,000

Perl——$ 95,000

Scala——$ 90,000

SQL——$ 62,000

Delphi——$ 60,000

专家程序员使用诸如 bash 和 Perl 语言。因此，当决定职位薪资结构的时候，这些语言更受企业喜欢。谷歌的 Go（$ 50,000）语言俨然已经获得了关注，比更受欢迎的用于为苹果生态系统编码 app 的 Swift 表现更佳。Golang 被设计得更让C语言老将喜欢，其代码一直以来保持着可读性和简洁。它被接受主要是因为速度，快速的编码会话非常便捷。软件工程师以及应用软件开发是这种语言两个选择最多的工作。选择后端 web 开发的也相当多。

流行的Web开发框架：



Web框架中，AngularJS，是图表的佼佼者。然而，相当大百分比的开发人员已经开始或计划转移到几个月前发布的 AngularJS 2。那些不盯着 AngularJS 2 的准备迎接更新的 Facebook 造的 ReactJS。

热门研究主题：

除了编程语言和框架这些东西之外，调查也提及了开发者社区中感兴趣的流行领域。



机器学习和大数据自去年以来一直是 IT 行业的热门话题。

未来君认为能赚钱的编程语言都是好语言！~

本文来自：网易订阅
          
          感谢 mengyidan1988 投递这篇资讯
                    
          
          
          
            已有 0 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>38</docid>
	<title>Spark 2.0 时代全面到来 —— 2.0.1 版本发布</title>
	<link>http://www.iteye.com/news/31875</link>
	<author></author>
	<content>
          距离Spark 2.0.0发布两个月后，Spark 2.0.1版本发布了，这是一个修正版本，共处理了300多个Issue，涉及spark稳定性和bug等方面的修复 ，它的发布意味着Spark 2.0接近生产环境使用要求，想要尝试Spark 2.0的可以动手了。

Apache Spark 2.0是基于spark branch-2.x 开发的，相比于branch-1.0，它在功能和性能等方面均有巨大改进。在性能方面，Spark 2.x 有2~10倍的提升；在功能方面，Spark SQL中的Dataset变得成熟，Spark 2.x通过Dataset重构了Spark Streaming和MLlib的API，进而使得这两个系统在易用性和性能方面有重大提升，在不久的将来，Dataframe/Dataset API（high-level API）将取代RDD API（low-level API），成为主流的Spark编程接口。

Apache Spark 2.x在性能和功能方面的改进主要包括：

1. 性能方面

相比于Spark 1.0，Spark 2.0在引擎性能方面有重大优化，其优化主要体现在Spark Core和Spark SQL两个系统上，其优化主要得益于Tungsten计划（“钨丝计划”），其主要动机是优化Spark内存和CPU的使用，使其能够逼近物理机器的性能极限。

利用“整阶段代码生成”（“whole stage code generation”），使得SQL和DataFrame中算子性能优化2-10倍
通过“向量化计算”提升Parquet格式文件的扫描吞吐率
提升ORC格式文件的读写性能
提升Catalyst查询优化器性能


2. 功能方面

（1）Spark Core/SQL:Tungsten Phase 2，优化CPU与Memory方面

“钨丝计划”完成第二阶段任务，在内存和CPU使用方面进一步优化Spark引擎性能，重构了大量数据结构和算法的实现，使得Dataframe/Dataset性能得到显著提升，这使得Dataframe/Dataset有能力成为其他几个系统（比如Spark Streaming和MLlib）的基础API。

注：“钨丝计划”包括三个方面的优化：

Memory Management and Binary Processing： Java GC严重，且java对象内存开销大，可采用类似C语言机制，直接操纵binary data（sun.misc.Unsafe）
Cache-aware Computation：合理使用CPU的L1/L2/L3 cache，设计对cache友好的算法
Code Generation：可去除条件检查，减少虚函数调度等

（2）Spark SQL: 统一DataFrame与Dataset API

众所周知，在Spark 1.x中，DataFrame API存在很多问题，包括不是类型安全的(not type-safe)，缺乏函数式编程能力（not object-oriented）等，为了克服这些问题，社区引入了Dataset，相比于DataFrame，它具有以下几个特点：类型安全，面向对象编程方式；支持非结构化数据（json）；java与scala统一接口和性能极好的序列化框架等，她将成为Spark未来主流的编程接口（RDD API是low-level API，而Dataset则是high-level API）。

（3）Spark SQL：支持SQL 2003

Spark SQL在通用性方面有重大突破，它跑通了所有（99个）TPC-DS查询 ，并有以下几个改进：

解析器可同时支持ANSI-SQL 和Hive QL
实现了DDL
支持大部分子查询
支持View

（4）Spark Streaming：引入Structured Streaming

Spark Streaming基于Spark SQL（DataFrame / Dataset ）构建了high-level API，使得Spark Streaming充分受益Spark SQL的易用性和性能提升。Spark Streaming重构的API主要是面向结构化数据的，被称为“Structured Streaming”，其主要特性包括：

DataFrame / Dataset API的支持
提供了Event time, windowing, sessions, sources & sink等API
连接流式数据与静态数据集
交互式查询结果：通过JDBC server将RDD结果暴露出去，以便于交互式查询

（5）Spark MLlib: MLlib 2.0诞生

Spark MLlib朝着2.0进化，主要体现在机器学习模型的多样化，持久化和定制化上，具体包括：

广义线性模型的全面实现
Python & R API的支持
增强模型持久化能力
Pipieline定制化

Apache Spark 2.0在功能和性能的重大改进，使得它在分布式计算领域进一步巩固了自己的地位，随着Spark应用越来越广泛，它将变成数据工程师的一项基本技能。

Apache Spark 2.0.1 发行说明：https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315420&version=12336857

Apache Spark 2.0.1下载地址：http://spark.apache.org/downloads.html

Apache Spark 2.0 新特性与展望：http://www.chinahadoop.cn/course/697

来自：开源中国
          
          感谢 mengyidan1988 投递这篇资讯
                    
          
          
          
            已有 0 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>39</docid>
	<title>【最好用的】Java APNS开源库apns4j-1.1.0发布</title>
	<link>http://www.iteye.com/news/31869-javaapns-apns4j-apns</link>
	<author></author>
	<content>
          
开源地址：https://github.com/teaey/apns4j
【最好用的】Java APNS开源库apns4j-1.1.0发布


提升API的易用性，移除繁杂的初始化逻辑，入口集中到cn.teaey.apns4j.Apns4j


提供简单的异步服务


增加ios7以上静默推送设置（content-available）


sendAndFlush -> send 或许开始就是个错误！


减少builder模式，能够更好的与spring等框架做集成


groupId调整：cn.teaey.apns4j


<dependency>
    <groupId>cn.teaey.apns4j</groupId>
    <artifactId>apns4j</artifactId>
    <version>1.1.0</version>
</dependency>

//Step 1
ApnsChannelFactory apnsChannelFactory = Apns4j.newChannelFactoryBuilder()
.keyStoreMeta("${path to your keystore}")
.keyStormPwd("${keystore password}")
.build();

//Setp 2
ApnsChannel apnsChannel = apnsChannelFactory.newChannel();

//Step 3 create & init notify payload
ApnsPayload apnsPayload = Apns4j.newPayload()
        .alertActionLocKey("FixMe")
        .alertTitle("Title")
        .alertBody("Pushed by apns4j")
        .sound("default");

//Step 4 send via channel
apnsChannel.send(TestConts.deviceToken, apnsPayload);

//Step 5 in the end, apnsChannel can be Recycle and Reuse 
apnsChannel.close();
 
有疑问请留言，或者联系本人： masfay@me.com github：http://github.com/teaey
 
 
开源地址：https://github.com/teaey/apns4j

          
          感谢 冲杯茶喝 投递这篇资讯
                      声明：本文系ITeye网站发布的原创资讯，严禁任何网站转载本文，否则必将追究法律责任！
                    资讯来源：http://github.com/teaey
          
          
          
            已有 0 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>40</docid>
	<title>Hugo：一个快速、现代的静态网页引擎</title>
	<link>http://www.iteye.com/news/31874</link>
	<author></author>
	<content>
          Hugo是一个制作静态页面的工具，非常灵活，可以以多种形式工作，是制作博客、文档、个人履历等非常合适的工具。Hugo激发了创造力，让建造网页变得充满乐趣。

一次编写，到处运行
Hugo可能是你用过的最容易安装的软件——只要下载打开就可以了！Hugo并不依赖任何权限、数据库、运行库、解释器或外部库。使用Hugo编写的网站可以运行在S3，Github Pages，Dropbox或其它任何静态网页托管上。

强大的性能
Hugo为速度和性能而生。我们付出很大的精力来尽可能地减少编译时间。大多数编译只要毫秒级的时间就可以完成！

灵活
Hugo可以灵活地适应你的任务。你可以使用任何你喜欢的方式组织URL，通过自定义的index页面和分类组织内容。使用任何形式定义你自己的元数据，例如YAML、TOML或JSON。最棒的是，Hugo不需要任何配置文件就可以处理这些差异。Hugo就是好。

讨论

sfifs说：我喜欢Hugo最大的原因是：几乎所有其它的引擎都要求我安装完整的Ruby或JavaScript开发环境，才能预览我的博客，而Hugo就不需要。
eberkund说：为什么我要从jekyll换到Hugo呢？现在已经有无数的网站生成工具了，而且它们几乎都有差不多的特性。有没有两者都用过的人来解释一下，这些工具究竟有什么不同？
andmarios说：对我来说，最重要的是他们的静态库如何。


我确实用过Jekyll，但是对于一个会运行很多年的网站来说，照顾一个ruby app是我不想插手的事。也许我的服务器和我的桌面端的Ruby版本不同，也许一些Ruby Gems已经在我的发行版安装了，但不是我想要的，这就得重新手动安装它们，每次Gem提供的功能出了什么问题，我就得手动重新安装一下，还得SSH到服务器再重装。有时候由于发行版的原因，一些Gem还不能装。也许jekyll哪天发了新版本，用了最新Ruby特性。

Hugo在这方面有很多优势，比如内置服务器、实时载入和在本地测试时方便的链接转换等。

托管地址：https://github.com/spf13/hugo
          
          感谢 mengyidan1988 投递这篇资讯
                      声明：本文系ITeye网站发布的原创资讯，严禁任何网站转载本文，否则必将追究法律责任！
                    
          
          
          
            已有 0 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>41</docid>
	<title>TIOBE 2016年10月编程语言排行榜：Go语言将问鼎2016年度编程语言宝座？</title>
	<link>http://www.iteye.com/news/31872</link>
	<author></author>
	<content>
          距2016年度编程语言的宣布仅剩三个月了，这次宝座将会花落谁家呢？与去年同期相比，2016年仅有两种语言的增长率超过了1%，分别是Go语言和Groovy语言。

不过要注意的是：Groovy在2015年底出现了爆炸性的增长，因此在2017年1月左右的增长速度可能就不会太快了。因此，谷歌的Go语言似乎就是那个唯一了，这其中Docker这个以Go语言写就的容器应用普及开来很可能在其中助了一把力。

其他类似Objective-C、Swift以及R语言等编程语言的年度增长率虽接近1%，但应当还达不到年度编程语言的标准。特别是再次挤入候选名单的Objective-C，从2014年4月的12.875%下跌到2016年1月的1.074%，不过目前又在回升了。

编程语言排行榜 TOP20 榜单



下面是第 21-50 位的编程语言，排名如下：



Top 10编程语言TIOBE指数走势（2002-2016）



下面是50-100名：由于差异较小，仅将名称列在下面（按照首字母排序）
引用
(Visual) FoxPro, 4th Dimension/4D, ABC, ActionScript, APL, AutoLISP, bc, BlitzMax, Bourne shell, C shell, CFML, cg, Common Lisp, Crystal, Eiffel, Elixir, Elm, Forth, Hack, Icon, IDL, Inform, Io, J, Julia, Korn shell, Kotlin, Maple, ML, MQL4, MS-DOS batch, NATURAL, NXT-G, OCaml, OpenCL, Oz, Pascal, PL/I, PowerShell, REXX, S, Simulink, Smalltalk, SPARK, SPSS, Standard ML, Stata, Tcl, VBScript, Verilog

Top 10编程语言排行榜更长期走势（1986-2016）
（注：该位次取自12个月的平均值）



年度编程语言（2003-2015）



必须声明这个榜单本身采集的是英文世界的数据，虽然在反映趋势上有一些参考意义，但与中国的实际情况不完全符合，而且，这张采样本身也有相当大的局限性。

【说明】

TIOBE 编程语言社区排行榜是编程语言流行趋势的一个指标，每月更新。这份排行榜排名基于互联网上有经验的程序员、课程和第三方厂商的数量。排名使用著名的搜索引擎（诸如Google、MSN、Yahoo!、Wikipedia、YouTube以及Baidu等）进行计算。请注意这个排行榜只是反映某个编程语言的热门程度，并不能说明一门编程语言好不好，或者一门语言所编写的代码数量多少。

这个排行榜可以用来考查你的编程技能是否与时俱进，也可以在开发新系统时作为一个语言选择依据。具体解释可以点击这里查看。

原文链接： TIOBE Index for October 2016
          
          感谢 mengyidan1988 投递这篇资讯
                      声明：本文系ITeye网站发布的原创资讯，严禁任何网站转载本文，否则必将追究法律责任！
                    
          
          
          
            已有 0 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>42</docid>
	<title>深度学习性能提升的诀窍</title>
	<link>http://www.iteye.com/news/31871</link>
	<author></author>
	<content>
          引用原文：How To Improve Deep Learning Performance 
作者： Jason Brownlee 
翻译： KK4SBB 责编：何永灿
克服过拟合和提高泛化能力的20条技巧和诀窍

你是如何提升深度学习模型的效果？ 
这是我经常被问到的一个问题。 
有时候也会换一种问法： 
我该如何提高模型的准确率呢？ 
……或者反过来问： 
如果我的网络模型效果不好，我该怎么办？ 
通常我的回答是“具体原因我不清楚，但我有一些想法可以试试”。 
然后我会列举一些我认为能够提升性能的方法。 
为了避免重复罗列这些内容，我打算在本文中把它们都写出来。 
这些想法不仅可以用于深度学习，事实上可以用在任何机器学习的算法上。



如何提升深度学习的性能 
Pedro Ribeiro Simoes拍摄

提升算法性能的想法
这个列表并不完整，却是很好的出发点。 
我的目的是给大家抛出一些想法供大家尝试，或许有那么一两个有效的方法。 
往往只需要尝试一个想法就能得到提升。 
如果你用下面某一种想法取得了好效果，请在评论区给我留言！ 
如果你还有其它想法或是对这些想法有拓展，也请告诉大家，或许会对我们大家有帮助！ 
我把这个列表划分为四块：

从数据上提升性能
从算法上提升性能
从算法调优上提升性能
从模型融合上提升性能

性能提升的力度按上表的顺序从上到下依次递减。举个例子，新的建模方法或者更多的数据带来的效果提升往往好于调出最优的参数。但这并不是绝对的，只是大多数情况下如此。 
我在文章中添加了不少博客教程和相关的经典神经网络问题。 
其中有一些想法只是针对人工神经网络，但大多数想法都是通用性的。你可以将它们与其它技术结合起来使用。 
我们开始吧。

1.从数据上提升性能
调整训练数据或是问题的抽象定义方法可能会带来巨大的效果改善。甚至是最显著的改善。 
下面是概览：

收集更多的数据
产生更多的数据
对数据做缩放
对数据做变换
特征选择

重新定义问题
1）收集更多的数据
你还能收集到更多的训练数据吗？ 
你的模型的质量往往取决于你的训练数据的质量。你需要确保使用的数据是针对问题最有效的数据。 
你还希望数据尽可能多。 
深度学习和其它现代的非线性机器学习模型在大数据集上的效果更好，尤其是深度学习。这也是深度学习方法令人兴奋的主要原因之一。 
请看下面的图片：



什么是深度学习？ 
幻灯片来自Andrew Ng

不总是数据阅读效果越好，多数情况下如此。如果让我选择，我会选择要更多的数据。 
相关阅读：

数据集压倒算法

2) 产生更多的数据
深度学习算法往往在数据量大的时候效果好。 
我们在上一节已经提到过这一点。 
如果由于某些原因你得不到更多的数据，也可以制造一些数据。

如果你的数据是数值型的向量，那么随机生成已有向量的变形向量。
如果你的数据是图像，用已有的图像随机生成相似图像。
如果你的数据是文本，做法你懂得……

这类做法通常被称为数据扩展或是数据生成。 
你可以使用生成模型，也可以用一些简单的小技巧。 
举个例子，若是用图像数据，简单地随机选择和平移已有的图像就能取得很大的提升。它能提升模型的泛化能力，如果新的数据中包含这类变换就能得到很好的处理。 
有时候是往数据中增加噪声，这相当于是一种规则方法，避免过拟合训练数据。 
相关阅读：

深度学习中的图像数据扩充
训练含有噪声的数据

3) 对数据做缩放
此方法简单有效。 
使用神经网络模型的一条经验法宝就是： 
将数据缩放到激活函数的阈值范围。 
如果你使用sigmoid激活函数，将数据缩放到0~1之间。如果选用tanh激活函数，将值域控制在-1~1之间。 
输入、输出数据都经过同样的变换。比如，如果在输出层有一个sigmoid函数将输出值转换为二值数据，则将输出的y归一化为二进制。如果选用的是softmax函数，对y进行归一化还是有效的。 
我还建议你将训练数据扩展生成多个不同的版本：

归一化到0 ~ 1
归一化到-1 ~ 1
标准化
然后在每个数据集上测试模型的性能，选用最好的一组生成数据。 
如果更换了激活函数，最好重复做一次这个小实验。 
在模型中不适合计算大的数值。此外，还有许多其它方法来压缩模型中的数据，比如对权重和激活值做归一化，我会在后面介绍这些技巧。 
相关阅读：

我需要对输入数据（列向量）做标准化吗?
如何用Scikit-Learn准备机器学习的输入数据

4） 对数据做变换
与上一节的方法相关，但是需要更多的工作量。 
你必须真正了解所用到的数据。数据可视化，然后挑出异常值。 
先猜测每一列数据的分布

这一列数据是不是倾斜的高斯分布，若是如此，尝试用Box-Cox方法纠正倾斜
这一列数据是不是指数分布，若是如此，则进行对数变换
这一列数据是不是存在某些特性，但是难以直观地发现，尝试一下对数据平方或者开方
是否可以将特征离散化，以便更好地强调一些特征
凭你的直觉，尝试几种方法

是否可以用投影的方法对数据预处理，比如PCA？
是否可以将多个属性合并为单个值？
是否可以发掘某个新的属性，用布尔值表示？
是否可以在时间尺度或是其它维度上有些新发现？
神经网络有特征学习的功能，它们能够完成这些事情。 
不过你若是可以将问题的结构更好地呈现出来，网络模型学习的速度就会更快。 
在训练集上快速尝试各种变换方法，看看哪些方法有些，而哪些不起作用。 
相关阅读：

如何定义你的机器学习问题
特征挖掘工程，如何构造特征以及如何提升
如何用Scikit-Learn准备机器学习的输入数据

5） 特征选择
神经网络受不相关数据的影响很小。 
它们会对此赋予一个趋近于0的权重，几乎忽略此特征对预测值的贡献。 
你是否可以移除训练数据的某些属性呢？ 
我们有许多的特征选择方法和特征重要性方法来鉴别哪些特征可以保留，哪些特征需要移除。 
动手试一试，试一试所有的方法。 
如果你的时间充裕，我还是建议在相同的神经网络模型上选择尝试多个方法，看看它们的效果分别如何。

也许用更少的特征也能得到同样的、甚至更好的效果。
也许所有的特征选择方法都选择抛弃同一部分特征属性。那么就真应该好好审视这些无用的特征。
也许选出的这部分特征给你带来了新的启发，构建出更多的新特征。
相关阅读：

特征选择入门介绍
基于Python的机器学习中的特征选择问题

6) 问题重构
在回到你问题的定义上来。 
你所收集到的这些观测数据是描述问题的唯一途径吗？ 
也许还有其它的途径。也许其它途径能更清晰地将问题的结构暴露出来。 
我自己非常喜欢这种练习，因为它强迫我们拓宽思路。很难做好。尤其是当你已经投入大量的时间、精力、金钱在现有的方法上。 
即使你列举了3 ~ 5种不同的方式，至少你对最后所选用的方式有充足的信心。

也许你可以将时间元素融入到一个窗口之中
也许你的分类问题可以转化为回归问题，反之亦然
也许可以把二值类型的输出转化为softmax的输出
也许你可以对子问题建模
深入思考问题是一个好习惯，最好在选择工具下手之前先完成上述步骤，以减少无效的精力投入。 
无论如何，如果你正束手无策，这个简单的连续能让你思如泉涌。 
另外，你也不必抛弃前期的大量工作，详情可以参见后面的章节。

相关阅读：

如何定义机器学习问题

2. 从算法上提升性能
机器学习总是与算法相关。 
所有的理论和数学知识都在描述从数据中学习决策过程的不同方法（如果我们这里仅讨论预测模型）。 
你选用深度学习来求解，它是不是最合适的技术呢？ 
在这一节中，我们会简单地聊一下算法的选择，后续内容会具体介绍如何提升深度学习的效果。 
下面是概览：

算法的筛选
从文献中学习
重采样的方法

我们一条条展开。

1） 算法的筛选
你事先不可能知道哪种算法对你的问题效果最好。 
如果你已经知道，你可能也就不需要机器学习了。 
你有哪些证据可以证明现在已经采用的方法是最佳选择呢？ 
我们来想想这个难题。 
当在所有可能出现的问题上进行效果评测时，没有哪一项单独的算法效果会好于其它算法。所有的算法都是平等的。这就是天下没有免费的午餐理论的要点。

也许你选择的算法并不是最适合你的问题。 
现在，我们不指望解决所有的问题，但当前的热门算法也许并不适合你的数据集。 
我的建议是先收集证据，先假设有其它的合适算法适用于你的问题。 
筛选一些常用的算法，挑出其中适用的几个。

尝试一些线性算法，比如逻辑回归和线性判别分析
尝试一些树模型，比如CART、随机森林和梯度提升
尝试SVM和kNN等算法
尝试其它的神经网络模型，比如LVQ、MLP、CNN、LSTM等等
采纳效果较好的几种方法，然后精细调解参数和数据来进一步提升效果。 

将你所选用的深度学习方法与上述这些方法比较，看看是否能击败他们？ 
也许你可以放弃深度学习模型转而选择更简单模型，训练的速度也会更快，而且模型易于理解。 
相关阅读：

一种数据驱动的机器学习方法
面对机器学习问题为何需要筛选算法
用scikit-learn筛选机器学习的分类算法

2） 从文献中学习
从文献中“窃取”思路是一条捷径。 
其它人是否已经做过和你类似的问题，他们使用的是什么方法。 
阅读论文、书籍、问答网站、教程以及Google给你提供的一切信息。 
记下所有的思路，然后沿着这些方向继续探索。 
这并不是重复研究，这是帮助你发现新的思路。

优先选择已经发表的论文 
已经有许许多多的聪明人写下了很多有意思的事情。利用好这宝贵的资源吧。 
相关阅读：

如何研究一种机器学习算法
Google学术

3） 重采样的方法
你必须明白自己模型的效果如何。 
你估计的模型效果是否可靠呢？ 
深度学习模型的训练速度很慢。 
这就意味着我们不能用标准的黄金法则来评判模型的效果，比如k折交叉验证。

也许你只是简单地把数据分为训练集和测试集。如果是这样，就需要保证切分后的数据分布保持不变。单变量统计和数据可视化是不错的方法。
也许你们可以扩展硬件来提升效果。举个例子，如果你有一个集群或是AWS的账号，我们可以并行训练n个模型，然后选用它们的均值和方差来获取更稳定的效果。
也许你可以选择一部分数据做交叉验证（对于early stopping非常有效）。
也许你可以完全独立地保留一部分数据用于模型的验证。

另一方面，也可以让数据集变得更小，采用更强的重采样方法。


也许你会看到在采样后的数据集上训练得到的模型效果与在全体数据集上训练得到的效果有很强的相关性。那么，你就可以用小数据集进行模型的选择，然后把最终选定的方法应用于全体数据集上。
也许你可以任意限制数据集的规模，采样一部分数据，用它们完成所有的训练任务。
你必须对模型效果的预测有十足的把握。 

相关阅读：

用Keras评估深度学习模型的效果
用重采样的方法评估机器学习算法的效果

3. 从算法调优上提升性能
你通过算法筛选往往总能找出一到两个效果不错的算法。但想要达到这些算法的最佳状态需要耗费数日、数周甚至数月。 
下面是一些想法，在调参时能有助于提升算法的性能。

模型可诊断性
权重的初始化
学习率
激活函数
网络结构
batch和epoch
正则项
优化目标
提早结束训练

你可能需要指定参数来多次（3-10次甚至更多）训练模型，以得到预计效果最好的一组参数。对每个参数都要不断的尝试。 
有一篇关于超参数最优化的优质博客：

如何用Keras网格搜索深度学习模型的超参数

1） 可诊断性
只有知道为何模型的性能不再有提升了，才能达到最好的效果。 
是因为模型过拟合呢，还是欠拟合呢？ 
千万牢记这个问题。千万。 
模型总是处于这两种状态之间，只是程度不同罢了。 
一种快速查看模型性能的方法就是每一步计算模型在训练集和验证集上的表现，将结果绘制成图表。



在训练集和验证集上测试模型的准确率

如果训练集的效果好于验证集，说明可能存在过拟合的现象，试一试增加正则项
如果训练集和验证集的准确率都很低，说明可能存在欠拟合，你可以继续提升模型的能力，延长训练步骤。
如果训练集和验证集的曲线有一个焦点，可能需要用到early stopping的技巧了

经常绘制类似的图表，深入研究并比较不同的方法，以提高模型的性能。

这些图表也许是你最有价值的诊断工具。 
另一种有效的诊断方法是研究模型正确预测或是错误预测的样本。 
在某些场景下，这种方法能给你提供一些思路。

也许你需要更多的难预测的样本数据
也许你可以从训练集中删去那些容易被学习的样本
也许你可以有针对性地对不同类型的输入数据训练不同的模型

相关阅读：

用Keras展现深度学习模型的训练过程
机器学习算法的过拟合和欠拟合

2） 权重的初始化
有一条经验规则：用小的随机数初始化权重。 
事实上，这可能已经足够了。但是这是你网络模型的最佳选择吗？ 
不同的激活函数也可以有不同的应对策略，但我不记得在实践中存在什么显著的差异。 
保持你的模型结构不变，试一试不同的初始化策略。 
记住，权重值就是你模型需要训练的参数。几组不同的权重值都能取得不错的效果，但你想得到更好的效果。

尝试所有的初始化方法，找出最好的一组初始化值
试一试用非监督式方法预学习，比如自动编码机
尝试用一组现有的模型权重参数，然后重新训练输入和输出层（迁移学习）

记住，修改权重初始化值的方法与修改激活函数或者目标函数的效果相当。 
相关阅读：

深度网络模型的初始化

3） 学习率
调节学习率也能带来效果提升。 
这里也有一些探索的思路：

尝试非常大、非常小的学习率
根据参考文献，在常规值附近用网格化搜索
尝试使用逐步减小的学习率
尝试每隔固定训练步骤衰减的学习率
尝试增加一个向量值，然后用网格搜索

大的网络模型需要更多的训练步骤，反之亦然。如果你添加了更多的神经节点和网络层，请加大学习率。 
学习率与训练步骤、batch大小和优化方法都有耦合关系。

相关阅读：

使用Keras对深度学习模型进行学习率调节
反向传播算法该选用什么学习率？

4） 激活函数
也许你应该选用ReLU激活函数。 
仅仅因为它们的效果更好。 
在ReLU之前流行sigmoid和tanh，然后是输出层的softmax、线性和sigmoid函数。除此之外，我不建议尝试其它的选择。 
这三种函数都试一试，记得把输入数据归一化到它们的值域范围。 
显然，你需要根据输出内容的形式选择转移函数。 
比方说，将二值分类的sigmoid函数改为回归问题的线性函数，然后对输出值进行再处理。同时，可能需要调整合适的损失函数。在数据转换章节去寻找更多的思路吧。 
相关阅读：

为何使用激活函数？

5） 网络拓扑结构
调整网络的拓扑结构也会有一些帮助。 
你需要设计多少个节点，需要几层网络呢？ 
别打听了，鬼知道是多少。 
你必须自己找到一组合理的参数配置。

试一试加一层有许多节点的隐藏层（拓宽）
试一试一个深层的神经网络，每层节点较少（纵深）
尝试将上面两种组合
尝试模仿近期发表的问题类似的论文
尝试拓扑模式和书本上的经典技巧（参考下方的链接）
这是一个难题。越大的网络模型有越强的表达能力，也许你就需要这样一个。 
更多晨的结构提供了抽象特征的更多结构化组合的可能，也许你也需要这样一个网络。 
后期的网络模型需要更多的训练过程，需要不断地调节训练步长和学习率。 
相关阅读： 
下面的链接可能给你提供一些思路：

我的网络模型该设计几层呢？
我的网络模型该设计几个节点呢？

6） batch和epoch
batch的大小决定了梯度值，以及权重更新的频率。一个epoch指的是训练集的所有样本都参与了一轮训练，以batch为序。 
你尝试过不同的batch大小和epoch的次数吗？ 
在前文中，我们已经讨论了学习率、网络大小和epoch次数的关系。 
深度学习模型常用小的batch和大的epoch以及反复多次的训练。 
这或许对你的问题会有帮助。

尝试将batch大小设置为全体训练集的大小（batch learning）
尝试将batch大小设置为1（online learning）
用网格搜索尝试不同大小的mini-batch（8，16，32，…）
尝试再训练几轮epoch，然后继续训练很多轮epoch
尝试设置一个近似于无限大的epoch次数，然后快照一些中间结果，寻找效果最好的模型。 
有些模型结构对batch的大小很敏感。我觉得多层感知器对batch的大小很不敏感，而LSTM和CNN则非常敏感，但这都是仁者见仁。

相关阅读：

什么是批量学习、增量学习和在线学习？
直觉上，mini-batch的大小如何影响（随机）梯度下降的效果？

7） 正则项
正则化是克服训练数据过拟合的好方法。 
最近热门的正则化方法是dropout，你试过吗？ 
Dropout方法在训练过程中随机地略过一些神经节点，强制让同一层的其它节点接管。简单却有效的方法。

权重衰减来惩罚大的权重值
激活限制来惩罚大的激活函数值

尝试用各种惩罚措施和惩罚项进行实验，比如L1、L2和两者之和。 
相关阅读：

使用Keras对深度学习模型做dropout正则化
什么是权值衰减？

8） 优化方法和损失函数
以往主要的求解方法是随机梯度下降，然而现在有许许多多的优化器。 
你尝试过不同的优化策略吗？ 
随机梯度下降是默认的方法。先用它得到一个结果，然后调节不同的学习率、动量值进行优化。 
许多更高级的优化方法都用到更多的参数，结构更复杂，收敛速度更快。这取决于你的问题，各有利弊吧。 
为了压榨现有方法的更多潜力，你真的需要深入钻研每个参数，然后用网格搜索法测试不同的取值。过程很艰辛，很花时间，但值得去尝试。 
我发现更新/更流行的方法收敛速度更快，能够快速了解某个网络拓扑的潜力，例如：

ADAM
RMSprop

你也可以探索其它的优化算法，例如更传统的算法（Levenberg-Marquardt）和比较新的算法（基因算法）。其它方法能给SGD创造好的开端，便于后续调优。 
待优化的损失函数则与你需要解决的问题更相关。 
不过，也有一些常用的伎俩（比如回归问题常用MSE和MAE），换个损失函数有时也会带来意外收获。同样，这可能也与你输入数据的尺度以及所使用的激活函数相关。 
相关阅读：

梯度下降优化算法概览
什么是共轭梯度和Levenberg-Marquardt？
深度学习的优化方法，2011

9） Early Stopping
你可以在模型性能开始下降的时候停止训练。 
这帮我们节省了大量时间，也许因此就能使用更精细的重采样方法来评价模型了。 
early stopping也是防止数据过拟合的一种正则化方法，需要你在每轮训练结束后观察模型在训练集和验证集上的效果。 
一旦模型在验证集上的效果下降了，则可以停止训练。 
你也可以设置检查点，保存当时的状态，然后模型可以继续学习。 
相关阅读：

如何在Keras给深度学习模型设置check-point
什么是early stopping？

4. 用融合方法提升效果
你可以将多个模型的预测结果融合。 
继模型调优之后，这是另一个大的提升领域。 
事实上，往往将几个效果还可以的模型的预测结果融合，取得的效果要比多个精细调优的模型分别预测的效果好。 
我们来看一下模型融合的三个主要方向：

模型融合
视角融合
stacking

1） 模型融合
不必挑选出一个模型，而是将它们集成。 
如果你训练了多个深度学习模型，每一个的效果都不错，则将它们的预测结果取均值。 
模型的差异越大，效果越好。举个例子，你可以使用差异很大的网络拓扑和技巧。 
如果每个模型都独立且有效，那么集成后的结果效果更稳定。 
相反的，你也可以反过来做实验。 
每次训练网络模型时，都以不同的方式初始化，最后的权重也收敛到不同的值。多次重复这个过程生成多个网络模型，然后集成这些模型的预测结果。 
它们的预测结果会高度相关，但对于比较难预测的样本也许会有一点提升。 
相关阅读：

用scikit-learn集成机器学习算法
如何提升机器学习的效果

2） 视角融合
如上一节提到的，以不同的角度来训练模型，或是重新刻画问题。 
我们的目的还是得到有用的模型，但是方式不同（如不相关的预测结果）。 
你可以根据上文中提到的方法，对训练数据采取完全不同的缩放和变换技巧。 
所选用的变化方式和问题的刻画角度差异越大，效果提升的可能性也越大。 
简单地对预测结果取均值是一个不错的方式。

3）stacking
你还可以学习如何将各个模型的预测结果相融合。 
这被称作是stacked泛化，或者简称为stacking。 
通常，可以用简单的线性回归的方式学习各个模型预测值的权重。 
把各个模型预测结果取均值的方法作为baseline，用带权重的融合作为实验组。

Stacked Generalization (Stacking)

总结
各抒己见吧

补充资料
还有一些非常好的资料，但没有像本文这么全面。 
我在下面列举了一些资料和相关的文章，你感兴趣的话可以深入阅读。

神经网络常见问答
如何用网格搜索法求解深度学习模型的超参数
深度神经网络必知的技巧
如何提升深度神经网络的验证准确率？

如果你知道其它的好资源，欢迎留言。
          
          感谢 mengyidan1988 投递这篇资讯
                      声明：本文系ITeye网站发布的原创资讯，严禁任何网站转载本文，否则必将追究法律责任！
                    
          
          
          
            已有 0 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>43</docid>
	<title>TEDx漕河泾沙龙报名开启</title>
	<link>http://www.iteye.com/news/31868</link>
	<author></author>
	<content>
          
原力
生于识
补偿演化，盘涅再生
我们感知，而后彳亍而行

原力
连结过去与未来的起承流传
见证知识与技术的天作之合
甚至于，在艺术与文学的影子里
丈量着未知世界，如梦之梦

后来，有人拿起钥匙
将规则解密
有人似飞蛾扑火，却胜涅槃重生
跨越万水千山，从此不必远渡重洋
有人结下殊胜之缘，寻着前人轨迹
脚步错落笃定

在奇思妙想和能量场中
原力定义了怎样的真实？
在历史漫漫长路中
原力如何永燃灵感的火花？

且再问一次，你是谁？
生命来去，而
原力是你我，你我是原力









2016年10月25日（周二）13:00
TEDxCaohejingPark沙龙活动将在
上海漕河泾新兴技术开发区万丽酒店举行
活动分2场，中间休息60分钟，预计17:00结束

除非有特殊情况，我们希望您能参与整场活动。我们也期待您能在休息时与观众和演讲人互动。

TED观众非常开放多元，且拥有包容创新观点的心态，我们希望 TEDxCaohejingPark沙龙提供同样轻松的环境，每个人都乐于沟通交流。

当天的日程安排（暂定）

13:00～13:30  -   签到
13:30～15:00  -   第一场
15:00～15:50  -   中场休息
15:50～17:00  -   第二场

报名地址：http://www.bagevent.com/event/223002
          
          感谢 rnifeasy1 投递这篇资讯
                      声明：本文系ITeye网站发布的原创资讯，严禁任何网站转载本文，否则必将追究法律责任！
                    
          
          
          
            已有 0 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>44</docid>
	<title>谷歌发布神经机器翻译 翻译质量接近笔译人员</title>
	<link>http://www.iteye.com/news/31867</link>
	<author></author>
	<content>
          


据外媒报道，谷歌于昨日发布了网页版和移动版的谷歌翻译。在汉译英的过程中，会采用全新的神经机器翻译，而这个App每天要进行一千八百万次这样的翻译。此外，谷歌针对这个翻译系统的运作原理，发表了一篇学术论文。
早前，谷歌就曾表示它们在谷歌翻译中运用了神经网络技术，但只限于实时视觉翻译这个功能。前段时间，谷歌的高级员工Jeff Dean曾告诉VentureBeat，谷歌已经在尝试把越来越多的深度学习功能融入到谷歌翻译中。除此之外，谷歌的一位发言人在邮件中告诉VentureBeat，最新的神经机器翻译是他们努力研发深度学习功能的成果。
实际上，谷歌一直以来都在致力于将深度神经网络融入它旗下越来越多的应用软件中，其中包括Google Allo和Inbox by Gmail。这个功能可以帮助谷歌更快捷、更有效地处理它们的数据。
谷歌的神经机器翻译（GNMT）对八层长的短时记忆递归神经网络（LSTM-RNNs）依赖性很强。“通过层间残留联系可以加强梯度流。”谷歌科学家在论文中写道。在图像处理器的帮助下，神经网络一旦变得足够成熟，谷歌就可以靠它尚未发布的张量处理单元进行数据处理。
虽然神经机器翻译并不永远是最佳之选，但是谷歌的各种尝试显示，在某些情况下它还是有过人之处的。



“人们对这个翻译系统的评价显示，比起之前基于短语的翻译系统，在翻译多种语言时，神经学习翻译系统的错误率已经降低了60%，其中包括英法互译，英西互译以及英汉互译。附加实验的结果显示，翻译系统的质量将和笔译人员平均水准更加接近。”

在昨天发表的一篇博文中，Google Brain Team的研发科学家Quoc Le和Mike Schuster提到，有了双语评分员的帮助，在翻译Wikipedia上的多语种样句时，谷歌神经机器翻译的错误率实际上已经降低了55%到85%。
尽管如此，这个系统还是不完美的。“神经机器翻译还是会犯一些笔译人员永远都不可能犯的错误，比如漏了一些单词、把一些常见的名字或是少见的专有名词翻错、对文章语境缺乏整体把控等等。所以，我们还是有很大的进步空间。但不可否认的是，神经机器翻译具有里程碑意义。”

来自：网易科技
          
          感谢 mengyidan1988 投递这篇资讯
                    
          
          
          
            已有 1 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>45</docid>
	<title>Mozilla宣布停止 Firefox OS 的开发</title>
	<link>http://www.iteye.com/news/31866</link>
	<author></author>
	<content>
          Mozilla 去年底宣布终止智能手机实验，停止开发和销售 Firefox OS 智能手机，它在2016年7月底停止了Firefox OS的商业开发，如今则宣布终止所有 Firefox OS 相关工作。Firefox OS 是一个开源项目，现在它的维护将全部转交给社区。

Mozilla开发者称，为了快速演化和启用 Gecko引擎的新架构变化，它的Platform Engineering 团队需要从 mozilla-central移除与Firefox OS/B2G OS相关的所有代码，继续从事B2G OS开发的开源社区将需要fork Gecko，从一个独立的分支维护一个完整版的Gecko引擎。Mozilla称这是一个痛苦但必要的决定。




来自：cnBeta.COM
          
          感谢 mengyidan1988 投递这篇资讯
                    
          
          
          
            已有 1 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>46</docid>
	<title>Java 9 将推迟到2017年7月发布</title>
	<link>http://www.iteye.com/news/31865</link>
	<author></author>
	<content>
          本来以为在明年3月可以尝鲜的Java 9却要延迟了，据外媒报道，甲骨文宣布原定于2017年3月推出的Java 9将再延至2017年7月发布，主要原因是Java 9内置的模组化架构Jigsaw需要更长的时间来开发。



甲骨文Java平台的架构师Mark Reinhold在OpenJDK的邮件列表中提到了这个新的发布日期
引用
Despite this progress, at this point it's clear that Jigsaw needs more time. We recently received critical feedback that motivated a redesign of the module system's package-export feature [5], without which we'd have failed to achieve one of our main goals. There are, beyond that, still many open design issues [6], which will take time to work through. Looking at the release as a whole, the number of open bugs that are new in JDK 9 is quite a bit larger than it was at this point in JDK 8. The maintainers of many popular projects are now actively testing against the JDK 9 EA builds [7], but we'd like to see even more in order to be confident that potential issues have been found and reported. For these reasons I hereby propose a four-month extension of the JDK 9 schedule, moving the General Availability (GA) milestone to July 2017.

Mark Reinhold称，由于收到使用者反馈，需要重新设计模组化架构Jigsaw中的Package-Export功能，更重要的是Java 9中还存在大量安全漏洞和Bug需要修复。

据悉，模组化架构Jigsaw将成为Java 9的核心功能，甲骨文也证实将会在Java9中新增新命令行工具Jshell、支持HTTP/2与新增多项API等。另外于今年1月推出的JDK9早期试用版不再支持Java浏览器外挂。

本文来自：太平洋新闻和pixelstech.net
          
          感谢 mengyidan1988 投递这篇资讯
                    
          
          
          
            已有 10 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>47</docid>
	<title>微信小程序，大多数人都搞错的八个问题</title>
	<link>http://www.iteye.com/news/31864</link>
	<author></author>
	<content>
          引用
声明：本文为CSDN原创投稿文章。 
作者：王安，数字天堂DCloud公司创始人兼CEO 
责编：陈秋歌，关注微信开发等领域，寻求报道或者投稿请发邮件chenqg#csdn.net。研发心得、项目实战、前沿技术、外文翻译……，只要是技术干货，十分欢迎投稿至chenqg#csdn.net。人人都是主编，这里就是你的舞台。 
欢迎加入“微信开发技术”群，参与热点、难点技术交流。请加群主微信「Rachel_qg」,申请入群，务必注明「公司+职位」。

小程序目前被炒得沸沸扬扬，无数媒体和企业借机获取阅读流量。

这再次证明一点，微信想让什么火，真的就能让什么火。这种能力真是全中国再也没有人有了，政府也没有。

但四处传的消息很多是失真的，废话不说，先列出8个多数人都搞错的问题：

1.小程序是HTML5；
2.小程序是B/S的；
3.把M站改改就可以接入到小程序里；
4.小程序体验不佳；
5.小程序适合低频长尾应用；
6.小程序是新的Appstore；
7.小程序做不起来，需求不高；
8.小程序会做起来，但会和原生应用长期并存。
以上8个是很多人凭直觉得出的结论，但真正深度调研和思考后，发现直觉和真相差好远。
引用注：本文有技术、有商业，不懂技术的可以只看商业相关的。
1.小程序不是HTML5

小程序是微信全新定义的规范，是基于XML+JS的，不支持也不兼容HTML，兼容受限的部分CSS写法。



上图为一个小程序的代码目录，后缀名分别是wxml、wxss和js。

不过微信对wxml的全称定义也不是weixin xml，而是WeiXin Markup Language，很霸气的要自成体系感。自然wxss也是WeiXin Style Sheets喽。

因为很多人把xml念成叉妹儿，现在大家不要念错哦，不要念达不流叉妹儿，要念微信妹儿。

下面是一段wxml示例，相对于早期的XML，扩展了花括号模板的写法。



虽然是要求强闭合的XML，但if写法和标准的XML也不一样。

JS部分，小程序支持ECMAScript6，由于没有Web，自然DOM、Window这些都不能用，jQuery就更没啥关系了。

很多小白说JS不就是HTML的一部分吗？HTML是归W3C管的，JS是归ECMA国际管的。JS是无处不在的一种解释性脚本语言，除了浏览器里，还有运行在服务器上的Nodejs，运行在PC和Mac机的nodewebkit…

也就是说小程序是微信基于XML和JS定义的一套标记语言，全新的生态，一个轻OS。 
开发工具、UI框架也都是腾讯做的，过去Web上的三方工具、框架生态，完全没有用。

也就是小程序开发者，将会是一个独立的职业。

培训机构有的忙了，在这个全新生态下，一切都推到重来。

顺便也澄清一个误区，小程序和腾讯X5引擎也没关系。X5是QQ浏览器团队的，是基于HTML的，但小程序是微信团队自研的。

2.小程序不是B/S

微信宣传的一个重点，是触手可得，不用安装。

但小程序并不是B/S的在线页面，它是C/S架构的。

在wxml里，通过wx.request（类似ajax）或socket连接服务器。

很多人不明白C/S应用为什么也可以即点即用，不用安装。其实这不是微信的首创，首创是DCloud的流应用。只要是动态语言，加上合适的算法，就可以先下载部分程序并运行，然后边用边下，类似于流媒体。

别忘了微信也同时宣传了强大的离线能力，毕竟B/S的切屏体验太烂，C/S才能有更好的用户体验。

只是不知道微信对小程序的空间占用问题会怎么管理，反正微信以后肯定是越来越大。

3.M站不能改造成小程序

其实看明白前2点，自然就明白第3点了。一个基于HTML的、B/S的M站，跟小程序并无关系。

老板们可能认为M站或之前公众号里的Wap站简单改改就可以接入小程序，然后对工程师报的工期不可理解，此时工程师可以把此文转给老板看，小程序是相当于重新做了一个App，从开发、设计、测试、运维升级都是单独的一套。哦，你还得加个学习成本和风险，如此新的东西一次搞利索的可能性不大。

如果你之前有一个服务号的Wap站，你也不能放弃服务号而只做小程序，所以你的业务得多头维护。

这里有一个很大的问题，就是做一版小程序，ROI（投资回报）是不是正的？新开一条产品线并长期运维，代价绝对不菲，对应的用户流量够大吗？ARPU值（单位用户价值）够高吗？能收回投资吗？这个问题有点大，下面单说。

4.小程序的体验不如原生应用？反了

我知道这个观点有点违反大多数人的直觉。但世界在变。

用户的使用体验是由很多要素作用影响的，过去我们说的体验主要是进入应用后的操作流畅度。关于这个维度，很多观点是：小程序嘛，操作流畅度基于Web和原生App之间。

其实随着手机硬件和网络的发展，两年内上市的手机上，包括几百元的低端机，小程序的操作流畅度和原生的差距，用户是感受不到的。其实在小程序之前，HTML5+和React Native也都早做到了这一点。

而且你也可以反过来想想，小程序作为微信在移动互联网下半场最重要的战略，如果操作流畅度不好，张小龙会出来站台还批准它上线？

除了操作流畅度，用户体验还受很多环节影响。

有个在其他开发者生态不常见的事情，就是微信出了UI框架并很强调各个小程序的体验一致性，它希望用户在微信里使用各个小程序时，UI基本一致，用户不会觉得乱，不需要学习和适应。

然后微信还有统一的账户登陆体系，再加上小程序本身触手可得的特点，那么想象下用户的综合感受：不停使用各种触手可得、用完即走、UI体验一致、操作流畅、不用登陆注册的小程序们，还是挺爽的，这种体验比用原生应用还爽。

当时代发展到App的操作体验已经无法再有明显提升后，用户体验痛点已经从简单的在App里操作的流畅度转向其他地方了。如果还抱着旧观念不放，只能被转换思路的竞品超越。

5.小程序并非只适合低频或长尾应用

iPhone刚出Appstore时，确实首先活跃在其中的都是创业者，但最后所有人都卷进来了。

小程序的设计目标可是大生态，没有定位于只吸纳低频长尾应用。所有人都被卷进来是迟早的事。

有人觉得入口太深会导致高频应用仍然留在桌面，但入口的深浅是会演化的，如果微信感觉到目前的入口深度影响了用户便利性，它立即会提高，它会在桌面创建快捷方式。如果某天微信出了独立的桌面laucher或小程序入口App，也不要诧异。下一代的微信，核心已经不是微信这个通信App，核心已经是庞大的小程序生态了，从哪个入口进入就不重要了。哦对了，如果腾讯委托代工出了不能装原生应用的手机也不要诧异，哦，如果支持iPhone用户0元换机也不要诧异。

跑题了，不过当前的小程序生态发展也确实存在先后关系，低频的比高频的先进来、光脚的比穿鞋的先进来。

原生App体量已经很大的玩家，就是穿鞋的，他们希望用户持续保留他们的原生App在桌面。

在移动互联网上半场，有个游戏规则：因为App偏重，用户手机里一个品类一般只装一个App，那么烧钱买到最多流量的，就是最后的赢家，而竞品想把用户再抢过来就非常难了。

上半场烧了无数钱剩下的大玩家们，面对游戏规则的改变想必心情是极其复杂的。 
可是复杂也没办法，腾讯旗下亲儿子、干儿子占据移动互联网大半壁江山，小程序里，刚开始就会有大玩家，其他竞争对手跟不跟呢，心情复杂也得跟啊。

当然最尴尬的还是阿里、百度们，看着别人抽你血却想不出办法。

6.小程序不是应用商店，是OS

很多人说微信想做应用商店，只能说太小瞧微信了，人家明明要做操作系统嘛。 
2015年张小龙就在筹划应用号了，闭门搞了一年多怎么会是个应用商店这种小生意。应用商店只是OS生态里的一个发行环节，而微信恰恰不打算挣发行的钱。

国内的应用商店，都是中心化的流量分发，做的是卖流量的生意。用户看到的，不是最好的，而是出价最高的。不止是应用商店，百度现在也是这个问题，顺便吐槽下百度的搜索第三定律：出价最高者，对自己的网站最有自信，也就是对最终用户最有价值。

微信显然不是这样的思路，微信的去中心化，其实是一种保证良品得到更大流量的机制，这种机制非常有利于生态的起步。

一个小程序，没有下载刷榜，微信会采集用户的实际使用量、反复使用率、新增传播趋势，给你推荐最好的。

不扯发行的事了。小程序确实是按OS标准打造的，开发语言、IDE都是自成体系。Facebook推出React Native时也没做IDE，并且全部开源，但微信的野心不一样，真的是从头到尾要建设和控制生态，并且是封闭生态，就像iOS的生态一样。甚至小程序的开发者，每年也要像iOS的开发者一样交几百元年费。

有人会觉得手机原生OS才是老大，这种二级生态搞不起来。其实在PC上，Windows上的互联网就是二级生态，做的比Windows原生生态大，当初Flash算是三级生态了，如果不是HTML5打压，也做的非常大了。但也确实存在一个问题，Apple是极其封闭的，在Apple上二级生态想做很大是非常难的，而Android就不一样了，尤其是中国的Android，不像国外被Google控制着，在国外若有人乱搞，Google Play可以将其下架，但在中国就谁都拿微信没招了。

说到这里还是真诚感谢中国政府的，你的英明决策，给中国互联网创造了一个引领全球的机会。在国外，Apple和Google是既得利益者，还没人撼得动，但中国出现了这样的机会，移动互联网下半场，中国终于要开始引领全球了！

如果在Apple下架微信时，中国政府能够以反垄断法之类的名义杀下Apple，那就是又帮了大忙了（微信是不是应该给我付咨询费啊）。

7.小程序的需求不强烈？很多未来，不体验过自己是想不到的

福特说，你问用户要什么交通工具，用户会说他想要一匹更快的马，没人会认为汽车是未来。

iPhone发布时，Nokia和黑莓嘲笑说：全触摸屏不靠谱，按键是最佳用户体验。然后临死还说：我们没做错什么，不知道为什么会这样。

那么小程序的核心理念：“触手可得”，到底是不是用户需求，是不是未来方向？ 
Of cause！这跟电容触摸屏是一样的道理，直击人性底层，能让你的大脑皮层产生爽的感觉。

所有让人爽的东西，都是发展方向。而一旦一个有影响力的巨头去做，就成了不可逆转的前进方向。

微信，是一个尤其深谙人性、擅长让人爽的产品，你一旦爽过了，就退不回去了。 
想象下一切触手可得的上帝感，手机里全是可以放心的、高品质的、无需适应UI的、不用登陆注册的服务，触手可得、用完即走，太爽了。然后你就会变成它的拥护者，以证明自己更酷。有人在用原生应用，你就会告诉他：hi，你不知道小程序吗？怎么还在用那么笨重的大程序啊。如果有个开发商的应用只有原生版，你就会骂：这是什么老土厂商，竟然还让我下载安装，不用了！

8.小程序和原生应用将长期并存？原生必将没落

小程序生态是原生生态的下一代替代品，是时代的进化产物。Windows原生应用当然也不会消失，但大部分服务都在互联网上了。小程序看中的也是这块，邮件客户端什么的就不要在小程序里用了，但你目前在PC浏览器里使用的各种服务，全部会纳入小程序的生态范围。

小程序出来后，原生生态和其对抗是让历史倒退，没有意义。就像微信冲击运营商的短信业务时，电信运营商的对抗也是无意义的。

对抗小程序的，不会是原生生态，只能是其他触手可及的类小程序生态。

微信这条大鲶鱼进来后，Apple、Google等众多巨头都会有反应，他们不是没技术，只是原来躺着挣钱很舒服，不愿意革自己命，但多久会落实行动也难讲，毕竟只是为了中国市场而革全世界市场的命，好像不太划算。

有人觉得革掉原生应用的命，过程会很长，其实这个进程的进度是由微信控制的。 
微信做的越激进，引发的旧利益既得者的反弹会越严重；但温水煮青蛙的杀原生生态，又存在用户习惯培养的成功把握度不高的问题。

小程序毕竟是全新事物，培养C端用户习惯是需要引爆点的，微信自然不缺引爆能力。

当初要做游戏，开屏就是打飞机，然后全民都在打飞机。

当初要做支付，推出群红包，然后全民都在抢红包，瞬间颠覆支付宝的地位。

11月小程序将面向手机用户开放，现在引爆设计肯定已经做了不止一稿，比如开屏就推一个很好玩的小程序，造成全民都用小程序的热潮，然后所有观望的开发商全都会跳下海。再然后微信持续推出比一比手机清洁度在好友中的排名之类的玩意，持续给优质小程序和用户需求做对接，过半年就没人下载原生应用了，再过半年原来装的原生应用也大多卸了。

那些被颠覆的人能怎样呢？就像马云在微信红包出来后虽然嚷嚷这是偷袭珍珠港，可你又没有原子弹打回去啊。事实上小程序不是偷袭珍珠港，这是降维打击，就像三体人留下那句话：毁灭你，与你何干。

8个常见误区解释了，再简单说下小程序里的机会和威胁。

小程序的创业机会在于：
1.人才市场，因为小程序工程师将有一个从0开始高速增长的红利，其中的线上线下培训、招聘、书籍出版等相关领域会享受一次红利。
2.如果你是个光脚的创业者，那么就豁出去认真做小程序，抛弃原生，从微信用户的特点出发重新设计产品，做轻、做服务化，一定要用完即走，不要考虑次日留存，当时用爽最重要。
3.充分利用社交特点，做社交类服务，比如多人互动、上下游联动。小程序是可以发到群里的、朋友圈里的，想想群红包这种社交产品，想象空间很大。
4.充分发挥线下优势，做线下商家服务。线下扫码入口，基本就是微信的天下。
5.由于阿里、百度等一些巨头不会入驻小程序，留下一片空间给创业者。
6.其实被我公开说了的机会，可能就不是机会了…

小程序的威胁：

1.如果你在原生App下已经是既得利益者，那么你做小程序的话，ROI高概率不划算，因为新开一个产品，从产品、开发、测试、持续运维升级，资源消耗太大了，由于短期内小程序的体量和你已经拥有的原生体量还差不少，分出精兵强将干这事会让你很郁闷。如果你不担心小程序创业潮里的新兵蛋子干翻你，你就暂时不用管小程序这事了。否则，ROI为负你也得搞。
2.原生生态下生存的很多公司的商业模式会受到很大冲击，安卓应用市场、原生应用广告产业链、原生工程师培训，都会受影响。篇幅有限，想要咨询如何减少冲击，可以私聊我微博weibo.com/wangan2000。
3.哦，通篇没说，虽然浏览器不是原生生态产物，但也会被架空。标准浏览器的体验无法与小程序抗衡。就是小程序这个生态如果占据了用户心智和时间，原生和浏览器生态都会受冲击。
4.就个人职业而言，原生开发、UI设计、流量运营这些领域的人的失业率会增加。微信有统一的UI要求，然后UI设计师就躺枪了。获取流量的玩法，也和原生不一样了，没法买买买了，不过移动互联网进入下半场后，很多公司的流量采购部门本身也在裁员。很多人还怀疑Android和iOS的开发工程师会不会真的失业，你觉得iPhone出来后，symbian工程师会不会失业呢？虽然当年也确实有很多人认为不会失业吧。

最后，作为一个技术人员，吐槽下：我不喜欢封闭！

虽然然并卵吧。

哦，对了，最后再提醒下被小程序冲击的人，你们的不爽不要来喷我，又不是我要冲击你们，提醒你们是好事。
          
          感谢 mengyidan1988 投递这篇资讯
                      声明：本文系ITeye网站发布的原创资讯，严禁任何网站转载本文，否则必将追究法律责任！
                    
          
          
          
            已有 6 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>48</docid>
	<title>详解微信小程序开发教程</title>
	<link>http://www.iteye.com/news/31862</link>
	<author></author>
	<content>
          微信应用开放的服务和组件包含如下：

视图容器：视图(View)、滚动视图、Swiper
基础内容：图标、文本、进度条
表单组件：按钮、表单等等
操作反馈
导航
媒体组建：音频、图片、视频。
地图
画布
文件操作能力
网络：上传下载能力、WebSocket
数据：数据缓存能力
位置：获取位置、查看位置
设备：网络状态、系统信息、重力感应、罗盘
界面：设置导航条、导航、动画、绘图等等
开放接口：登录，包括签名加密，用户信息、微信支付、模板消息

审核：
根据《微信小程序平台服务协议》，里面有关描述如下：

2.4 为确保微信小程序平台、微信公众平台、其他用户等各方的安全、稳定及良好的用户体验，腾讯将对需要发布的小程序进行发布审核。

“发布审核”是指由用户发起，将其完成初始化开发的小程序提交至腾讯，由腾讯自行或委托第三方对该小程序的合法性、合理性、安全性、稳定性、可操作性、用户体验等各方面，采用包括但不限于开发信息核对、安全测试、UI测试、随机测试、动态测试、安全测试等方式，进行审查、甄别、试验与评估的过程。发布审核结果包括审核通过与审核不通过两种。审核不通过的，该小程序将无法发布。

之后小程序的审核极有可能采取和App store类似的策略，但相比微信其他平台的审核，各位严格和复杂。

教程：
微信应用号（小程序，「应用号」的新称呼）终于来了！目前还处于内测阶段，微信只邀请了部分企业参与封测。想必大家都关心应用号的最终形态到底是什么样子？怎样将一个「服务号」改造成为「小程序」？我们暂时以一款简单的第三方工具的实例，来演示一下开发过程吧。

OK，为了让大家尽快看到这份教程，博卡君注定要熬夜了！今晚开始更新，希望明天一早就能发布第一篇教程！记录开始！看看几天能完成变身吧！

序言

开始开发应用号之前，先看看官方公布的「小程序」教程吧！（以下内容来自微信官方公布的「小程序」开发指南）

本文档将带你一步步创建完成一个微信小程序，并可以在手机上体验该小程序的实际效果。这个小程序的首页将会显示欢迎语以及当前用户的微信头像，点击头像，可以在新开的页面中查看当前小程序的启动日志。

1. 获取微信小程序的 AppID

首先，我们需要拥有一个帐号，如果你能看到该文档，我们应当已经邀请并为你创建好一个帐号。注意不可直接使用服务号或订阅号的 AppID。 利用提供的帐号，登录 https://mp.weixin.qq.com，就可以在网站的「设置」-「开发者设置」中，查看到微信小程序的 AppID 了。



注意：如果我们不是用注册时绑定的管理员微信号，在手机上体验该小程序。那么我们还需要操作「绑定开发者」。即在「用户身份-开发者」模块，绑定上需要体验该小程序的微信号。本教程默认注册帐号、体验都是使用管理员微信号。

2.创建项目

我们需要通过开发者工具，来完成小程序创建和代码编辑。

开发者工具安装完成后，打开并使用微信扫码登录。选择创建「项目」，填入上文获取到的 AppID，设置一个本地项目的名称（非小程序名称），比如「我的第一个项目」，并选择一个本地的文件夹作为代码存储的目录，点击「新建项目」就可以了。

为方便初学者了解微信小程序的基本代码结构，在创建过程中，如果选择的本地文件夹是个空文件夹，开发者工具会提示，是否需要创建一个 quick start 项目。选择「是」，开发者工具会帮助我们在开发目录里生成一个简单的 demo。



项目创建成功后，我们就可以点击该项目，进入并看到完整的开发者工具界面，点击左侧导航，在「编辑」里可以查看和编辑我们的代码，在「调试」里可以测试代码并模拟小程序在微信客户端效果，在「项目」里可以发送到手机里预览实际效果。

3. 编写代码

点击开发者工具左侧导航的「编辑」，我们可以看到这个项目，已经初始化并包含了一些简单的代码文件。最关键也是必不可少的，是 app.js、app.json、app.wxss 这三个。其中，.js 后缀的是脚本文件，.json 后缀的文件是配置文件，.wxss 后缀的是样式表文件。微信小程序会读取这些文件，并生成小程序实例。

下面我们简单了解这三个文件的功能，方便修改以及从头开发自己的微信小程序。

app.js 是小程序的脚本代码。我们可以在这个文件中监听并处理小程序的生命周期函数、声明全局变量。调用 MINA 提供的丰富的 API，如本例的同步存储及同步读取本地数据。

//app.js
App({
  onLaunch: function () {
    //调用API从本地缓存中获取数据
    var logs = wx.getStorageSync('logs') || []
    logs.unshift(Date.now())
    wx.setStorageSync('logs', logs)
  },
  getUserInfo:function(cb){
    var that = this;
    if(this.globalData.userInfo){
      typeof cb == "function" && cb(this.globalData.userInfo)
    }else{
      //调用登录接口
      wx.login({
        success: function () {
          wx.getUserInfo({
            success: function (res) {
              that.globalData.userInfo = res.userInfo;
              typeof cb == "function" && cb(that.globalData.userInfo)
            }
          })
        }
      });
    }
  },
  globalData:{
    userInfo:null
  }
})

app.json 是对整个小程序的全局配置。我们可以在这个文件中配置小程序是由哪些页面组成，配置小程序的窗口 背景色，配置导航条样式，配置默认标题。注意该文件不可添加任何注释。

{
  "pages":[
    "pages/index/index",
    "pages/logs/logs"
  ],
  "window":{
    "backgroundTextStyle":"light",
    "navigationBarBackgroundColor": "#fff",
    "navigationBarTitleText": "WeChat",
    "navigationBarTextStyle":"black"
  }
}

app.wxss 是整个小程序的公共样式表。我们可以在页面组件的class属性上直接使用app.wxss中声明的样式规则。

/**app.wxss**/
.container {
  height: 100%;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: space-between;
  padding: 200rpx 0;
  box-sizing: border-box;
}

4.创建页面

在这个教程里，我们有两个页面，index 页面和 logs 页面，即欢迎页和小程序启动日志的展示页，他们都在 pages 目录下。微信小程序中的每一个页面的【路径+页面名】都需要写在 app.json 的 pages 中，且 pages 中的第一个页面是小程序的首页。

每一个小程序页面是由同路径下同名的四个不同后缀文件的组成，如：index.js、index.wxml、index.wxss、index.json。.js 后缀的文件是脚本文件，.json 后缀的文件是配置文件，.wxss 后缀的是样式表文件，.wxml 后缀的文件是页面结构文件。

index.wxml是页面的结构文件：

<view class="container">
  <view  bindtap="bindViewTap" class="userinfo">
    <image class="userinfo-avatar" src="{{userInfo.avatarUrl}}" background-size="cover"></image>
    <text class="userinfo-nickname">{{userInfo.nickName}}</text>
  </view>
  <view class="usermotto">
    <text class="user-motto">{{motto}}</text>
  </view>

本例中使用了 、、来搭建页面结构，绑定数据和交互处理函数。

index.js是页面的脚本文件，在这个文件中我们可以监听并处理页面的生命周期函数、获取小程序实例，声明并处理数据，响应页面交互事件等。

//index.js
//获取应用实例
var app = getApp()
Page({
  data: {
    motto: 'Hello World',
    userInfo: {}
  },
  //事件处理函数
  bindViewTap: function() {
    wx.navigateTo({
      url: '../logs/logs'
    })
  },
  onLoad: function () {
    console.log('onLoad')
    var that = this
    //调用应用实例的方法获取全局数据
   app.getUserInfo(function(userInfo){
      //更新数据
      that.setData({
        userInfo:userInfo
      })
    })
  }
})
index.wxss是页面的样式表：

/**index.wxss**/
.userinfo {
  display: flex;
  flex-direction: column;
  align-items: center;
}
.userinfo-avatar {
  width: 128rpx;
  height: 128rpx;
  margin: 20rpx;
  border-radius: 50%;
}
.userinfo-nickname {
  color: #aaa;
}
.usermotto {
  margin-top: 200px;
}

页面的样式表是非必要的。当有页面样式表时，页面的样式表中的样式规则会层叠覆盖 app.wxss 中的样式规则。如果不指定页面的样式表，也可以在页面的结构文件中直接使用 app.wxss 中指定的样式规则。

index.json是页面的配置文件：

页面的配置文件是非必要的。当有页面的配置文件时，配置项在该页面会覆盖 app.json 的 window 中相同的配置项。如果没有指定的页面配置文件，则在该页面直接使用 app.json 中的默认配置。

logs的页面结构

<!--logs.wxml-->
<view class="container log-list">
  <block wx:for-items="{{logs}}" wx:for-item="log">
    <text class="log-item">{{index + 1}}. {{log}}</text>
  </block>
</view>

logs 页面使用 控制标签来组织代码，在 上使用 wx:for-items 绑定 logs 数据，并将 logs 数据循环展开节点

//logs.js
var util = require('../../utils/util.js')
Page({
  data: {
    logs: []
  },
  onLoad: function () {
    this.setData({
      logs: (wx.getStorageSync('logs') || []).map(function (log) {
        return util.formatTime(new Date(log))
      })
    })
  }
})

运行结果如下：



5.手机预览

开发者工具左侧菜单栏选择「项目」，点击「预览」，扫码后即可在微信客户端中体验。



目前，预览和上传功能尚无法实现，需要等待微信官方的下一步更新。

如你所见，微信官方给出的开发指南还非常简单，很多细节、代码和功能都没有明确的展示，所以接下来就到博卡君展示实力的时候啦！开发教程正式开始！

第一章：准备工作
做好准备工作很重要。开发一个微信应用号，你需要提前到微信的官方网站（weixin.qq.com）下载开发者工具。

1.下载最新微信开发者工具，打开后你会看到该界面：



2. 点击「新建 web+」项目，随后出现如下画面：



3. 该页面内的各项内容需要注意——


AppID：依照官方解释来填。
Appname: 项目最外层文件夹名称，如你将其命名为「ABC」，则之后的全部项目内容均将保存在「/ABC/…」目录下。
本地开发目录：项目存放在本地的目录。
注：再次强调，如果你和团队成员共同开发该项目，则建议你们使用同样的目录名称及本地目录，以确保协同开发的统一性。如果你之前已有项目，则导入过程与以上内容近似，不再赘述。


4. 准备工作全部完成后，点击「新建项目」按钮，弹出框点「确定」



5. 如上图所示，此刻，微信开发者工具已经为你自动构建了一个初始的demo项目，该项目内包含了一个微信应用项目所需具备的基本内容和框架结构。点击项目名称（图中即「cards」）进入该项目，就能看到整个项目的基本架构了：



第二章：项目构架
微信目前用户群体非常庞大，微信推出公众号以后，火爆程度大家都看得到，也同样推动着 Html 5 的高速发展，随着公众号业务的需求越来越复杂，应用号现在的到来也是恰到好处。

博卡君发现，微信提供给开发者的方式也在发生全面的改变：从操作 DOM 转为操作数据，基于微信提供的一个过桥工具实现很多 Html 5 在公众号很难实现的功能，有点类似于 hybrid 开发，不同于 hybrid 开发的方式是：微信开放的接口更为严谨，结构必须采用他提供给的组件，外部的框架和插件都不能在这里使用上，让开发者完全脱离操作 DOM，开发思想转变很大。

工欲善其事，必先利其器。理解它的核心功能非常重要，先了解它的整个运作流程。

生命周期：

在index.js里面：



开发者工具上 Console 可以看到：



在首页 console 可以看出顺序是 App Launch–>App Show–>onLoad–>onShow–>onReady。

首先是整个 app 的启动与显示，app 的启动在 app.js 里面可以配置，其次再进入到各个页面的加载显示等等。可以想象到这里可以处理很多东西了，如加载框之类的都可以实现等等。

路由：

路由在项目开发中一直是个核心点，在这里其实微信对路由的介绍很少，可见微信在路由方面经过很好的封装，也提供三个跳转方法。

wx.navigateTo(OBJECT)：保留当前页面，跳转到应用内的某个页面，使用wx.navigateBack可以返回到原页面。

wx.redirectTo(OBJECT)：关闭当前页面，跳转到应用内的某个页面。

wx.navigateBack()：关闭当前页面，回退前一页面。

这三个基本上使用足够，在路由方面微信封装的很好，开发者根本不用去配置路由，往往很多框架在路由方面配置很繁琐。

组件：

此次微信在组件提供方面也是非常全面，基本上满足项目需求，故而开发速度非常快，开发前可以认真浏览几次，开发效率会很好。

其它：

任何外部框架以及插件基本上无法使用，就算原生的 js 插件也很难使用，因为以前的 js 插件也基本上全部是一操作 dom 的形式存在，而微信应用号此次的架构是不允许操作任何 dom，就连以前开发者们习惯使用的动态设置的rem.js也是不支持的。

此次微信还提供了 WebSocket，就可以直接利用它做聊天，可以开发的空间非常大。

跟公众号对比博卡君发现，开发应用号组件化，结构化，多样化。新大陆总是充满着惊喜，更多的彩蛋等着大家来发现。

更多信息请点击原文链接：http://xituqu.com/508.html
          
          感谢 mengyidan1988 投递这篇资讯
                    
          
          
          
            已有 1 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>49</docid>
	<title>百度正式宣布推出深度学习开源平台PaddlePaddle</title>
	<link>http://www.iteye.com/news/31863</link>
	<author></author>
	<content>
          百度近日正式对外宣布开放其深度学习开源平台PaddlePaddle，这也是继Google、Facebook、IBM后又一家将人工智能技术开源的公司。



PaddlePaddle的前身是百度于2013年自主研发的深度学习平台Paddle（Parallel Distributed Deep Learning，并行分布式深度学习），且一直为百度内部工程师研发使用。

PaddlePaddle在深度学习框架方面，覆盖了搜索、图像识别、语音语义识别理解、情感分析、机器翻译、用户画像推荐等多领域的业务和技术。今年9月1日百度世界大会上，百度首席科学家Andrew Ng（吴恩达）首次宣布将百度深度学习平台对外开放，命名PaddlePaddle。

目前，PaddlePaddle已实现CPU/GPU单机和分布式模式，同时支持海量数据训练、数百台机器并行运算，以应对大规模的数据训练。此外，PaddlePaddle具备高质量GPU代码，提供了Neural Machine Translation、推荐、图像分类、情感分析、Semantic Role Labelling等5个Task，每个Task都可迅速上手，且大部分任务可直接套用。



PaddlePaddle研发负责人徐伟
百度资深科学家、PaddlePaddle研发负责人徐伟介绍：“在PaddlePaddle的帮助下，深度学习模型的设计如同编写伪代码一样容易，设计师只需关注模型的高层结构，而无需担心任何琐碎的底层问题。未来，程序员可以快速应用深度学习模型来解决医疗、金融等实际问题，让人工智能发挥出最大作用。”

目前，PaddlePaddle已在百度30多项主要产品和服务之中发挥着作用，如外卖的预估出餐时间、预判网盘故障时间点、精准推荐用户所需信息、海量图像识别分类、字符识别（OCR）、病毒和垃圾信息检测、机器翻译和自动驾驶等领域。以外卖行业为例，外卖员等待商家出餐的时间耗时严重，百度将不同时段商家的客流量、菜品的制作时间和订单量等数据交给了PaddlePaddle，经过对海量数据的深度学习处理，如今，百度外卖的内部系统可以预估每个商家菜品出餐时间，及时告知外卖员，提高了送餐效率，系统也可以更加合理地规划取餐和送餐的路线。

早在今年1月份，百度便将可让人工智能软件运行更高效的关键代码Warp-CTC开源。该代码可直接用在当前的人工智能框架中，不仅比传统CTC训练速度快百倍，还大幅降低了对硬件的要求，缩短了研发的周期，让端到端的深度学习变得简单、快速。

据百度方面透露，9月底百度还将发布PaddlePaddle的新版本，将全面支持Mac操作系统、以及Cuda8.0和GCC5.4的支持，同时进一步优化了安装过程，帮助更多开放者更好地“上手”。

公开资料显示，谷歌于去年11月发布并开源了新的机器学习平台TensorFlow，供相关技术人员进行语音识别或照片识别等多项机器深度学习领域的研究；IBM几乎同步采取了行动，于去年11月底宣布通过Apache软件基金会免费为外部程序员提供System ML人工智能工具的源代码；社交巨头Facebook，继去年1月公开多款深度学习人工智能工具后，又于去年12月开源了Big Sur人工智能硬件架构。（李根）

来自：新浪科技
          
          感谢 mengyidan1988 投递这篇资讯
                    
          
          
          
            已有 4 人发表留言，猛击->>这里<<-参与讨论
          
          
ITeye推荐

—软件人才免语言低担保 赴美带薪读研！— 

          </content>
</doc>
<doc>
	<docid>50</docid>
	<title>Google 发布了一款办公神器，剑指微软的 Surface Hub</title>
	<link>http://www.ifanr.com/737451?utm_source=rss&utm_medium=rss&utm_campaign=</link>
	<author></author>
	<content>
在微软的 Surface 系列产品线中，除了面向个人消费者的 Surface Pro 系列和 Surface Book 笔记本，还有一款专门面向企业用户的巨屏设备 Surface Hub；其典型的使用场景就是会议室办公协作。

图自：microsoft
如今，Google 也推出了一款与 Surface Hub 功能相似的产品，名为 Jamboard。

图自：Google
基于云端的 Jamboard
在 Google 官方描述中，Jamboard 是一款用来协作的数码白板（digital whiteboard）,它能够让团队成员实时分享和合作完成创意；更重要的是，按照 Google 的一贯思维，Jamboard 同样也是基于云端服务的。
在具体的配置上，Jamboard 搭载了一块 55 英寸的可触控 4K 分辨率屏幕，拥有高清摄像头、扬声器和 WiFi 连接功能。为了让 Jamboard 能够在更多场合下使用，Google 还为它设计了一个配有轮子的底盘，可以方便用户移动。
软件方面，它所搭载的是经过特殊定制的 Android 系统。由于是基于云端，Jamboard 可以实现不同成员在不同设备之间的同步操作，而操作的结果在可以 Jamboard 上同步显示；不仅如此，它还可以实现基于网络的远程协作。

图自：YouTube
Jamboard 还与 Google 的 G Suite 套件实现了深度整合。G Suite 套件中集合了一系列办公专用的 app，可以更加方便地实现办公功能。举个例子，在会议中，用户可以直接将文档从 Drive 中拖动到 Jamboard 上，非常方便。
另外，为了让 Jamboard 更好地运作，Google 为它专门提供了两个附件：一个可以精准操作的电子手写笔和一个电子橡皮擦。这两个附件都不需要充电，当用户使用它们在 Jamboard 上操作时，Jamboard 能够识别并给予相应的反馈。

电子手写笔和电子橡皮擦
不仅如此，Jamboard 还能自动识别用户用手指做出的擦拭动作。
按照 Google 官方的说法，Jamboard 的售价会低于 6000 美元，将会在明年开卖。作为对比，微软推出的 55 英寸版 Surface Hub 售价为 8999 美元。
G Suite 的硬件外壳
虽然 Google 已经声明 Jamboard 的售价不高于 6000 美元，但它的定位非常明显，就是面向企业级用户，而与个人消费者没有太大的关系。
另外值得注意的是，在 Google 官方博客宣布将要推出 Jamboard 的，正是 Google 方面负责 G Suite 的产品经理 TJ Varghese。
实际上，Jamboard 与 G Suite 之间的关系无比密切。

图自：slickmedia
G Suite 成立于不到一个月之前。今年 9 月 30 日，Google 在官网宣布，将在原有的企业办公应用（Google Apps for Work）的基础之上，组建一个全新的企业办公服务，并命名为 G Suite。
G Suite 包含了一系列可以被应用于办公领域的服务，其中有 Gmail，Docs，Slides，Drive，Calendar，Hangouts 等。它们之所以被整合在一起，一方面是品牌方面的统一，但另一方面可以说是 Google 在向企业办公市场的一次重大迈进。

图自：networkedindia
而 G Suite 的直接对手，恰恰是微软的 Office、OneDive、Skype for Work 等一系列办公应用。
值得注意的是，G Suite 中不仅仅加入了 Google 原有的应用，还集合了 Google 包括自然语言处理、图像识别、智能分析在内的多项人工技能技术。
不过，G Suite 虽然优秀，但它毕竟是不存在实体的服务；从功能实现的角度，它还缺乏一个适用于办公场景的硬件承载者。而 Jamboard 的出现，恰恰扮演了这个角色。
因此可以说，Jamboard 是 G Suite 的硬件外壳，也可以说是 Google 迈向企业办公市场的另外一个重大决策。

图自：Google
另外，鉴于 Google 的 Chromebook 在教育市场已经有所积累，其与 Jamboard 同样也可以在教育办公领域形成犄角之势。这也可以认为是 Google 的另一个布局。

图自：Google
正面对抗 Surface Hub
简单来说，Jamboard 的出现，是 Google 在企业办公市场针对微软 Surface Hub 的一次正面挑战。
从目前的情况来看，Jamboard 应该是处于 Google 规划中的硬件产品；Google 在官方的介绍中，只是提到 2017 年可以买到，但是关于这项产品的配置介绍，却是非常笼统的。

图自：Microsoft
而相比较而言，微软的 Surface Hub 早已经在今年 3 月份发货，而且在实际的办公体验上，Surface Hub 并不差。虽然价格相对高了一点，但 Surface Hub 依然不失为一个有力的竞争对手。
题图来自：YouTube
#欢迎关注爱范儿认证微信公众号：AppSolution（微信号：appsolution），发现新酷精华应用。
爱范儿 |
原文链接 ·
查看评论 ·
新浪微博




 </content>
</doc>
<doc>
	<docid>51</docid>
	<title>为了让你在国外花钱更方便，这些公司也是拼了</title>
	<link>http://www.ifanr.com/737392?utm_source=rss&utm_medium=rss&utm_campaign=</link>
	<author></author>
	<content>
在刚刚过去没多久的十一假期里，我们足不出户也能在朋友圈里看遍世界。出境游成为了不少年轻人规避出游高峰，享受假期的优选。
国家旅游局发布的数据显示，2016 年上半年中国公民出境旅游人数达 5903 万人次，比上年同期增长 4.3％。中国已经成为泰国、日本、韩国、越南、俄罗斯、朝鲜、马尔代夫、英国等国的第一大入境旅游客源地。
盯上出境游的可不只是游客，不少中国互联网公司也希望借此机遇，实现自己“梦寐以求”的国际化战略。
出境游：携程的下一个强劲增长点
10 月 24 日，携程宣布与美国纵横、海鸥、途风等三大旅行社企业达成战略投资与合作，全面布局北美旅游市场，携程集团高级副总裁、携程旅游 CEO 杨涛表示，走向国际化、布局出境游目的地是携程的重要战略。

在线旅游平台配合目的地旅行社推出在线旅游产品，这样的模式或许会不断推动携程出境游业务总交易额（GMV）的提高。目前携程出境游业务除常规的跟团游之外，还囊括了包括接送机、国际租车、门票预订、Wi-Fi、保险等独立业务，若能够通过旅行团的形式向消费者打包出售旅游产品，对于携程的收入是一个利好消息。
现阶段，出境游既是投资者非常关注的一块业务，也是携程目前增长最为快速的业务。在今年 3 月份的 2015 财年第四季度财报电话会议上，高盛集团分析师 David Jin 询问了携程出境游和境内游总交易额比例问题，携程联席总裁兼 COO 孙洁表示：
目前，出境游业务在公司总 GMV 中的占比仍然很小，但是出境游业务在公司总 GMV 中的占比增长速度远远快于境内游业务。我们预计，到 2020 年，出境游业务 GMV 将在公司总 GMV 中占到相当大的比重。
类似的高管发言还出现在 2016 财年第一、第二财季中，方便我们对携程出境游业务的发展趋势有所了解：
出境游对公司各个业务的营收均有贡献。具体来看，出境游对携程酒店业务的贡献有 15%－20%，对机票营收的贡献有 30%，度假业务占 60%－70%。——携程 CFO 王肖璠（2016 财年第一季度）
 
我们将会通过专注于提供一站式在线旅游平台，扩展出境游业务覆盖度，提高运营效率，以强化携程的行业领导地位。——携程 CEO 梁建章（2016 财年第二季度）

根据艾瑞咨询发布的《2016 年中国在线旅游度假市场研究报告》，2015 年出境游业务占据了国内在线旅游市场 52.6% 的份额，市场规模非常可观，携程网则在此占据了 25.1% 的市场份额，高居市场首位。
移动支付：银联、蚂蚁金服形成两大重要势力
中国是目前世界上移动支付最为发达的国家，没有之一。这样的市场环境也促使中国银联、支付宝等国字号银行机构及互联网公司加快海外布局，为中国人在海外放心买买买打好支付基础。
而在这场商战中，中国银联显然依靠官方地位走得更快更好。
10 月 21 日，银联国际宣布与德国两大收单机构 B+S Card Service、CardProcess 公司达成合作，总计开通当地 5 万多家商户受理银联卡，使德国商户的银联卡受理覆盖率从现在的 30% 提升到 50%。银联还透露在英国、法国、瑞士、捷克等国家有超过 100 所大学支持使用银联卡支付学费。

（图片来自：Business Insider）
银联官方数据显示，截止至今年 9 月 13 日，银联卡可以在境外 1800 万家商家和 130 万台 ATM 使用。服务方面，如在境外发生卡片丢失、被盗等情况，持卡人能在 220 多个国家和地区超过 40 万个服务网点，申请最高等值 5000 美元的现金支援。
除了商业上的合作，银联还在向境外输出芯片卡标准。10 月 16 日，银联宣布与亚洲支付联盟 7 家会员机构达成芯片卡标准授权合作，这也意味着银联主推的云闪付功能未来很有可能会出现在新加坡、泰国、韩国、马来西亚、印尼等亚太国家，进一步降低中国消费者境外刷卡的门槛。
不过，考虑到银联是直接由央行批准成立的金融服务机构，既是金融市场的参与者，也一定程度上影响着市场规则的制定，所以银联的境外发展路径具有唯一性，不可复制。以支付宝为代表的互联网金融公司，更多地是凭借自己在移动端上的优势及对用户需求的把握，在消费者和商户需求最旺盛的领域展开激烈竞争。

（一家支持支付宝消费的韩国店铺，图片来自：新华社）
以韩国为例，根据第一财经报道，拉卡拉在今年 8 月末宣布与韩国友利银行达成合作推出本地结算业务。至此，得益于中国游客赴韩旅游、购物的巨大需求，中国已有支付宝、微信支付和拉卡拉三大移动支付品牌进驻韩国。早在 2015 年 5 月，韩国副总理崔炅焕就与马云会面，表示将全力支持支付宝在韩国普及。目前，韩国不少主流免税店都已经支持使用支付宝结算。
在几大互联网金融公司里，蚂蚁金服的海外布局规模或许是最庞大的。在此前有关双 11 启动仪式以及蚂蚁金服与 First Data、惠尔丰（Verifone）达成合作的报道中，爱范儿（微信号：ifanr）曾经提到目前支付宝“全球收全球付”服务覆盖全球超过 200 个国家和地区，支持美元、英镑、欧元等 18 种货币一键结算。在 9 月份刚刚结束的二十国集团工商峰会（B20）中，马云提出并倡议的 eWTP（世界电子贸易平台）得到了极高的关注，若 eWTP 最终能成为现实，那蚂蚁金服及旗下支付宝作为阿里的金融“触手”，势必会借 eWTP 的发展，进一步推动其全球支付网络的建设。

将中国企业推向海外的不一定是战略，也可能是消费者的欲望
国际化一直都是中国互联网公司的难题，不少在国际化稍有起色的互联网公司都还仅仅停留在东南亚地区的发展中国家。但是在对出境游市场有所研究之后，爱范儿（微信号：ifanr）却发现了国际化发展在这个领域独特的呈现方式。
伴随着中国游客的脚步走出国门的，既有蚂蚁金服、微信支付、携程、中国银联这样的互联网巨头及金融机构和他们成规模的境外业务，也有小米 MIUI 境外漫游、滴滴出行海外版等偏重于给消费者提供多一点便利的特殊服务。而这些公司的业务之所以能在海外开花，无一例外不是与他们最熟悉的消费群体——中国消费者的需求保持紧密联系。
不过在看似充满成就的“布局”之下，这些互联网公司也要花更多精力确保自己的海外业务的基层执行能力，否则即使将自己的 Logo 贴得满大街都是，也不一定能确保海外服务的体验能够与国内保持一致。毕竟向消费者提供服务的并不是行业大佬，而是分布在街头巷尾的便利店收银员、专车司机等典型的本土服务业劳动者。
题图来自：Frontier Magazine
  
    

      
        
          麦玮琪 (Michael)
          
          关注无人机、汽车，探讨商业模式和科技产品与社会的结合。工作邮箱：michael@ifanr.com
        
      
    
    
      
                    
          
                                  邮箱

          
                
    
  
  #欢迎关注爱范儿认证微信公众号：AppSolution（微信号：appsolution），发现新酷精华应用。
爱范儿 |
原文链接 ·
查看评论 ·
新浪微博




 </content>
</doc>
<doc>
	<docid>52</docid>
	<title>“美国滴滴”Lyft CEO 否认公司在售，但靠“补贴大法”还可以撑多久？</title>
	<link>http://www.ifanr.com/737357?utm_source=rss&utm_medium=rss&utm_campaign=</link>
	<author></author>
	<content>
昨日《华尔街日报》主办的 WSJDLive 全球科技大会上，Uber 在美国最大的竞争对手、美国第二大打车应用 Lyft 的联合创始人 John Zimmer 出席，并当场否认了 Lyft 正在对外出售的传闻，并称 10 月订单量实现翻倍。
订单量翻倍，营利能力还是堪忧
John Zimmer 透露，自去年年底以来 Lyft 在美国的订单数量已经实现翻倍，今年 10 月已达到 1700 万份，而去年 12 月仅为约 700 万份。
今年 6 月份，Lyft 聘请了投资银行 Qatalyst Partners LP 作战略顾问，而 Qatalyst Partners LP 以擅长公司买卖交易著称，因此引来部分分析师怀疑出售 Lyft 的大门已经打开，竞购方可能是资金充裕又对共享专车领域兴趣盎然的通用汽车。

这一怀疑也部分源于 Lyft 自身的状况。自 2007 年创立以来，Lyft 通过多轮融资募集到 20 多亿美元，与 Uber 在美国本土的交战还在持续，目前仍处于亏损状态。对此，齐默在大会上仍对 Lyft 的盈利时间表缄口不言。
但他透露 Lyft 有在未来几年内 IPO（首次公开招股）的计划。“我们并不急于上市，IPO 才是最接近的事。”John Zimmer 说。
虽然今年还有 2 个月左右才结束，但 John Zimmer 估计今年 Lyft 的订单总量会是去年的三倍。目前 Lyft 主要的业务拓展方式仍是共享专车平台的“土办法”：通过大量的现金补贴吸引新用户和专车司机。因此，Lyft 的盈利能力被这部分支出大大压缩，成为目前为止还没有实现盈利的主要原因。
John Zimmer 不断强调 Lyft 订单数量的上升，却避而不谈盈利计划，反而让 Lyft 尴尬处境欲盖弥彰——订单越多，亏得越多。
在“反 Uber”这件事上，Lyft 越来越孤单
在 Lyft 努力扩大美国市场份额，老对手 Uber 则更多把精力投向全球业务。另外，反吞下 Uber 在华业务的滴滴出行也不满足于中国市场，开始思考扩张全球业务。
同样是 WSJDLive 大会上，滴滴出行总裁柳青高调宣布公司将向中国以外的市场扩张，夺取全球份额。美国这块沃土当然也在滴滴考虑范围内。

而去年滴滴还是 Lyft 带头的“反 Uber 同盟”成员之一，成员包括新加坡共享专车平台 Grab 和印度的 Ola。这个联盟试图引导乘客在另一个国家选择联盟成员服务，借此抵抗 Uber 的全球拓展步伐。而滴滴一旦进入美国市场，自然就成为 Lyft 的直接对手。
对此，John Zimmer 表示，Lyft 与滴滴出行的关系有点类似美剧《摩登家庭》 中成员相互独立的关系。他发言称，滴滴出行与 Uber 中国业务合并“很大程度上不会影响到双方的工作或是业务”。

和 Uber、滴滴相比，Lyft 目前独有的优势在于和通用汽车的良好的关系。今年年初，通用汽车向 Lyft 投资 5 亿美元，而且双方计划在未来五年内合作进军无人驾驶市场。
在分工上看，Lyft 能为通用汽车提供丰富的客户资源，而通用汽车作用实力雄厚的老牌汽车制造商能为 Lyft 提供硬件和技术支持。
如果这个无人驾驶计划能落地， Lyft 能将每公里的费用降至 5 美元。更低的价格意味着 Lyft 有希望逃脱目前“补贴多越亏，为了市场份额又不能不补贴”的怪圈。
进入 2016 年下半年，包括 Uber 在内的共享专车平台们都在苦苦寻觅新的盈利模式。都说“补贴烧钱一时，占领市场一世”，但烧的钱什么时候才能转变为真正的盈利，除了视乎平台本身财政状况外，还要看投资人们对这一领域的信心。
从 Lyft 目前的状况看来，投资人们的耐心极限已经临近了。无人驾驶开发也好，市场兼并也好，全球共享专车平台思考变现方式的进程将显著加快。
题图自：《华尔街日报》
插图自：《华尔街日报》、Bidnessetc
  
    

      
        
          刘 浩南
          
          请指教~工作邮箱：liuhaonan@ifanr.com
        
      
    
    
      
                                邮箱

          
                
    
  
  #欢迎关注爱范儿认证微信公众号：AppSolution（微信号：appsolution），发现新酷精华应用。
爱范儿 |
原文链接 ·
查看评论 ·
新浪微博




 </content>
</doc>
<doc>
	<docid>53</docid>
	<title>逼宫亚马逊，Google Express 又放大招了</title>
	<link>http://www.ifanr.com/737296?utm_source=rss&utm_medium=rss&utm_campaign=</link>
	<author></author>
	<content>
据今日美国报道，Google 宣布旗下 Google Express 服务将在美国新增 13 个州，真正做到“只要你掏钱，我哪里都可以送”。
 

（图片来自：Google Express 官方 Facebook 截图）
 
从 Google Express 官方 Facebook 帐号 10 月 25 日发布的消息可以看到，目前 Google Express 已经覆盖了美国的东西两岸。而据统计其辐射范围将接近美国本土的 90% 区域，近 7000 万消费者将因此受益。

不求最全，但求最远
 
我们的宗旨是让人们能够更加便利地从他们真正喜爱的商店购物。
Google Express 总经理布莱恩•艾略特 （Brian•Elliott）的发言强调了 Google Express 存在的核心价值：便利以及独特。
自 2013 年 3 月在美国旧金山和硅谷范围内上线以来，Google Express 的扩张脚步一直没停下来。艾略特曾在九月份表示，其计划在今年底将 Google Express 的在线购物速运从目前的 20 个州和区域拓展至全国范围。
 

（图片来自：SiliconBeat）
 
为了与电商大佬亚马逊（Amazon）进行竞争，Google 这次也是下了血本了。为了加快在全国范围内的扩张，Google 表示将“忍痛”放弃一部分业务，如部分生鲜和食品杂货的运送，以与已占先机的亚马逊 Prime 服务一较高下。
 

（图片来自：TechCrunch）
 
与亚马逊提供的海量商品不同，Google Express 目前合作的 50 多个商家均是经过“精挑细选”的，“我们挑选的商家都是人们真正喜欢的，他们会有种自己不是在网购的感觉。” 艾略特指出了 Google Express 的精品化战略。
与此同时，Google 还计划与第三方运输机构进行合作，以降低其运营成本。
 
赶超亚马逊，道阻且长
 
从 Google Express 上线以来，一直拿来与亚马逊旗下的 Prime 进行比较。但无论是从运输效率、价格竞争还是渠道铺设来看，Google Express 要走的路还很远。

运输效率—— Google Express 和 亚马逊 Prime 都支持最快当日送达。


 价格—— 尽管现在都是付费服务，Google Express 提供两种付费方式供消费者选择：95 美元的年费会员或者 4.99 美元起的单次付费；亚马逊则比较单一，需花费 99 美元订购 Prime 的年费会员才可获得当日送达服务。


 渠道——这一直以来是 Google 的短板，与亚马逊的直接供货渠道相比，Google Express 只能借助与传统经销商的合作完成货源供给。

 

（图片来自：Bidness Etc）
 
雪上加霜的是，一直引以为豪的访问流量优势也被亚马逊拿走了。
据彭博社报道，几乎将近一半的美国在线购物消费者选择在亚马逊网站或者移动客户端上直接搜索购物信息，这对于以搜索引擎作为核心业务又渴望在电商领域突围的的 Google 来说无疑是一个不好的信号。
可以预见的是，Google Express 这一波大招尽管炫目，但要狙击亚马逊还要多修炼。
题图来自：TechCrunch
#欢迎关注爱范儿认证微信公众号：AppSolution（微信号：appsolution），发现新酷精华应用。
爱范儿 |
原文链接 ·
查看评论 ·
新浪微博




 </content>
</doc>
<doc>
	<docid>54</docid>
	<title>柳青说滴滴要展开国际化扩张，但这是一件不容易的事</title>
	<link>http://www.ifanr.com/737354?utm_source=rss&utm_medium=rss&utm_campaign=</link>
	<author></author>
	<content>
据新浪科技，滴滴出行总裁柳青在 WSJDLive 2016 全球科技大会表示，滴滴将向中国以外的市场扩张，夺取全球份额。在此过程中，滴滴有可能与一些国家的当地专车服务展开合作。
我们希望全球化。
与 Uber 直接在全球建立业务的全球化做法不同， 一直以来，滴滴更多的是通过投资入股海外的打车出行平台的方式形成自己的全球化布局。
例如说，滴滴与 Uber 中国合并后不久，就投资了东南亚地区最大出行软件 Grab。
但这不是滴滴第一次投资 Grab 了，也不是它第一次投资海外打车平台。2015 年 9 月，滴滴入股美国本土打车软件 Lyft 1 亿美元，之后又投资入股印度 Ola 3000 万美元。
这是滴滴在海外扩张至今采取的主要策略：曲线救国，通过投资进入其他海外市场。
除此之外，滴滴还通过打通产品线的做法进行扩张。
今年 4 月，滴滴还在美国上线了“滴滴海外”，用户可以用滴滴 app 呼叫到 Lyft 的汽车。值得一提的是，为方便中国用户使用，“滴滴海外”界面均为中文显示，整体叫车发单流程与国内保持一致。
与 Lyft 产品互通这一做法让滴滴在形式上直接进入了美国市场。
去年年底，滴滴、Lyft、Grab、Ola 四方曾宣布建立全球合作框架，这次合作除了能够为他们带来更多收益和更大的用户群体之外，全球无缝连接的服务也让他们具有和 Uber 一样的全球竞争力。

图自：Tech in Asia
这次四方合作也被视为形成了“全球反 Uber 联盟”。但这个联盟却在滴滴与 Uber 中国合并后陷入了尴尬甚至瓦解的境地。
Lyft 与 Uber 在美国市场的竞争已有很长时间，本来 Lyft 与滴滴合作，前者以为有了合作伙伴能一起对抗 Uber，但却没想到合作方转而又与竞争对手合作。Lyft 官方发言人 Alecandra LaManna 曾说：“我们将进一步评估与滴滴之间的伙伴关系。”
除了 Lyft，Grab 和 Ola 也得重新审视和思考自己和滴滴的合作得到哪个层面合适。外媒在对 Ola 相关人士采访时获知，Ola 仍坚持认为滴滴出行收购 Uber 中国不会给公司业务构成影响，因为滴滴只是 Ola 的小股东，且虽然 Ola 和滴滴出行有结盟关系，但好在尚未发布合作产品。
滴滴没有像 Uber 一样亲力亲为地去开拓疆土，而是通过与本土化的打车软件合作形成海外格局，但也由于复杂和不确定的合作关系，让自己的全球计划变得有些窘迫和尴尬。

图自：PitchBook
如今，滴滴柳青重新表示，滴滴正在评估应该进军哪些新市场，既有可能与当地现有的专车公司合作，也有可能与之竞争。如果滴滴认为当地的企业实力不够强大，便有可能与之竞争。
这是一场艰难的比赛，但我们乐在其中。
也就是说，滴滴可能不仅仅通过各种嫁接形式来进入海外市场，也将会采取和 Uber 相似的做法，直接进入当地市场提供打车服务。
当然，Uber 在进军国际市场时引起了不少争议和冲突，这也将是滴滴全球化进程里不得不面对的问题。
题图自：新浪科技
  
    

      
        
          陈 诗蔚 (chenshiwei)
          
          VR/AR/新兴媒介，工作邮箱：chenshiwei@ifanr.com
        
      
    
    
      
                                邮箱

          
                
    
  
  #欢迎关注爱范儿认证微信公众号：AppSolution（微信号：appsolution），发现新酷精华应用。
爱范儿 |
原文链接 ·
查看评论 ·
新浪微博




 </content>
</doc>
<doc>
	<docid>55</docid>
	<title>Uber 自动驾驶全球首秀：193 公里和 5 万罐啤酒的完美征途</title>
	<link>http://www.ifanr.com/737231?utm_source=rss&utm_medium=rss&utm_campaign=</link>
	<author></author>
	<content>
今年 8 月被 Uber 以 6.8 亿美元收购的自动驾驶卡车公司 Otto ，终于在本月 20 日清晨，完成了全球首次自动驾驶卡车送货任务。
司机沃尔特‧马丁将这辆 18 轮卡车从柯林斯堡的啤酒厂开上 25 号州际公路后，打开了“接管”开关，卡车进入自动驾驶模式，他随即起身离开驾驶座，来到后座系好安全带闭目养神。

图片来自：视频截图
在无人干扰的情况下，Otto 的卡车自动行驶了 120 英里（约 193 公里）到达科罗拉多的斯普林斯，为安海斯-布希啤酒厂（Anheuser-Busch）运送了 5 万罐百威啤酒。
科罗拉多州运输部的官员参与了线路规划，并监督了自动驾驶的整个流程。卡车在行驶过程中，科罗拉多的巡逻队全程跟随。交货后，啤酒厂支付了 470 美元的运费。
卡车司机要失业了？
这并不是一次普通的测试，193 公里的路程意味着 Otto 自动驾驶系统已具有实际使用价值。
不同于特斯拉的 Autopilot 自动驾驶系统，Otto 的系统提供真正的“4 级”自主权，一旦进入州际公路，它便能完全接管驾驶，解放司机双手。
这一原理与飞机的自动驾驶类似：飞行员操纵飞机起飞到一定高度后开启自动驾驶仪，使飞机自动按指定的姿态、航向、高度和速度飞行，而飞行员则集中精力完成其它与飞行安全相关的工作。
不过司机们无需为此担心，Otto 的自动驾驶系统仅能用于高速公路，因为高速路的路况较为简单，不必避让行人、自行车或停靠在路边的车辆等。同时，它会让车辆保持安全的车距，并且只有当绝对有必要时才会变道。
而在到达目的地的最后几公里，由于进入到路况复杂的市区，还是需要切换到手动驾驶模式。

图片来自：The Verge
Otto 也表示他们的系统并不打算跟司机们抢饭碗，其联合创始人 Lior Ron 认为，在未来，司机的角色可能更像“港口引航员”，负责引导卡车到达目的地。
至少在可预见的未来，驾驶员仍然是自动驾驶中不可或缺的一员，但除了处理复杂道路情况外，他们也能做些其他事。“我觉得我得练习一下瑜伽。”马丁在采访中开玩笑说道。
卡车有望率先实现大规模自动驾驶

图片来自：The Verge
Otto 安装在测试卡车的自动驾驶装备并不复杂，包括位于驾驶室和拖车的 3 个激光雷达检测单元、一个连接到保险杠的雷达螺栓、一个位于挡风玻璃上方的高精度摄像头。车内还有两个红色按钮用于控制自动驾驶系统开关，一个靠近方向盘，另一个在座位后面的卧铺旁。
Otto 表示这些硬件适用于任何具有自动变速器的卡车，而他们的最终目标，无疑是想要实现美国 430 万卡车的自动化。
至于为何选择从卡车行业切入自动驾驶市场，Otto 自有其考量。
卡车运输占据了美国 70%的运费，每年运送货物达 105 亿吨。然而，卡车司机的数量却远远无法满足需求。美国卡车协会估计，当前卡车司机的缺口约为 4.8 万名，到了 2024 年，这一数字可能会扩大至 17.5 万。
Otto 的自动驾驶系统虽然不能完全取代司机，但无疑能提高运输效率，缓解日益短缺的卡车司机资源缺口。
更重要的是，自动驾驶技术还能进一步提高道路的安全性。

图片来自： MashableAsia
美国交通部数据显示，美国卡车年行驶里程占所有车型里程数的 5.6%，但事故率却高达 9.5％。每年约有 400 万起卡车交通事故，造成约 4000 人死亡，而几乎每一起事故都是人为因素造成的。
而在长途驾驶中，电脑往往人脑更为靠谱，因为电脑并不存在疲劳驾驶的情况。
此外，有研究表明，使用自动驾驶卡车可以使排放量降低 15% ，行驶 10 万英里可以节约 5000 英镑。
然而，Otto 自动驾驶硬件成本高达 3 万美元，在投入商用之前，还需要解决成本过高的问题。
目前，Otto 正在致力于提升自动驾驶系统的基本体验，比如让加速和制动过程更为平滑稳定，以及提高车道控制等。他们的长期目标还包括预测其他司机的可能行为、建筑区域导航、处理突发恶劣天气等危险。
题图来自：The Verge
  
    

      
        
          吴羚
          
          工作邮箱：wuling@ifanr.com
        
      
    
    
      
                                邮箱

          
                
    
  
  #欢迎关注爱范儿认证微信公众号：AppSolution（微信号：appsolution），发现新酷精华应用。
爱范儿 |
原文链接 ·
查看评论 ·
新浪微博




 </content>
</doc>
<doc>
	<docid>56</docid>
	<title>瑞典政府：只要带相机的无人机都不能飞！</title>
	<link>http://www.ifanr.com/737080?utm_source=rss&utm_medium=rss&utm_campaign=</link>
	<author></author>
	<content>
摄像无人机 = 监控摄像头？这是瑞典最新法规给带摄像头的无人机所下的定义。
自无人机出现以来，有关于 “偷拍”、“窥视” 等词就一直跟在它的后面，无论在搜索引擎还是视频网站搜索都能有大量相关信息。由于无人机可以进行远程操控，飞离机主对他人进行拍摄。对于瑞典来说，这严重违反了他们的监视法。

图片来源：Drones Globe
因此在近日，瑞典法院裁定，将带摄像头的无人机列入禁用名单，除非是为了调查犯罪案件或事故，否则像是婚礼、新闻等需要使用无人机的活动都要申请许可证。
虽然这一法规可以有效保护个人隐私不被泄露，但却严重打击了瑞典正在蓬勃发展的无人机市场。据统计，仅 2015 年瑞典就售出了 2 万多台无人机，其中有 1000 多台都用于商业目的。

题图来源：PetaPixel
瑞典本土工商业组织就带头反对，他们不仅认为该法案过于严格，而且执行起来也相当困难。哪怕摄像无人机被禁止，其他类型的无人机却没有，所以执法机构将很难判断无人机是否在飞行时进行偷拍，也很难采集偷拍的证据。
也许瑞典在反对声音下可以考虑更改这一法案，毕竟 2014 年的时候他们还裁决摄像无人机合法可用。
瑞典可以学习一下友邻英国的无人机法案：
小型无人机（SUA）操作员必须保持时时刻刻能看见无人机，对无人机能够完全掌控，在飞行时为其他飞行器、人群、车辆以及建筑保持一定的空间以免发生碰撞事故。
配备相机的无人机不得在距离人群、车辆、建筑等 50 米内飞行，在拥挤的地段，无人机不得在距离拥挤物体 150 米以内的空间里飞行。
 
题图来源：YouTube
#欢迎关注爱范儿认证微信公众号：AppSolution（微信号：appsolution），发现新酷精华应用。
爱范儿 |
原文链接 ·
查看评论 ·
新浪微博




 </content>
</doc>
<doc>
	<docid>57</docid>
	<title>苹果的汽车操作系统，可能是“黑莓出品”</title>
	<link>http://www.ifanr.com/737136?utm_source=rss&utm_medium=rss&utm_campaign=</link>
	<author></author>
	<content>
据彭博（Bloomberg）最新报道，苹果安排了数十名软件工程师在其加拿大办公室里研发自动驾驶汽车操作系统，并且其中二十多位都是从黑莓（BlackBerry）QNX 公司挖过来的。
这些工程师现工作于苹果在渥太华市郊坎纳塔的办公室，离 QNX 办公室只有五分钟步行距离。坎纳塔办公室成立于今年年初，据说是苹果为了能够更好地吸纳当地自动驾驶软件人才的举措。

图片来自 TechCrunch
虽然 QNX 最为广为人知是建造车内娱乐系统，但其于去年就开始研发自动驾驶车辆软件了。苹果看似特别青睐黑莓 QNX 成员，不仅挖来了工程师，早于今年 7 月，已经聘请前黑莓 QNX 负责人 Dan Dodge 参与到苹果汽车项目 Project Titan 中，而另一位关键人物则是 QNX 前资深工程师 Derrick Keefe。
Dan Dodge（图片来自 investottawa.ca）
由于苹果不是在建造实体车，因此有传闻说他们已经开始用 VR 模拟测试自动驾驶平台了。该 VR 技术由另一 Project Titan 的小队研发而成，由 VR/AR 专家 Doug Bowman 带领。
虽然在过去的一年里， Project Titan 招来了很多“大人物”，但在上个月的时候，项目组经历一次大型裁员。该项目目前应该仍处于一个不太稳定的状态，并且工程师也面临着一个强硬的最后期限 —— 要在 2017 年秋季推出可行的自动驾驶操作系统。
题图来自：汽车科技
#欢迎关注爱范儿认证微信公众号：AppSolution（微信号：appsolution），发现新酷精华应用。
爱范儿 |
原文链接 ·
查看评论 ·
新浪微博




 </content>
</doc>
<doc>
	<docid>58</docid>
	<title>Google 光纤撂挑子了，超高速网络想说爱你不容易</title>
	<link>http://www.ifanr.com/737040?utm_source=rss&utm_medium=rss&utm_campaign=</link>
	<author></author>
	<content>
 
曾经羡慕嫉妒恨美国人可以使用比传统光纤快近 100 倍的 Google 光纤 （Google Fiber），此前为了升级服务， Google 光纤更是刚与高速无线网络服务商 Webpass 达成合作，现如今推出这一试验性服务的 Google 却表示撑不住了。
美国时间 10 月 25 日，Google 光纤宣布暂停其在十座“有潜力城市”的光纤拓展计划。
Google 光纤 CEO 克里格•巴瑞特（Craig•Barratt）在官方博客上发表文章称:
我们必须坚持不只是攻城掠地，更要稳固城池——不断扩展科技、商业、政策的边界——坚持做提供超高速网络服务的行业领导者。因此我们决定重塑我们的计划去达到这样的目标。我们必须做出改变去更加关注商业和产品体系。更为重要的是，这个计划将强化我们在新科技和资源调度方法的关注度，以此带来比今天更为丰富的超高速网络资源。
Google 光纤从 2011 年 3 月开始上线，目前该服务已经拓展至美国的 13 个城市地区，而此次关停的 10 座“有潜力城市”包括：达拉斯、杰克森维尔、洛杉矶、俄克拉荷马城、凤凰城、波特兰、圣何塞以及坦帕。
 

（图片来自：Google Fiber Official Blog）
据 engadget 消息，此次关停并不会影响之前已铺设及确定上线 Google 光纤的城市，而此前处于公司“扩张版图”内的城市包括芝加哥、达拉斯、波特兰、坦帕以及圣迭戈，其继续 Google 光纤服务的前景不容乐观。
尽管巴瑞特指出 Google 光纤的服务订阅数以及营收在不断增长，但基于高额的光纤网络运营费用和未达标的订阅人数之间的矛盾，Google 可能会考虑暂缓其在物理光纤缆线方面的铺设。

（图片来自：Tech News Today）
此外有消息称，Google 光纤将同时裁减这些城市的员工，受影响人数近百分之九。
巴瑞特同样在博客文章中表达了自己离开 Google 光纤 CEO 职位的意愿，称其将继续担任顾问。
题图来自：VentureBeat
#欢迎关注爱范儿认证微信公众号：AppSolution（微信号：appsolution），发现新酷精华应用。
爱范儿 |
原文链接 ·
查看评论 ·
新浪微博




 </content>
</doc>
<doc>
	<docid>59</docid>
	<title>为了运送 88 包棉花，这两家银行用上了区块链和物联网</title>
	<link>http://www.ifanr.com/736607?utm_source=rss&utm_medium=rss&utm_campaign=</link>
	<author></author>
	<content>
比特币的“激情不再”，让人们愈发不再通过各大“投机者斗兽场”去尝试暴富。这也让人们对于区块链愈发正视和乐于尝试。

图片来自 BusinessInsider
前天，澳洲联邦银行（CBA）、富国银行（Wells Fargo）就利用区块链、物联网进行了一次银行间贸易事物的试验，目的只有一个：验证区块链技术和物联网技术的实用性。
此次交易的双方实际上同属博瑞棉花（Brighann Cotton），只不过分别为美国部和澳大利亚部，交易的内容是将一批 88 包的棉花从美国的德克萨斯州运到中国的青岛。

图片来自 Skuchain
而他们使用的区块链技术是美国区块链公司 Skuchain 的 Brackets 系统，而在 Skuchain 上也有这套系统主要优势的简短介绍：

它能够促进金融家对常见的交易物如采购订单、发票、库存和付款约定进行贷款，同时允许贷款成为抵押资产，放款操作将会直接由真实世界中的事件触发。
它提供了一个实时可信的交易状态查看能力，这也大幅度的提升了交易中所有参与者的信息透明度，并且帮助他们建立更加强的相互信任，从而建立稳定的供应链生态。
它增强了市场中抵押资产的流动性，同时也大幅提升了常见的“贸易融资”工具，比如保理，融资 PO，供应商库存管理等。这同时也提供了进行更深层次金融的机会。

在这次交易中，这些特点也有所体现。原来交易中的大量的纸质信用证通过存储在私有分布式账本上的一个数字智能合约来执行。这份智能合约由计算机代码编写，一旦达到合约条件，交易就会自动执行。

图片来自 Wikipedia
而合约条件自然就是棉花到达目的地，博瑞棉花使用了一个 GPS 设备来追踪运输中货物的地理定位。一旦他们到达最终目的地青岛，智能合约就会自动放出资金。
澳洲联邦银行的现金流和交易服务处执行总经理 Michael Eidel 表示：
我们想要证明区块链、智能合约与物联网可以实现结合。从客户的角度看，三项技术的结合可以实实在在地带来令人兴奋、简单便捷和直接的用户体验。
通过这三项技术，原来需花费数日的人工处理程序如今可以在几分钟内完成，这就降低了成本，提高了效率。除此之外信息的透明度也有所上升，因为所有数据都在同一个私有分布式账本之上，并且数据还不可篡改，发生业务欺诈的可能心也大大减少。Eidel 还表示：
我们追求的并不是建立一个完全灵活的模型，而是开始真正使用一些非常重要的新技术，把各种技术结合起来并带来一个可行的良好用户体验。
下一步，他们打算拓展物联网部分——尝试将他们运用在保险行业中，提高对投保货物的监控。Eidel表示：
可以进行一个试验，添加一些传感器来测试湿度和温度是否超出范围。然后对这些条件进行编码的智能合约就会发出通知，告知保险公司货物可能已经损坏。
用这种方法，在出现一些失误情况时会变得十分有用，因为这些设备可以准确展示什么时候发生了事故，事故当时的货物所有权归谁所有。
从技术的角度来看，这一次尝试想要推广开来还将面对各种各样的困难，但是很明显，各种各样的测试必然会成为区块链接下来发展的一个重要阶段。尤其是对于很多只利用部分特性的应用场景，能否真得对现有模式形成冲击，也只有试过才知道。
题图来自 Myrugshop
  
    

      
        
          李 赓
          
          鲜衣怒马少年。
        
      
    
    
      
                                邮箱

          
                
    
  
  #欢迎关注爱范儿认证微信公众号：AppSolution（微信号：appsolution），发现新酷精华应用。
爱范儿 |
原文链接 ·
查看评论 ·
新浪微博




 </content>
</doc>
<doc>
	<docid>60</docid>
	<title>刚拿到 5000 万美元的 Hyperloop One，打算再融资 2.5 亿美元</title>
	<link>http://www.ifanr.com/737087?utm_source=rss&utm_medium=rss&utm_campaign=</link>
	<author></author>
	<content>
尽管 Hyperloop One 刚在本月中旬拿到了 5000 万美元融资，但他们将在明年初启动新一轮融资，这一次的目标是 2.5 亿美元。
《福布斯》最先爆出这一消息，他们声称获得了该公司的一份融资文件，并透露 Hyperloop One 目前的项目经费预算已经超出了提出这一概念的马斯克当初的估计。这份文件的真实性已经得到了其 CEO 罗博·洛依德(Rob Lloyd) 的证实。

图片来自：Forbes
Hyperloop One 自成立以来，筹集的资金已达 1.6 亿美元。Hyperloop One 计划在 2020 年建成货物运输系统以及 2021 年完成乘客输送系统，但《福布斯》获得的文件显示，这两个项目的成本与预期有很大出入。
马斯克在 2013 年提交的白皮书中，预计从洛杉矶到湾区的 Hyperloop 线路成本只需要 60 亿美元，即每英里不超过 1150 万，远低于高铁。
然而，Hyperloop One 在 7 月和 10 月提交给潜在投资者和地产开发商的报告却显示，许多项目的成本比“钢铁侠”预估的要高得多，比如一条长为 107 英里的环湾区隧道或隧道+高架轨道组合，花费就将达到 90 亿至 130 亿美元，即每英里成本为 8400 万至 1.21 亿美元。

图片来自：Time
此外，在上一轮融资中，Hyperloop One 透露他们打算在迪拜的杰贝阿里港建设超级高铁运输系统，这条从迪拜到阿布扎比的线路长达 93 英里，预计成本为 48 亿美元。
也就是说，在 Hyperloop One 第一期总计长达 520 英里的超级高铁项目中，成本预算达到了 640 亿美元。
尽管 Hyperloop One 一直强调一旦项目上马，利润将相当可观，单个项目至少也有 35% 的利润率，到 2030 年可能会更高。但 Hyperloop One 至今仍只是一个原型产品，公司在自身发展和证明概念的可行性上耗资巨大。
这也是为什么在短短两年内，虽然手握 1.6 亿美元投资，Hyperloop One 却仍在继续寻求融资的原因。
融资文件表明， Hyperloop One 已经开始在以每年 5% 的收益与 15%的 C 轮最终股价折让，向投资者提供额外的可换股票据。一些风投专家认为这可能表明该公司正在进行大量现金积累，用于 Pishevar（创始人）所说的“资本密集型业务”。
对于 Hyperloop One 画的大饼，很多人始终持怀疑态度，而 Hyperloop One 一直也是麻烦不断，比如与其联合创始人布罗根·班布罗根（BroganBamBrogan）之间的官司及内讧等。但外界依然认为，Hyperloop One 是最有可能实现“超级高铁”这一设想的公司。
题图来自：Tech Crunch
  
    

      
        
          吴羚
          
          工作邮箱：wuling@ifanr.com
        
      
    
    
      
                                邮箱

          
                
    
  
  #欢迎关注爱范儿认证微信公众号：AppSolution（微信号：appsolution），发现新酷精华应用。
爱范儿 |
原文链接 ·
查看评论 ·
新浪微博




 </content>
</doc>
<doc>
	<docid>61</docid>
	<title>词错率降低至 5.9%，微软今日发布用于语音识别技术的“认知工具包”</title>
	<link>http://www.ifanr.com/737013?utm_source=rss&utm_medium=rss&utm_campaign=</link>
	<author></author>
	<content>
一个月前，微软的对话语音识别技术在产业标准 Switchboard 语音识别基准测试中实现了词错率（word error rate, 简称 WER）低至 6.3% 的突破 ，创造当时该领域内错误率最低纪录。
近期，微软进一步将词错率降低至 5.9%，首次达成与专业速记员持平而优于绝大多数人的表现。
微软的语音识别技术可以一次又一次刷新纪录，在很大程度上要归功于 CNTK (Computational Network Toolkit) 这个开源工具。
该系统最初是为了研究语音应用而建立，后来拓展发展成为微软本地化深度学习系统。CNTK 工具包已于一年前在 GitHub 上开源，目前包括微软人工智能个人助理小娜和 HoloLens 的语音识别都是基于 CNTK 实现的。CNTK 跟其他开源软件最大的区别是它能做大规模、分布式的机器学习，同时保证强大的性能。
今天，该工具包进行了更新，新增一个被称为“认知工具包”的测试版。
针对语音识别的研究可以追溯到上个世纪七十年代 DARPA（Defense Advanced Research Projects Agency，美国国防部先进研究项目局，主要致力于美国国防高新技术的研究、开发和应用）资助的一个相关项目。此后几十年，越来越多研究机构和大型公司陆续加入其中。
“这次突破是过去二十多年语音识别技术不断积累的结果，”微软主管语言及对话研究组的研究员 Geoffrey Zweig 称。

现在微软达到的 5.9% 的词错率是什么概念？
在行业标准 Switchboard 语音识别任务测试中，人类对照组（由专业速记员组成）将对话语音转录成文字，目前有记录的最低词错率就是 5.9%，这就意味着微软的语音识别系统的语音识别能力已经高于世界上绝大多数人而与人类专业高手持平，创造了一项新的世界纪录。
此次语音识别的里程碑式突破将对消费者和商业产品产生深远影响，因为语音识别技术能够显著增强人们的日常计算体验。这些产品包括像 XBOX 的娱乐设备、像微软小娜（Cortana）的生产力工具以及能实现实时语音到文本转录的个人人工智能助手。
微软团队的词错率虽然实现了与人类专业速记员持平的 5.9%，但这并不代表计算机就能完美识别出每一个单词。如果哪天计算机能完美识别“蓝瘦”、“香菇”，那语音识别技术又将达到一个新的高度。
  
    

      
        
          陈 诗蔚 (chenshiwei)
          
          VR/AR/新兴媒介，工作邮箱：chenshiwei@ifanr.com
        
      
    
    
      
                                邮箱

          
                
    
  
  #欢迎关注爱范儿认证微信公众号：AppSolution（微信号：appsolution），发现新酷精华应用。
爱范儿 |
原文链接 ·
查看评论 ·
新浪微博




 </content>
</doc>
<doc>
	<docid>62</docid>
	<title>美国《消费者报告》汽车可靠性排名出炉，特斯拉沦为垫底之一</title>
	<link>http://www.ifanr.com/736513?utm_source=rss&utm_medium=rss&utm_campaign=</link>
	<author></author>
	<content>
以权威性著称的《消费者报告》（Consumer Reports，CR）最新发布了《2017 年车辆品牌可靠性调查报告》。
这份调查报告采用给读者发放问卷的形式，调查读者自己驾驶的车辆在过去 12 个月内出现的故障和质量问题，并且派专人在指定道路对车辆进行试驾，再加上安全性数据。综合统计调查结果后，依据车型可靠性分数来预测出品牌平均分数。
今年，CR 共收集了超过 50 万辆车的数据，涵盖了从 2000 年到 2016 年的 300 多个车型，还有 2017 年初的几个车型，以下是品牌排名：

表现依旧优异
在 CR 的分析报告中，使用了 “不足为奇” 这一词来形容排在榜单一、二位的雷克萨斯和丰田。它们以一贯优异的表现继续留在了榜首，CR 还称赞其造车模式已相当稳定。
同时，在十大可靠车型报告中，丰田普锐斯、4Runner 车型和雷克萨斯 CT 200h、GX、GS 车型均有上榜，两家就已经占比一半。

图片来源：Motor Trend
紧随其后的别克由于其核心产品线成熟，车辆的大多数问题也已经解决，同时别克推出的几款新车也相当亮眼，可能对未来的品牌表现产生影响。今年别克生产线还进行了调整，将 SUV 的产量下调，轿车占比更多。
都是 Model X 惹的祸
排在末尾的几家品牌中，第一次上榜的特斯拉特别显眼。这是由于在去年年底发布的 2016 年最佳车型中，CR 给予了特斯拉 Model S 的两款车型超过 100 分的超高评分。这在当时相当轰动，因为这是 CR 所给予过的最高评分。
对于 Model S 《消费者报告》给予的评价是：
Model S P85D 具有里程碑式的意义。
虽然 Model S 表现优异，却也难以补上 Model X 的大窟窿。
看了标题的你，可能以为给特斯拉背锅的会是他们的自动驾驶系统，然而 Model X 车型的硬件隐患更多。从灵敏传感器误判到漏雨，再到鹰翼门故障，系统升级后又再次故障…… 这一系列的问题让车主不胜其烦。
（爱范儿（微信号：ifanr）曾经报道过《用 8 万美元的 Tesla Model X“鹰翼门” 切黄瓜？这并不是玩笑》）

图片来源：YouTube
因此 Model X 也被 CR 列入了今年排名最低的 10 款车型之一。
 有起有落
与 2016 年的汽车品牌可靠度排名相比，英菲尼迪完成了一次逆袭。从 24 名到第 8 名，足足上升了 16 个名次，除了看好他们发布的几款新车，CR 也把 Q70 车型列入了十大可靠性车辆，从而拉动了品牌平均分，CR 评价到：
Q70 操作相当敏捷，转向和行驶稳定，后排宽敞的 L 型座椅十分舒适，并且和同类车辆比价格更为优惠。

图片来源：Motor Trend
反观跌幅中最为意外的，是一向 “稳重” 的本田，这次它跌出了最可靠品牌的名单，在这方面可谓是第二个“特斯拉”。本田思域的问题接连不断，断轴隐患、引擎故障、电子系统故障…… 停售、召回一系列风波使这个可靠的日系品牌丧失了一大批忠实顾客。
电子化系统
关于品牌榜单，CR 调查了十几个车辆可能存在不足的方面，包括嘈杂的制动器、糟糕的内饰、主要保险杠的损坏，还有保修期外变速器维修或四轮驱动系统的故障……

图片来源：YouTube
大部分品牌的下跌，都是因为自己某一款车型出现了非常严重的问题。并且由于近年来，汽车厂商大量加入复杂的电子化操作系统，而系统故障导致的差评也存在很多。
鉴于这是一份纵观行业的榜单，而如果消费者需要依据榜单选购车辆，那么车型可靠性榜单可能会更加有参考性。

 
题图来源：Wired
#欢迎关注爱范儿认证微信公众号：AppSolution（微信号：appsolution），发现新酷精华应用。
爱范儿 |
原文链接 ·
查看评论 ·
新浪微博




 </content>
</doc>
<doc>
	<docid>63</docid>
	<title>几乎确认：新款 MacBook Pro 将搭配 OLED 触控按键和 Touch ID</title>
	<link>http://www.ifanr.com/737015?utm_source=rss&utm_medium=rss&utm_campaign=</link>
	<author></author>
	<content>
当地时间 10 月 27 日苹果新品发布会的看点，已经没有悬念地落在了新款 MacBook Pro 的身上；而且这一次，人们不仅需要一款新产品，还需要一个新设计。

据外媒 MacRumors 报道，在最新发布的 macOS Sierra 10.12.1 版本中，有一张关于在 MacBook 电脑上使用 Apple Pay 的图片。而在这种图片中，一款标有 MacBook Pro 的产品赫然出现，而且它的键盘上方拥有一个黑色长条，这个长条取代了现有的一排按键。

图片显示，这台 MacBook Pro 正在请求用户确认是否使用 Apple Pay；长条上同时显示了一些内容，这些内容是在提示用户将手指按在长条最右侧的一个模块来使用 Apple Pay。
从这些细节来看，几乎可以确认，这个长条就是一个能够显示不同内容的 OLED 可触控屏幕，而长条右侧则配置了一个可以进行指纹识别的 Touch ID 模块。
根据此前的消息，这块 OLED 屏幕将可能被命名为“Magic Toolbar”。
另外还有一张细节图，可以隐约看到在 OLED 屏幕的右侧，有一个相对独立的模块；二者从视觉上几乎是一体的，但细看依然能分辨出来。

注意细看 OLED 长条右侧
关于上述内容的真实性，还可以从此前新款 MacBook Pro 的泄露图中得到佐证。
今年五六月份，一组关于新款 MacBook Pro 外壳的泄露图曝光。其中一张泄露图显示，在键盘上方的确取消了一排实体功能键，留出了给 OLED 屏的空间；此空间右侧有一个小的相对独立模块，可以用来放置 Touch ID 模块。

这张泄露图所展示的信息，与我们目前所看到的新款 MacBook Pro 设计完全相符合。
另外，从这组泄露图中也能看到关于新款 MacBook Pro 在外观上的其他设计。比如说，机身两侧将会有 4 个 USB Type-C 接口和一个 3.5 毫米耳机接口；而 SD 卡槽、HDMI 接口和 MagSafe 接口将会被取消。

关于新款 MacBook Pro 的相关配置预测，可以参见爱范儿（微信 ID：ifanr）此前的文章。不过正如文章所言：
这次新的 MacBook Pro 真的不需要期待，你所需要做的只有存钱和买。
另外，爱范儿（微信 ID:ifanr）目前也已经受邀参加此次苹果新品发布会，届时我们将第一时间带来新款 MacBook Pro 的相关信息，敬请关注。
#欢迎关注爱范儿认证微信公众号：AppSolution（微信号：appsolution），发现新酷精华应用。
爱范儿 |
原文链接 ·
查看评论 ·
新浪微博




 </content>
</doc>
<doc>
	<docid>64</docid>
	<title>对话网约车：外地司机的愤怒，京籍司机的尴尬</title>
	<link>http://www.ifanr.com/736986?utm_source=rss&utm_medium=rss&utm_campaign=</link>
	<author></author>
	<content>
整理 | 崔神
与我一同坐在直播间里的，是三位西服笔挺、皮鞋锃亮的网约车师傅，他们都是模范专车司机。这与我们上次在街上抓的形色各异的网约车司机看上去很不一样。
之前我在问领英要这些师傅介绍的时候，对方扔过来的是网上对他们的新闻报道。所以真聊起来，感觉他们显然对采访这事儿并不陌生。他们的观点表述没有那么凌厉，却有另一种独特。
三位网约车师傅：

边师傅 户籍燕郊 京牌车 做网约车 1 年多
王师傅 户籍河南 公司车 做网约车 1 年多
张师傅 户籍北京 京牌车 做网约车 1 年半


壹
博望志：新政关于网约车的排量和轴距要求，会带来哪些影响？
张师傅：排量的 T 是个关键问题，我感觉现在德系车一般带 T 的多一些，你像日系车就不带 T，它就是 1.8，这就非常尴尬。
滴滴专车现在主要路上跑的，有几项大车，天籁、本田雅阁、帕萨特、凯美瑞，那毫无疑问帕萨特是大众的车，一般都是 1.8T 以上的。
那凯美瑞、本田雅阁包括天籁，它没有带 T 的，要么就 2.0，要么就 1.8。但是一般我们拉活儿的人都会买 1.8 的，终归要省油，成本减少了。一旦出台 1.8T，那可以想象一下……
所以我不太赞成这个 1.8T。因为轴距我认为差不多都能达到，因为专车属于 B 级车，它都会这样设计，就是这个 T 我觉得比较敏感一点。
边师傅：我觉得排量这个出来以后，限制了很多快车，快车这部分消费群体很大，他们觉得我只想把它作为代步工具，我就是从 A 点到 B 点，你把我带到那就行了，我就是这个要求。
但是我不想花太多钱，因为我工资没那么高，只想作为代步工具，他这样直接就把这么大一部分的消费群体给砍掉了。有这个 T 以后肯定都要 B 级车以上，这对普通老百姓来说是不公平的。
对于赚钱多的人没影响，我就打专车嘛，本身我也想舒适一些，对于我挣钱少的人，我就没办法了。所以对这个群体是有影响的。
这对老百姓来说是不利的，还有就是本身也不环保。排量越大肯定污染越多嘛，那还用说啊。那就违背了国家提倡的这个绿色出行，那你要那么大的排量，肯定污染就会多。
博望志：现在买新车也是 1.6 以下的购置税低？
边师傅：对呀，这就是为了绿色出行，所以才有补贴的。那前面说补贴鼓励大家买小排量的，这样减少环境污染，结果后边出这个政策，这个（1.8T 以下小排量）不能干（网约车）。这本身就跟大的政策、大方向相违背了。
王师傅：我觉得新政一出来，大部分车就不能干网约车了，专车有一部分，快车就更别提了。你想国内自主品牌的国产车，可能有一半以上都做不了网约车了。
把咱们民族产业就给打压下去了。我前两天查了一下资料，奥迪 A3 和宝马 3 系都够不了这个标准，更何况国产车了，更做不了了，轴距达不到。
排量国家倡导新能源和环保，都出 1.4 和 1.6 的。我觉得其实只要是合乎国家标准出厂的车辆，应该都能在路上运行。一个是干净整洁，服务质量跟得上，符合大众的出行就可以了。对国家发展经济，拉动内需，都有利嘛。
张师傅：我也有个观点，专车跟快车肯定有区别之分，专车就是要高端，因为价位是不一样的，所以要求车的档次也是不一样的。
应该是两个政策，细分化，快车有快车的政策，有不同需求的人，坐不一样的车。一刀切车就少了，大家的出行费用就增加了。就会出现车挑人了，不是人挑车了。我肯定选哪个活儿大挑哪个，就跟以前出租车一样了。

贰
博望志：关于户籍的问题，大家有什么想说的？
王师傅：我认为是歧视外地人，我首先就受到影响了，我就不能工作了。我认为我已经融入这个城市了，已经有感情了，对这个城市也做出了很多贡献。
保险啊，纳税啊该做的都做了，就是缺了一纸户口。我在北京也 10 多年了，我自认我比北京有的司机也不差。
张师傅：我对京籍还是挺赞同的，主要是北京实在是太拥堵了。如果把外地车都砍掉，我觉得北京也会缓解压力。再加上如果自私地说的话，我们的收入更有保障了。
关于京牌我觉得这个可以推给滴滴公司，它自己应该有一些车辆。滴滴应该有些自营的车辆，滴滴可以招聘司机，司机打工有个合理的工资定价，这也是解决问题的办法。
边师傅：京牌对于我来说我是赞成的，因为本身你就是在北京的路面上跑，如果不是京牌的话，他所有的档案全在外地，北京的交管部门什么都没有，存在安全隐患问题。所以京牌还是应该限的，要不然北京本身就很拥堵了。
我听说确实有一些村里的几个小伙子，家里有车，一想北京活儿多，拉滴滴能挣钱。从外地基本都是张家口或者河北周边的，说周一一大早开着车来北京了，然后天天拉，也不租房，困了累了就睡车里，睡几个小时起来再接着跑，到周六的时候开回去。
回家休息两天，周一又回来了。这肯定是增加了北京路面的交通拥堵，肯定是疲劳驾驶，这么干肯定存在危险，他不光是对自己安全不负责任，对别人，对乘客和别的车带来危险。
户籍的话限制有点苛刻了，像我就是外地户籍司机，我就是燕郊的，我依然会在北京生活，依然在北京工作，人还是在北京。因为我的家整个都在北京，顶多我换一个行业，难道我就不在北京了么？减少北京的人口了么？
互联网 + 的时代，是共享经济的时代，是为了把像我这样的，我自己的车本身自己闲着也是闲着。
其实我就是不拉网约车，我的车依然是在北京路上跑，就是把这些闲置的车给利用起来，改善大家的出行。所以如果把这块限制起来的话，对改善整个北京的出行是不利的。
据我了解很多公司的老板和上班族，他们现在已经习惯于用网约车了。因为用网约车很方便，不用我自己开车。
他们可以在车上完成很多办公，在去公司的路上接电话，安排工作上的事儿。他自己不开车了，也减少了路上的车辆，从这点上，是可以缓解北京交通压力的。
我遇到过坐我车的乘客跟我说，突然觉得我们家的车没用了，上班打滴滴挺好，又方便还省钱。自己不用开车，因为到公司楼下，像国贸地区，又堵车停车又贵。
我原来在国贸招商局大厦，停车第一小时 10 块，第二小时起每小时 15，一天停车费 100 多。有这钱完全够打车的了。互联网 + 是很好的方式，把大家的车利用起来，其实也减少了很多人开车了。
这个政策如果限制得很严，路上的网约车一定会少，他们打不着车的情况下，肯定又开始开自己的车了，这样对北京的发展是不利的。
叁
博望志：如果政策落地，下一步有什么打算？
王师傅：如果政策落地的话，第一我在（滴滴）公司肯定是干不了了。但我个人有车，我是京牌的车呀，你说我在北京干不了，我是回老家干？
回老家干我是京牌车，如果老家也限制外地车的话。因为北京出台的政策，外地是效仿的，包括限购啊、摇号啊各地不是都效仿么。
我要开着北京的车回去也不能干对吧？这样的话我就处于尴尬的状态了。车是北京的牌，人是外地的户口，回到原籍也被限呢？
更多访谈内容：语音
 
题图来源：South China Morning Post
#欢迎关注爱范儿认证微信公众号：AppSolution（微信号：appsolution），发现新酷精华应用。
爱范儿 |
原文链接 ·
查看评论 ·
新浪微博




 </content>
</doc>
